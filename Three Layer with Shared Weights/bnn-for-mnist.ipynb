{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"141370fc-54a4-4a6b-afcf-a7677dc6dc87","cell_type":"markdown","source":"# Soft Binary Neural Network with Recurrent Crossbar Recycling","metadata":{}},{"id":"508058d8-e23a-4c29-aad7-c2b233d621c9","cell_type":"markdown","source":"## Imports and Dataset","metadata":{}},{"id":"9a70e539-1dc9-4e36-9c9f-18fbdaeede1f","cell_type":"code","source":"import numpy as np\nimport torch\nimport cv2 as cv\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nfrom torchinfo import summary\nimport ast\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:05:24.355302Z","iopub.execute_input":"2025-01-27T07:05:24.355600Z","iopub.status.idle":"2025-01-27T07:05:31.733672Z","shell.execute_reply.started":"2025-01-27T07:05:24.355568Z","shell.execute_reply":"2025-01-27T07:05:31.732750Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":1},{"id":"d406d54c-db27-4536-a8c1-f46437f6fb71","cell_type":"code","source":"def plot_history(history, num_epochs, element):\n    epochs = range(len(history[list(history.keys())[0]]))\n    \n    fig, ax1 = plt.subplots(figsize=(12, 8))\n    \n    ax1.plot(epochs, history[\"train_loss\"], label=\"Train Loss\", color=\"blue\")\n    ax1.plot(epochs, history[\"val_loss\"], label=\"Validation Loss\", color=\"red\")\n    ax1.set_xlabel(\"Epochs\", fontsize=14)\n    ax1.set_ylabel(\"Loss\", fontsize=14, color=\"blue\")\n    ax1.tick_params(axis=\"y\", labelcolor=\"blue\")\n    ax1.legend(loc=\"upper left\")\n    ax1.grid(True)\n\n    ax2 = ax1.twinx()\n    ax2.plot(epochs, history[\"train_accuracy\"], label=\"Train Accuracy\", color=\"green\")\n    ax2.plot(epochs, history[\"val_accuracy\"], label=\"Validation Accuracy\", color=\"orange\")\n    ax2.set_ylabel(\"Accuracy (%)\", fontsize=14, color=\"green\")\n    ax2.tick_params(axis=\"y\", labelcolor=\"green\")\n    ax2.legend(loc=\"upper right\")\n\n    plt.title(f\"Training and Validation Metrics for {element}\", fontsize=16)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:05:31.735295Z","iopub.execute_input":"2025-01-27T07:05:31.735726Z","iopub.status.idle":"2025-01-27T07:05:31.742341Z","shell.execute_reply.started":"2025-01-27T07:05:31.735703Z","shell.execute_reply":"2025-01-27T07:05:31.741352Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":2},{"id":"7c658023-75df-4754-a617-a8ba6d08d068","cell_type":"code","source":"def test(model, test_loader, class_names=None):\n    model.eval()\n    total_loss = 0\n    total_correct = 0\n    total_samples = 0\n    all_predictions = []\n    all_labels = []\n\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item() * inputs.size(0)\n\n            _, predicted = torch.max(outputs, dim=1)\n            total_correct += (predicted == labels).sum().item()\n            total_samples += labels.size(0)\n\n            all_predictions.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    avg_loss = total_loss / total_samples\n    accuracy = (total_correct / total_samples) * 100\n\n    print(f\"Validation Loss: {avg_loss:.4f}\")\n    print(f\"Validation Accuracy: {accuracy:.2f}%\")\n    \n    cm = confusion_matrix(all_labels, all_predictions)\n\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel('Predicted Labels')\n    plt.ylabel('True Labels')\n    plt.title('Confusion Matrix')\n    plt.show()\n\n    return cm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:05:31.743767Z","iopub.execute_input":"2025-01-27T07:05:31.744087Z","iopub.status.idle":"2025-01-27T07:05:31.763813Z","shell.execute_reply.started":"2025-01-27T07:05:31.744055Z","shell.execute_reply":"2025-01-27T07:05:31.762990Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":3},{"id":"be5e8ced-6bb5-445a-9b04-bc268eec917e","cell_type":"markdown","source":"### MNIST Handwritten Digits","metadata":{}},{"id":"d3bbad17-7067-4f06-8755-012646ca9567","cell_type":"code","source":"class BinarizeAndAddNoiseTransform:\n    def __init__(self, threshold=0.5, noise_std=0.01):\n        self.threshold = threshold\n        self.noise_std = noise_std\n\n    def __call__(self, img):\n        img = transforms.ToTensor()(img).to(device)\n        img = (img > self.threshold).float()\n        noise = torch.randn(img.size(), device=device) * self.noise_std\n        noisy_img = img + noise\n        return noisy_img\n\nbinary_noise_transform = transforms.Compose([\n    BinarizeAndAddNoiseTransform(threshold=0.5, noise_std=0.05)\n])\n\ntrain_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=binary_noise_transform)\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=6000, shuffle=True)\n\ntest_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\ntest_loader = DataLoader(dataset=test_dataset, batch_size=10000, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:05:31.764576Z","iopub.execute_input":"2025-01-27T07:05:31.764844Z","iopub.status.idle":"2025-01-27T07:05:35.470733Z","shell.execute_reply.started":"2025-01-27T07:05:31.764824Z","shell.execute_reply":"2025-01-27T07:05:35.469753Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nFailed to download (trying next):\n<urlopen error [Errno 111] Connection refused>\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9.91M/9.91M [00:00<00:00, 17.9MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nFailed to download (trying next):\n<urlopen error [Errno 111] Connection refused>\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 28.9k/28.9k [00:00<00:00, 478kB/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nFailed to download (trying next):\n<urlopen error [Errno 111] Connection refused>\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1.65M/1.65M [00:00<00:00, 4.48MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nFailed to download (trying next):\n<urlopen error [Errno 111] Connection refused>\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4.54k/4.54k [00:00<00:00, 6.60MB/s]","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"id":"bc40653e-ebac-4013-9ecb-27dfe1370edd","cell_type":"code","source":"# Get a subset of the dataset\ntrain_in, train_lab = next(iter(train_loader))\nval_in, val_lab = next(iter(test_loader))\n\n# Move data to the appropriate device\ntrain_in, train_lab = train_in.to(device), train_lab.to(device)\nval_in, val_lab = val_in.to(device), val_lab.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:05:35.471593Z","iopub.execute_input":"2025-01-27T07:05:35.471834Z","iopub.status.idle":"2025-01-27T07:05:38.394350Z","shell.execute_reply.started":"2025-01-27T07:05:35.471813Z","shell.execute_reply":"2025-01-27T07:05:38.393706Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":5},{"id":"d76599e3-3d87-4d1d-bd5f-41ca0adab18f","cell_type":"code","source":"demo_ind = 1\nplt.imshow(train_in[demo_ind].cpu().squeeze(), cmap='gray')\nplt.title(f\"Label: {train_lab[demo_ind].item()}\")\nplt.axis('off')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:05:38.396382Z","iopub.execute_input":"2025-01-27T07:05:38.396630Z","iopub.status.idle":"2025-01-27T07:05:38.569381Z","shell.execute_reply.started":"2025-01-27T07:05:38.396610Z","shell.execute_reply":"2025-01-27T07:05:38.568549Z"},"jupyter":{"source_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":6},{"id":"d1b44def-df85-406b-87e8-fbc5b4f7fe7a","cell_type":"markdown","source":"## Custom Neural Network","metadata":{}},{"id":"391dfb3d-1a2b-42d7-9ff7-e3f0e831d50a","cell_type":"code","source":"torch.backends.cudnn.benchmark = True\n\ndef tensor_stats(tensor, name=\"Tensor\"):\n    tensor = tensor.to(device)\n    mean_magnitude = tensor.abs().mean().item()\n    print(f\"{name} - Mean Magnitude: {mean_magnitude:.2e}, Max: {tensor.max().item():.2e}, Min: {tensor.min().item():.2e}\")\n\nclass SoftBinaryRecurrentForwardNetwork(nn.Module):\n    def __init__(self, G_ON, G_OFF, V_INV, R_INV_1, R_INV_2, V_1, V_0, zeta, initial_factor, monitor_latents = False,\n                 crossbar=(64,64), input_size=784, recurrence_size=32, output_size=10, int_norm=True, data_in = 28, \n                 bin_active=True, monitor_volts=False, monitor_grads =True, dropout=0.01, int_lr=0.01, Scaling = 1):\n        super(SoftBinaryRecurrentForwardNetwork, self).__init__()\n\n        # Crossbar Weights\n        self.w = nn.Parameter(torch.empty(crossbar, device=device))\n        nn.init.xavier_uniform_(self.w)\n        self.w.data = initial_factor * self.w\n\n        # Circuit Parameters\n        self.G_ON = torch.tensor(G_ON, device=device)*Scaling\n        self.G_OFF = torch.tensor(G_OFF, device=device)*Scaling\n        self.V_INV = torch.tensor(V_INV, device=device)\n        self.R_INV_1 = torch.tensor(R_INV_1, device=device)\n        self.R_INV_2 = torch.tensor(R_INV_2, device=device) # Usually 3 or 4 times the first\n        self.V_1 = torch.tensor(V_1, device=device)\n        self.V_0 = torch.tensor(V_0, device=device)\n\n        # Architecture Parameters\n        self.crossbar_in = crossbar[0]\n        self.crossbar_out = crossbar[1]\n        \n        self.feedback = recurrence_size\n        self.data_in = data_in\n        self.first_bias = crossbar[0] - recurrence_size - data_in\n        self.r_passes = input_size // self.data_in\n        \n        self.second_layer = crossbar[1] - output_size - recurrence_size\n\n        self.final_bias = crossbar[0] - 2*self.second_layer\n        self.output_size = output_size\n        \n        # Training Variables\n        self.zeta = torch.tensor(zeta, device=device)\n        self.monitor_volts = monitor_volts\n        self.monitor_grads = monitor_grads\n        self.monitor_latents = monitor_latents\n        self.dropout = nn.Dropout(p=dropout)\n        self.int_lr = torch.tensor(int_lr, device=device)\n        self.int_norm = int_norm\n        self.bin_active = bin_active\n\n    def INV_AMP(self, x, R_INV):\n        return -self.V_INV * torch.tanh(R_INV * x / self.V_INV)\n\n    def SOFT_BIN(self, x):\n        if self.bin_active: return ((self.G_ON - self.G_OFF) * torch.sigmoid(x * self.zeta) + self.G_OFF)\n        else: return self.G_ON * x * self.zeta * 0.4\n\n    def PREPROCESS(self, img):\n        return (self.V_1 - self.V_0) * img.view(img.size(0), -1).to(device) + self.V_0\n\n    def forward(self, img):\n        # Preprocessing: Two States of input (V_ON and V_OFF)\n        img = self.PREPROCESS(img)\n\n        # RRAM Soft Binarization\n        g = self.SOFT_BIN(self.w)\n        if self.monitor_latents: tensor_stats(self.w, \"Latent Weights:\")\n\n        # Recurrent Encoding Layer\n        feedback = self.PREPROCESS(torch.zeros((img.shape[0], self.feedback), device=device))\n        bias = self.PREPROCESS(torch.tensor([1, -1] * (self.first_bias // 2), device=device).float().unsqueeze(0).repeat(img.shape[0], 1))\n\n        for r_pass in range(self.r_passes):\n            x = torch.cat((feedback, bias, img[:, r_pass * self.data_in:(r_pass + 1) * self.data_in]), dim=1)\n            x = F.linear(x, g[-self.feedback:, : ], bias=None)\n            x1 = self.INV_AMP(x, self.R_INV_1)\n            if self.monitor_volts: tensor_stats(x1, f\"Voltages in Recurrent Stage after pass {r_pass}\")\n            feedback = x1\n        \n        else:\n            x2 = self.INV_AMP(x, self.R_INV_2)\n            x = torch.cat((x1, x2), dim=1)\n        x = self.dropout(x)\n\n        # Feature Extraction Layer\n        x = F.linear(x, g[self.output_size : -self.feedback, : ], bias=None)\n        x1 = self.INV_AMP(x, self.R_INV_1)\n        x2 = self.INV_AMP(x, self.R_INV_2)\n        if self.monitor_volts: tensor_stats(x, f\"Voltages after h_layer {h_pass}\")\n        \n        bias2 = self.PREPROCESS(torch.tensor([1, -1] * (self.final_bias // 2), device=device).float().unsqueeze(0).repeat(img.shape[0], 1))\n        x = torch.cat((bias2, x1, x2), dim=1)\n        \n        # Classification Layer\n        x = F.linear(x, g[ : self.output_size, : ], bias=None)\n        return x\n\n    def backprop(self, ext_lr):\n        with torch.no_grad():\n            if self.w.grad is not None:\n                grad = self.w.grad.to(device)\n                if self.monitor_grads: tensor_stats(grad, \"Original Grads\")\n                if self.int_norm:\n                    grad[-self.feedback:, :] = self.int_lr * grad[-self.feedback:, :] / (torch.norm(grad[-self.feedback:, :]) + 1e-20)\n                    grad[self.output_size:-self.feedback, :] = self.int_lr * grad[self.output_size:-self.feedback, :] / (torch.norm(grad[self.output_size:-self.feedback, :]) + 1e-20)\n                    grad[:self.output_size, :] = self.int_lr * grad[:self.output_size, :] / (torch.norm(grad[:self.output_size, :]) + 1e-20)\n                grad = ext_lr * grad\n                if self.monitor_grads:\n                    tensor_stats(grad[-self.feedback:, :], \"Layer 1 Gradients\")\n                    tensor_stats(grad[self.output_size:-self.feedback, :], \"Layer 2 Gradients\")\n                    tensor_stats(grad[:self.output_size, :], \"Layer 3 Gradients\")\n                self.w -= grad\n                self.w.grad.zero_()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:11:23.289121Z","iopub.execute_input":"2025-01-27T07:11:23.289463Z","iopub.status.idle":"2025-01-27T07:11:23.306540Z","shell.execute_reply.started":"2025-01-27T07:11:23.289441Z","shell.execute_reply":"2025-01-27T07:11:23.305596Z"}},"outputs":[],"execution_count":32},{"id":"845d51da-8368-4c97-89a2-9fc1374f408b","cell_type":"markdown","source":"## Model Parameters","metadata":{}},{"id":"d332b5ba-a9e0-4e8a-bff9-3176267bef00","cell_type":"code","source":"params_RRAM = {\n    \"Scaling\": 1,\n    \"G_ON\": 6e-5,          # High conductance state\n    \"G_OFF\": 2.88e-6,        # Low conductance state\n    \"V_INV\": 1,            # Inverter voltage\n    \"R_INV_1\": 2e+3,         # Inverter resistance\n    \"R_INV_2\": 5e+3,\n    \"V_1\": 0.1,              # High input voltage\n    \"V_0\": -0.1,             # Low input voltage\n    \"zeta\": 1,               # Sharpness parameter for soft binarization \n    \"initial_factor\": 10,   # Initial weight scaling factor\n    \"crossbar\": (64, 64),      # Crossbar size (rows, columns)\n    \"input_size\": 784,        # Input size\n    \"recurrence_size\": 32,    # Feedback size for recurrent connections\n    \"output_size\": 10,        # Output size\n    \"data_in\": 28,\n    \"bin_active\": False,\n    \"monitor_volts\": False,        # check post inverter voltages flag\n    \"monitor_grads\": True,        # check gradient values\n    \"monitor_latents\": False,\n    \"dropout\": 0.01,          # Dropout probability  \n    \"lr\": 1,            # Training LR\n    \"epochs\": 300,          # Training Epochs\n    \"int_lr\": 10,           # Internal LR\n    \"int_norm\": True         # Normalizing Gradient\n}\n\nmodel_params = {k: v for k, v in params_RRAM.items() if k not in [\"noise_std\", \"batch_size\", \"lr\", \"epochs\"]}\nmodel_RRAM = SoftBinaryRecurrentForwardNetwork(**model_params).to(device)\nsummary(model_RRAM)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:19:30.119193Z","iopub.execute_input":"2025-01-27T07:19:30.119630Z","iopub.status.idle":"2025-01-27T07:19:30.133229Z","shell.execute_reply.started":"2025-01-27T07:19:30.119593Z","shell.execute_reply":"2025-01-27T07:19:30.132302Z"}},"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\nSoftBinaryRecurrentForwardNetwork        4,096\n├─Dropout: 1-1                           --\n=================================================================\nTotal params: 4,096\nTrainable params: 4,096\nNon-trainable params: 0\n================================================================="},"metadata":{}}],"execution_count":74},{"id":"282eeb54-9acf-4ce6-b5f0-13e5c60f8614","cell_type":"markdown","source":"### Loading Past Best Model","metadata":{}},{"id":"e0de620a-e0e4-49cf-863d-721e2680dd58","cell_type":"code","source":"try:\n    with open(f\"Best_Val_Accuracy.txt\", 'r') as f: val_best = float(f.read())\n    with open(f\"Best_Params.txt\", 'r') as f: params_best = ast.literal_eval(f.read())\n\n    model_best = SoftBinaryRecurrentForwardNetwork(**model_params).to(device)\n\n    print(\"Accuracy:\", val_best)\n    print(\"Parameters:\", params_best)\n\n    checkpoint = torch.load(f\"Best_model.pth\")\n    model_best.load_state_dict(checkpoint)\n    \nexcept Exception as e:\n    print(e)\n    val_best = 0\n    print(\"No Saved Model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:19:30.562804Z","iopub.execute_input":"2025-01-27T07:19:30.563157Z","iopub.status.idle":"2025-01-27T07:19:30.574280Z","shell.execute_reply.started":"2025-01-27T07:19:30.563124Z","shell.execute_reply":"2025-01-27T07:19:30.573194Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Accuracy: 11.35\nParameters: {'Scaling': 1, 'G_ON': 6e-05, 'G_OFF': 2.88e-06, 'V_INV': 10, 'R_INV_1': 20000.0, 'R_INV_2': 100000.0, 'V_1': 0.1, 'V_0': -0.1, 'zeta': 100, 'initial_factor': 0.1, 'crossbar': (64, 64), 'input_size': 784, 'recurrence_size': 32, 'output_size': 10, 'data_in': 28, 'bin_active': True, 'monitor_volts': False, 'monitor_grads': True, 'monitor_latents': False, 'dropout': 0.01, 'lr': 1, 'epochs': 300, 'int_lr': 0.01, 'int_norm': True}\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-75-c93657125f77>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(f\"Best_model.pth\")\n","output_type":"stream"}],"execution_count":75},{"id":"241d1155-be9e-4a7a-9a74-a1d608188350","cell_type":"markdown","source":"## Training:","metadata":{}},{"id":"25c0b7d2-7c19-4b3e-8856-76d2e9f32792","cell_type":"markdown","source":"### Training to a subset of Dataset First","metadata":{}},{"id":"efd13def-9751-4e0e-89bf-666a98444d02","cell_type":"code","source":"# Training parameters\nlr = params_RRAM[\"lr\"] / 25  # Initial learning rate\nnum_epochs = params_RRAM[\"epochs\"]\npatience = 10  # Number of epochs to wait for improvement\nmin_val_loss = float('inf')\nstagnant_epochs = 0\n\nfor epoch in range(num_epochs):\n    if epoch == 1:\n        lr *= 5\n    elif epoch == 2:\n        lr *= 5\n\n    model_RRAM.train()\n    outputs = model_RRAM(train_in)\n    loss = criterion(outputs, train_lab)\n    loss.backward()\n    model_RRAM.backprop(lr)\n\n    _, train_preds = torch.max(outputs, dim=1)\n    train_accuracy = (train_preds == train_lab).float().mean().item() * 100\n\n    # Evaluate on the validation set\n    model_RRAM.eval()\n    with torch.no_grad():\n        val_outputs = model_RRAM(val_in)\n        val_loss = criterion(val_outputs, val_lab).item()\n        _, val_preds = torch.max(val_outputs, dim=1)\n        val_accuracy = (val_preds == val_lab).float().mean().item() * 100\n\n    print(f\"Epoch {epoch + 1}, LR: {lr:.4f}, Train Loss: {loss.item():.4f}, \"\n          f\"Train Accuracy: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n\n    # Check for early stopping\n    if val_loss < min_val_loss:\n        min_val_loss = val_loss\n        stagnant_epochs = 0  # Reset counter if validation loss improves\n    else:\n        stagnant_epochs += 1\n\n    if stagnant_epochs >= patience:\n        print(f\"Early stopping triggered at epoch {epoch + 1}\")\n        break\n\n    # Learning rate adjustment\n    if epoch % 50 == 0 and epoch != 0:\n        lr /= 2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:19:31.074559Z","iopub.execute_input":"2025-01-27T07:19:31.074784Z","iopub.status.idle":"2025-01-27T07:19:33.913054Z","shell.execute_reply.started":"2025-01-27T07:19:31.074765Z","shell.execute_reply":"2025-01-27T07:19:33.912387Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Original Grads - Mean Magnitude: 7.16e-09, Max: 1.14e-07, Min: -6.94e-08\nLayer 1 Gradients - Mean Magnitude: 5.83e-03, Max: 5.34e-02, Min: -5.58e-02\nLayer 2 Gradients - Mean Magnitude: 6.38e-03, Max: 6.60e-02, Min: -7.33e-02\nLayer 3 Gradients - Mean Magnitude: 9.77e-03, Max: 7.11e-02, Min: -4.33e-02\nEpoch 1, LR: 0.0400, Train Loss: 2.3026, Train Accuracy: 9.68%, Val Loss: 2.3026, Val Accuracy: 10.32%\nOriginal Grads - Mean Magnitude: 6.89e-09, Max: 1.14e-07, Min: -6.43e-08\nLayer 1 Gradients - Mean Magnitude: 2.90e-02, Max: 2.71e-01, Min: -2.84e-01\nLayer 2 Gradients - Mean Magnitude: 3.20e-02, Max: 3.35e-01, Min: -3.63e-01\nLayer 3 Gradients - Mean Magnitude: 4.83e-02, Max: 3.60e-01, Min: -2.03e-01\nEpoch 2, LR: 0.2000, Train Loss: 2.3026, Train Accuracy: 9.73%, Val Loss: 2.3026, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 6.46e-09, Max: 1.14e-07, Min: -6.22e-08\nLayer 1 Gradients - Mean Magnitude: 1.45e-01, Max: 1.18e+00, Min: -1.68e+00\nLayer 2 Gradients - Mean Magnitude: 1.61e-01, Max: 1.52e+00, Min: -1.81e+00\nLayer 3 Gradients - Mean Magnitude: 2.35e-01, Max: 1.80e+00, Min: -9.80e-01\nEpoch 3, LR: 1.0000, Train Loss: 2.3026, Train Accuracy: 11.57%, Val Loss: 2.3026, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.62e-08, Max: 2.50e-07, Min: -2.66e-07\nLayer 1 Gradients - Mean Magnitude: 1.53e-01, Max: 1.20e+00, Min: -1.12e+00\nLayer 2 Gradients - Mean Magnitude: 1.76e-01, Max: 1.39e+00, Min: -1.06e+00\nLayer 3 Gradients - Mean Magnitude: 2.67e-01, Max: 1.85e+00, Min: -1.96e+00\nEpoch 4, LR: 1.0000, Train Loss: 2.3026, Train Accuracy: 11.58%, Val Loss: 2.3025, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 2.23e-08, Max: 3.73e-07, Min: -3.74e-07\nLayer 1 Gradients - Mean Magnitude: 1.56e-01, Max: 1.12e+00, Min: -9.73e-01\nLayer 2 Gradients - Mean Magnitude: 1.93e-01, Max: 1.12e+00, Min: -9.98e-01\nLayer 3 Gradients - Mean Magnitude: 2.77e-01, Max: 1.40e+00, Min: -1.40e+00\nEpoch 5, LR: 1.0000, Train Loss: 2.3025, Train Accuracy: 11.58%, Val Loss: 2.3025, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 2.07e-08, Max: 3.73e-07, Min: -3.73e-07\nLayer 1 Gradients - Mean Magnitude: 1.48e-01, Max: 1.03e+00, Min: -1.12e+00\nLayer 2 Gradients - Mean Magnitude: 1.94e-01, Max: 1.06e+00, Min: -1.05e+00\nLayer 3 Gradients - Mean Magnitude: 2.92e-01, Max: 1.07e+00, Min: -1.07e+00\nEpoch 6, LR: 1.0000, Train Loss: 2.3025, Train Accuracy: 11.58%, Val Loss: 2.3025, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 2.01e-08, Max: 3.71e-07, Min: -3.71e-07\nLayer 1 Gradients - Mean Magnitude: 1.19e-01, Max: 1.22e+00, Min: -1.27e+00\nLayer 2 Gradients - Mean Magnitude: 1.37e-01, Max: 1.54e+00, Min: -1.51e+00\nLayer 3 Gradients - Mean Magnitude: 2.92e-01, Max: 1.01e+00, Min: -1.01e+00\nEpoch 7, LR: 1.0000, Train Loss: 2.3025, Train Accuracy: 11.58%, Val Loss: 2.3025, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.83e-08, Max: 3.68e-07, Min: -3.68e-07\nLayer 1 Gradients - Mean Magnitude: 1.41e-01, Max: 9.38e-01, Min: -9.15e-01\nLayer 2 Gradients - Mean Magnitude: 2.37e-01, Max: 6.57e-01, Min: -6.52e-01\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.87e-01, Min: -9.87e-01\nEpoch 8, LR: 1.0000, Train Loss: 2.3024, Train Accuracy: 11.58%, Val Loss: 2.3025, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.73e-08, Max: 3.65e-07, Min: -3.65e-07\nLayer 1 Gradients - Mean Magnitude: 1.50e-01, Max: 8.86e-01, Min: -8.99e-01\nLayer 2 Gradients - Mean Magnitude: 2.52e-01, Max: 4.89e-01, Min: -4.89e-01\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.80e-01, Min: -9.80e-01\nEpoch 9, LR: 1.0000, Train Loss: 2.3024, Train Accuracy: 11.58%, Val Loss: 2.3024, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.70e-08, Max: 3.63e-07, Min: -3.63e-07\nLayer 1 Gradients - Mean Magnitude: 1.61e-01, Max: 5.99e-01, Min: -6.04e-01\nLayer 2 Gradients - Mean Magnitude: 2.60e-01, Max: 3.75e-01, Min: -3.78e-01\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.80e-01, Min: -9.80e-01\nEpoch 10, LR: 1.0000, Train Loss: 2.3023, Train Accuracy: 11.58%, Val Loss: 2.3024, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.68e-08, Max: 3.60e-07, Min: -3.60e-07\nLayer 1 Gradients - Mean Magnitude: 1.68e-01, Max: 5.34e-01, Min: -5.35e-01\nLayer 2 Gradients - Mean Magnitude: 2.65e-01, Max: 3.71e-01, Min: -3.73e-01\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.79e-01, Min: -9.79e-01\nEpoch 11, LR: 1.0000, Train Loss: 2.3023, Train Accuracy: 11.58%, Val Loss: 2.3024, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.67e-08, Max: 3.58e-07, Min: -3.58e-07\nLayer 1 Gradients - Mean Magnitude: 1.71e-01, Max: 4.20e-01, Min: -4.20e-01\nLayer 2 Gradients - Mean Magnitude: 2.66e-01, Max: 3.19e-01, Min: -3.19e-01\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.79e-01, Min: -9.79e-01\nEpoch 12, LR: 1.0000, Train Loss: 2.3023, Train Accuracy: 11.58%, Val Loss: 2.3024, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.66e-08, Max: 3.55e-07, Min: -3.55e-07\nLayer 1 Gradients - Mean Magnitude: 1.72e-01, Max: 3.86e-01, Min: -3.86e-01\nLayer 2 Gradients - Mean Magnitude: 2.66e-01, Max: 3.00e-01, Min: -3.09e-01\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.79e-01, Min: -9.79e-01\nEpoch 13, LR: 1.0000, Train Loss: 2.3022, Train Accuracy: 11.58%, Val Loss: 2.3023, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.65e-08, Max: 3.53e-07, Min: -3.53e-07\nLayer 1 Gradients - Mean Magnitude: 1.72e-01, Max: 3.59e-01, Min: -3.59e-01\nLayer 2 Gradients - Mean Magnitude: 2.66e-01, Max: 3.26e-01, Min: -3.27e-01\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.79e-01, Min: -9.79e-01\nEpoch 14, LR: 1.0000, Train Loss: 2.3022, Train Accuracy: 11.58%, Val Loss: 2.3023, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.64e-08, Max: 3.50e-07, Min: -3.50e-07\nLayer 1 Gradients - Mean Magnitude: 1.72e-01, Max: 3.44e-01, Min: -3.44e-01\nLayer 2 Gradients - Mean Magnitude: 2.65e-01, Max: 3.33e-01, Min: -3.30e-01\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.79e-01, Min: -9.79e-01\nEpoch 15, LR: 1.0000, Train Loss: 2.3022, Train Accuracy: 11.58%, Val Loss: 2.3023, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.62e-08, Max: 3.47e-07, Min: -3.47e-07\nLayer 1 Gradients - Mean Magnitude: 1.71e-01, Max: 3.72e-01, Min: -3.72e-01\nLayer 2 Gradients - Mean Magnitude: 2.17e-01, Max: 6.52e-01, Min: -6.10e-01\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.79e-01, Min: -9.79e-01\nEpoch 16, LR: 1.0000, Train Loss: 2.3021, Train Accuracy: 11.58%, Val Loss: 2.3023, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.61e-08, Max: 3.45e-07, Min: -3.45e-07\nLayer 1 Gradients - Mean Magnitude: 1.39e-01, Max: 6.92e-01, Min: -6.92e-01\nLayer 2 Gradients - Mean Magnitude: 7.47e-02, Max: 1.43e+00, Min: -1.41e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.79e-01, Min: -9.79e-01\nEpoch 17, LR: 1.0000, Train Loss: 2.3021, Train Accuracy: 11.58%, Val Loss: 2.3023, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.60e-08, Max: 3.42e-07, Min: -3.42e-07\nLayer 1 Gradients - Mean Magnitude: 1.33e-01, Max: 7.42e-01, Min: -7.42e-01\nLayer 2 Gradients - Mean Magnitude: 6.32e-02, Max: 1.31e+00, Min: -1.31e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.79e-01, Min: -9.79e-01\nEpoch 18, LR: 1.0000, Train Loss: 2.3020, Train Accuracy: 11.58%, Val Loss: 2.3022, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.59e-08, Max: 3.40e-07, Min: -3.40e-07\nLayer 1 Gradients - Mean Magnitude: 1.23e-01, Max: 8.47e-01, Min: -8.47e-01\nLayer 2 Gradients - Mean Magnitude: 1.64e-01, Max: 1.05e+00, Min: -1.17e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.79e-01, Min: -9.79e-01\nEpoch 19, LR: 1.0000, Train Loss: 2.3020, Train Accuracy: 11.58%, Val Loss: 2.3022, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.58e-08, Max: 3.37e-07, Min: -3.37e-07\nLayer 1 Gradients - Mean Magnitude: 8.71e-02, Max: 1.44e+00, Min: -1.44e+00\nLayer 2 Gradients - Mean Magnitude: 8.16e-02, Max: 1.01e+00, Min: -1.03e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.79e-01, Min: -9.79e-01\nEpoch 20, LR: 1.0000, Train Loss: 2.3020, Train Accuracy: 11.58%, Val Loss: 2.3022, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.56e-08, Max: 3.35e-07, Min: -3.35e-07\nLayer 1 Gradients - Mean Magnitude: 1.41e-01, Max: 6.39e-01, Min: -6.39e-01\nLayer 2 Gradients - Mean Magnitude: 1.09e-01, Max: 1.32e+00, Min: -1.19e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.79e-01, Min: -9.79e-01\nEpoch 21, LR: 1.0000, Train Loss: 2.3019, Train Accuracy: 11.58%, Val Loss: 2.3022, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.55e-08, Max: 3.32e-07, Min: -3.32e-07\nLayer 1 Gradients - Mean Magnitude: 9.73e-02, Max: 1.18e+00, Min: -1.18e+00\nLayer 2 Gradients - Mean Magnitude: 7.85e-02, Max: 1.18e+00, Min: -1.18e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.79e-01, Min: -9.79e-01\nEpoch 22, LR: 1.0000, Train Loss: 2.3019, Train Accuracy: 11.58%, Val Loss: 2.3022, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.54e-08, Max: 3.29e-07, Min: -3.29e-07\nLayer 1 Gradients - Mean Magnitude: 1.24e-01, Max: 9.86e-01, Min: -9.86e-01\nLayer 2 Gradients - Mean Magnitude: 9.06e-02, Max: 1.01e+00, Min: -1.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.79e-01, Min: -9.79e-01\nEpoch 23, LR: 1.0000, Train Loss: 2.3019, Train Accuracy: 11.58%, Val Loss: 2.3022, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.53e-08, Max: 3.27e-07, Min: -3.27e-07\nLayer 1 Gradients - Mean Magnitude: 4.98e-02, Max: 1.86e-01, Min: -1.86e-01\nLayer 2 Gradients - Mean Magnitude: 5.46e-02, Max: 1.30e+00, Min: -1.30e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.79e-01, Min: -9.79e-01\nEpoch 24, LR: 1.0000, Train Loss: 2.3018, Train Accuracy: 11.58%, Val Loss: 2.3021, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.52e-08, Max: 3.24e-07, Min: -3.24e-07\nLayer 1 Gradients - Mean Magnitude: 1.30e-01, Max: 6.40e-01, Min: -6.40e-01\nLayer 2 Gradients - Mean Magnitude: 5.68e-02, Max: 1.32e+00, Min: -1.31e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.79e-01, Min: -9.79e-01\nEpoch 25, LR: 1.0000, Train Loss: 2.3018, Train Accuracy: 11.58%, Val Loss: 2.3021, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.50e-08, Max: 3.22e-07, Min: -3.22e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.79e-01, Min: -9.79e-01\nEpoch 26, LR: 1.0000, Train Loss: 2.3018, Train Accuracy: 11.58%, Val Loss: 2.3021, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.49e-08, Max: 3.19e-07, Min: -3.19e-07\nLayer 1 Gradients - Mean Magnitude: 7.17e-02, Max: 3.65e-01, Min: -3.65e-01\nLayer 2 Gradients - Mean Magnitude: 7.75e-02, Max: 9.95e-01, Min: -9.95e-01\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.79e-01, Min: -9.79e-01\nEpoch 27, LR: 1.0000, Train Loss: 2.3017, Train Accuracy: 11.58%, Val Loss: 2.3021, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.48e-08, Max: 3.17e-07, Min: -3.17e-07\nLayer 1 Gradients - Mean Magnitude: 1.14e-01, Max: 1.03e+00, Min: -1.03e+00\nLayer 2 Gradients - Mean Magnitude: 7.19e-02, Max: 1.25e+00, Min: -1.26e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.79e-01, Min: -9.79e-01\nEpoch 28, LR: 1.0000, Train Loss: 2.3017, Train Accuracy: 11.58%, Val Loss: 2.3021, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.47e-08, Max: 3.14e-07, Min: -3.14e-07\nLayer 1 Gradients - Mean Magnitude: 1.36e-01, Max: 7.45e-01, Min: -7.45e-01\nLayer 2 Gradients - Mean Magnitude: 5.68e-02, Max: 1.31e+00, Min: -1.31e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.79e-01, Min: -9.79e-01\nEpoch 29, LR: 1.0000, Train Loss: 2.3017, Train Accuracy: 11.58%, Val Loss: 2.3020, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.46e-08, Max: 3.11e-07, Min: -3.11e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.79e-01, Min: -9.79e-01\nEpoch 30, LR: 1.0000, Train Loss: 2.3016, Train Accuracy: 11.58%, Val Loss: 2.3020, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.44e-08, Max: 3.09e-07, Min: -3.09e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.78e-01, Min: -9.78e-01\nEpoch 31, LR: 1.0000, Train Loss: 2.3016, Train Accuracy: 11.58%, Val Loss: 2.3020, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.43e-08, Max: 3.06e-07, Min: -3.06e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.78e-01, Min: -9.78e-01\nEpoch 32, LR: 1.0000, Train Loss: 2.3016, Train Accuracy: 11.58%, Val Loss: 2.3020, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.42e-08, Max: 3.04e-07, Min: -3.04e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.78e-01, Min: -9.78e-01\nEpoch 33, LR: 1.0000, Train Loss: 2.3016, Train Accuracy: 11.58%, Val Loss: 2.3020, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.41e-08, Max: 3.01e-07, Min: -3.01e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.78e-01, Min: -9.78e-01\nEpoch 34, LR: 1.0000, Train Loss: 2.3015, Train Accuracy: 11.58%, Val Loss: 2.3020, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.40e-08, Max: 2.98e-07, Min: -2.98e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.78e-01, Min: -9.78e-01\nEpoch 35, LR: 1.0000, Train Loss: 2.3015, Train Accuracy: 11.58%, Val Loss: 2.3020, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.38e-08, Max: 2.96e-07, Min: -2.96e-07\nLayer 1 Gradients - Mean Magnitude: 1.04e-02, Max: 6.16e-02, Min: -6.16e-02\nLayer 2 Gradients - Mean Magnitude: 5.46e-02, Max: 1.30e+00, Min: -1.30e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.78e-01, Min: -9.78e-01\nEpoch 36, LR: 1.0000, Train Loss: 2.3015, Train Accuracy: 11.58%, Val Loss: 2.3019, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.37e-08, Max: 2.93e-07, Min: -2.93e-07\nLayer 1 Gradients - Mean Magnitude: 1.33e-01, Max: 6.58e-01, Min: -6.58e-01\nLayer 2 Gradients - Mean Magnitude: 5.68e-02, Max: 1.32e+00, Min: -1.32e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.78e-01, Min: -9.78e-01\nEpoch 37, LR: 1.0000, Train Loss: 2.3014, Train Accuracy: 11.58%, Val Loss: 2.3019, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.36e-08, Max: 2.91e-07, Min: -2.91e-07\nLayer 1 Gradients - Mean Magnitude: 2.18e-02, Max: 1.78e-01, Min: -1.78e-01\nLayer 2 Gradients - Mean Magnitude: 7.17e-02, Max: 1.19e+00, Min: -1.19e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.78e-01, Min: -9.78e-01\nEpoch 38, LR: 1.0000, Train Loss: 2.3014, Train Accuracy: 11.58%, Val Loss: 2.3019, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.35e-08, Max: 2.88e-07, Min: -2.88e-07\nLayer 1 Gradients - Mean Magnitude: 5.08e-02, Max: 1.72e+00, Min: -1.72e+00\nLayer 2 Gradients - Mean Magnitude: 5.76e-02, Max: 1.36e+00, Min: -1.32e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.78e-01, Min: -9.78e-01\nEpoch 39, LR: 1.0000, Train Loss: 2.3014, Train Accuracy: 11.58%, Val Loss: 2.3019, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.34e-08, Max: 2.85e-07, Min: -2.85e-07\nLayer 1 Gradients - Mean Magnitude: 1.24e-01, Max: 1.10e+00, Min: -1.10e+00\nLayer 2 Gradients - Mean Magnitude: 5.68e-02, Max: 1.31e+00, Min: -1.35e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.78e-01, Min: -9.78e-01\nEpoch 40, LR: 1.0000, Train Loss: 2.3013, Train Accuracy: 11.58%, Val Loss: 2.3019, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.32e-08, Max: 2.83e-07, Min: -2.83e-07\nLayer 1 Gradients - Mean Magnitude: 6.52e-03, Max: 3.17e-02, Min: -3.17e-02\nLayer 2 Gradients - Mean Magnitude: 5.46e-02, Max: 1.30e+00, Min: -1.30e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.77e-01, Min: -9.77e-01\nEpoch 41, LR: 1.0000, Train Loss: 2.3013, Train Accuracy: 11.58%, Val Loss: 2.3019, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.31e-08, Max: 2.80e-07, Min: -2.80e-07\nLayer 1 Gradients - Mean Magnitude: 6.73e-04, Max: 6.43e-03, Min: -6.43e-03\nLayer 2 Gradients - Mean Magnitude: 5.36e-02, Max: 1.32e+00, Min: -1.32e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.77e-01, Min: -9.77e-01\nEpoch 42, LR: 1.0000, Train Loss: 2.3013, Train Accuracy: 11.58%, Val Loss: 2.3019, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.30e-08, Max: 2.77e-07, Min: -2.77e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.77e-01, Min: -9.77e-01\nEpoch 43, LR: 1.0000, Train Loss: 2.3013, Train Accuracy: 11.58%, Val Loss: 2.3018, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.29e-08, Max: 2.75e-07, Min: -2.75e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.77e-01, Min: -9.77e-01\nEpoch 44, LR: 1.0000, Train Loss: 2.3012, Train Accuracy: 11.58%, Val Loss: 2.3018, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.28e-08, Max: 2.72e-07, Min: -2.72e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.77e-01, Min: -9.77e-01\nEpoch 45, LR: 1.0000, Train Loss: 2.3012, Train Accuracy: 11.58%, Val Loss: 2.3018, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.26e-08, Max: 2.70e-07, Min: -2.70e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.77e-01, Min: -9.77e-01\nEpoch 46, LR: 1.0000, Train Loss: 2.3012, Train Accuracy: 11.58%, Val Loss: 2.3018, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.25e-08, Max: 2.67e-07, Min: -2.67e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.77e-01, Min: -9.77e-01\nEpoch 47, LR: 1.0000, Train Loss: 2.3011, Train Accuracy: 11.58%, Val Loss: 2.3018, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.24e-08, Max: 2.64e-07, Min: -2.64e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.76e-01, Min: -9.76e-01\nEpoch 48, LR: 1.0000, Train Loss: 2.3011, Train Accuracy: 11.58%, Val Loss: 2.3018, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.23e-08, Max: 2.62e-07, Min: -2.62e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.76e-01, Min: -9.76e-01\nEpoch 49, LR: 1.0000, Train Loss: 2.3011, Train Accuracy: 11.58%, Val Loss: 2.3018, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.22e-08, Max: 2.59e-07, Min: -2.59e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.76e-01, Min: -9.76e-01\nEpoch 50, LR: 1.0000, Train Loss: 2.3011, Train Accuracy: 11.58%, Val Loss: 2.3018, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.20e-08, Max: 2.57e-07, Min: -2.57e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.76e-01, Min: -9.76e-01\nEpoch 51, LR: 1.0000, Train Loss: 2.3010, Train Accuracy: 11.58%, Val Loss: 2.3017, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.19e-08, Max: 2.54e-07, Min: -2.54e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.88e-01, Min: -4.88e-01\nEpoch 52, LR: 0.5000, Train Loss: 2.3010, Train Accuracy: 11.58%, Val Loss: 2.3017, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.19e-08, Max: 2.53e-07, Min: -2.53e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.88e-01, Min: -4.88e-01\nEpoch 53, LR: 0.5000, Train Loss: 2.3010, Train Accuracy: 11.58%, Val Loss: 2.3017, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.18e-08, Max: 2.51e-07, Min: -2.51e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.88e-01, Min: -4.88e-01\nEpoch 54, LR: 0.5000, Train Loss: 2.3010, Train Accuracy: 11.58%, Val Loss: 2.3017, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.17e-08, Max: 2.50e-07, Min: -2.50e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.88e-01, Min: -4.88e-01\nEpoch 55, LR: 0.5000, Train Loss: 2.3010, Train Accuracy: 11.58%, Val Loss: 2.3017, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.17e-08, Max: 2.49e-07, Min: -2.49e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.88e-01, Min: -4.88e-01\nEpoch 56, LR: 0.5000, Train Loss: 2.3010, Train Accuracy: 11.58%, Val Loss: 2.3017, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.16e-08, Max: 2.47e-07, Min: -2.47e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.88e-01, Min: -4.88e-01\nEpoch 57, LR: 0.5000, Train Loss: 2.3009, Train Accuracy: 11.58%, Val Loss: 2.3017, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.16e-08, Max: 2.46e-07, Min: -2.46e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.88e-01, Min: -4.88e-01\nEpoch 58, LR: 0.5000, Train Loss: 2.3009, Train Accuracy: 11.58%, Val Loss: 2.3017, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.15e-08, Max: 2.45e-07, Min: -2.45e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.88e-01, Min: -4.88e-01\nEpoch 59, LR: 0.5000, Train Loss: 2.3009, Train Accuracy: 11.58%, Val Loss: 2.3017, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.14e-08, Max: 2.43e-07, Min: -2.43e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.88e-01, Min: -4.88e-01\nEpoch 60, LR: 0.5000, Train Loss: 2.3009, Train Accuracy: 11.58%, Val Loss: 2.3017, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.14e-08, Max: 2.42e-07, Min: -2.42e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.88e-01, Min: -4.88e-01\nEpoch 61, LR: 0.5000, Train Loss: 2.3009, Train Accuracy: 11.58%, Val Loss: 2.3017, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.13e-08, Max: 2.41e-07, Min: -2.41e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.87e-01, Min: -4.87e-01\nEpoch 62, LR: 0.5000, Train Loss: 2.3009, Train Accuracy: 11.58%, Val Loss: 2.3017, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.13e-08, Max: 2.39e-07, Min: -2.39e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.87e-01, Min: -4.87e-01\nEpoch 63, LR: 0.5000, Train Loss: 2.3009, Train Accuracy: 11.58%, Val Loss: 2.3017, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.12e-08, Max: 2.38e-07, Min: -2.38e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.87e-01, Min: -4.87e-01\nEpoch 64, LR: 0.5000, Train Loss: 2.3009, Train Accuracy: 11.58%, Val Loss: 2.3017, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.11e-08, Max: 2.37e-07, Min: -2.37e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.87e-01, Min: -4.87e-01\nEpoch 65, LR: 0.5000, Train Loss: 2.3008, Train Accuracy: 11.58%, Val Loss: 2.3017, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.11e-08, Max: 2.35e-07, Min: -2.35e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.87e-01, Min: -4.87e-01\nEpoch 66, LR: 0.5000, Train Loss: 2.3008, Train Accuracy: 11.58%, Val Loss: 2.3017, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.10e-08, Max: 2.34e-07, Min: -2.34e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.87e-01, Min: -4.87e-01\nEpoch 67, LR: 0.5000, Train Loss: 2.3008, Train Accuracy: 11.58%, Val Loss: 2.3017, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.10e-08, Max: 2.33e-07, Min: -2.33e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.87e-01, Min: -4.87e-01\nEpoch 68, LR: 0.5000, Train Loss: 2.3008, Train Accuracy: 11.58%, Val Loss: 2.3017, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.09e-08, Max: 2.31e-07, Min: -2.31e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.87e-01, Min: -4.87e-01\nEpoch 69, LR: 0.5000, Train Loss: 2.3008, Train Accuracy: 11.58%, Val Loss: 2.3017, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.08e-08, Max: 2.30e-07, Min: -2.30e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.87e-01, Min: -4.87e-01\nEpoch 70, LR: 0.5000, Train Loss: 2.3008, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.08e-08, Max: 2.29e-07, Min: -2.29e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.87e-01, Min: -4.87e-01\nEpoch 71, LR: 0.5000, Train Loss: 2.3008, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.07e-08, Max: 2.27e-07, Min: -2.27e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.87e-01, Min: -4.87e-01\nEpoch 72, LR: 0.5000, Train Loss: 2.3008, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.07e-08, Max: 2.26e-07, Min: -2.26e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.87e-01, Min: -4.87e-01\nEpoch 73, LR: 0.5000, Train Loss: 2.3008, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.06e-08, Max: 2.25e-07, Min: -2.25e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.87e-01, Min: -4.87e-01\nEpoch 74, LR: 0.5000, Train Loss: 2.3007, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.05e-08, Max: 2.24e-07, Min: -2.24e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.87e-01, Min: -4.87e-01\nEpoch 75, LR: 0.5000, Train Loss: 2.3007, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.05e-08, Max: 2.22e-07, Min: -2.22e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.87e-01, Min: -4.87e-01\nEpoch 76, LR: 0.5000, Train Loss: 2.3007, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.04e-08, Max: 2.21e-07, Min: -2.21e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.87e-01, Min: -4.87e-01\nEpoch 77, LR: 0.5000, Train Loss: 2.3007, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.03e-08, Max: 2.20e-07, Min: -2.20e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.87e-01, Min: -4.87e-01\nEpoch 78, LR: 0.5000, Train Loss: 2.3007, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.03e-08, Max: 2.18e-07, Min: -2.18e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.86e-01, Min: -4.86e-01\nEpoch 79, LR: 0.5000, Train Loss: 2.3007, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.02e-08, Max: 2.17e-07, Min: -2.17e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.86e-01, Min: -4.86e-01\nEpoch 80, LR: 0.5000, Train Loss: 2.3007, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.02e-08, Max: 2.16e-07, Min: -2.16e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.86e-01, Min: -4.86e-01\nEpoch 81, LR: 0.5000, Train Loss: 2.3007, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.01e-08, Max: 2.14e-07, Min: -2.14e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.86e-01, Min: -4.86e-01\nEpoch 82, LR: 0.5000, Train Loss: 2.3007, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.00e-08, Max: 2.13e-07, Min: -2.13e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.86e-01, Min: -4.86e-01\nEpoch 83, LR: 0.5000, Train Loss: 2.3006, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 9.98e-09, Max: 2.12e-07, Min: -2.12e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.86e-01, Min: -4.86e-01\nEpoch 84, LR: 0.5000, Train Loss: 2.3006, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 9.92e-09, Max: 2.10e-07, Min: -2.10e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.86e-01, Min: -4.86e-01\nEpoch 85, LR: 0.5000, Train Loss: 2.3006, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 9.86e-09, Max: 2.09e-07, Min: -2.09e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.86e-01, Min: -4.86e-01\nEpoch 86, LR: 0.5000, Train Loss: 2.3006, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 9.80e-09, Max: 2.08e-07, Min: -2.08e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.86e-01, Min: -4.86e-01\nEpoch 87, LR: 0.5000, Train Loss: 2.3006, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 9.74e-09, Max: 2.06e-07, Min: -2.06e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.86e-01, Min: -4.86e-01\nEpoch 88, LR: 0.5000, Train Loss: 2.3006, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 9.68e-09, Max: 2.05e-07, Min: -2.05e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.86e-01, Min: -4.86e-01\nEpoch 89, LR: 0.5000, Train Loss: 2.3006, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 9.62e-09, Max: 2.04e-07, Min: -2.04e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.86e-01, Min: -4.86e-01\nEpoch 90, LR: 0.5000, Train Loss: 2.3006, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 9.56e-09, Max: 2.02e-07, Min: -2.02e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.86e-01, Min: -4.86e-01\nEpoch 91, LR: 0.5000, Train Loss: 2.3006, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 9.50e-09, Max: 2.01e-07, Min: -2.01e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.86e-01, Min: -4.86e-01\nEpoch 92, LR: 0.5000, Train Loss: 2.3005, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 9.44e-09, Max: 2.00e-07, Min: -2.00e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.85e-01, Min: -4.85e-01\nEpoch 93, LR: 0.5000, Train Loss: 2.3005, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 9.38e-09, Max: 1.98e-07, Min: -1.98e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.85e-01, Min: -4.85e-01\nEpoch 94, LR: 0.5000, Train Loss: 2.3005, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 9.32e-09, Max: 1.97e-07, Min: -1.97e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.85e-01, Min: -4.85e-01\nEpoch 95, LR: 0.5000, Train Loss: 2.3005, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 9.25e-09, Max: 1.96e-07, Min: -1.96e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.85e-01, Min: -4.85e-01\nEpoch 96, LR: 0.5000, Train Loss: 2.3005, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 9.19e-09, Max: 1.94e-07, Min: -1.94e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.85e-01, Min: -4.85e-01\nEpoch 97, LR: 0.5000, Train Loss: 2.3005, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 9.13e-09, Max: 1.93e-07, Min: -1.93e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.85e-01, Min: -4.85e-01\nEpoch 98, LR: 0.5000, Train Loss: 2.3005, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 9.07e-09, Max: 1.92e-07, Min: -1.92e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.85e-01, Min: -4.85e-01\nEpoch 99, LR: 0.5000, Train Loss: 2.3005, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 9.01e-09, Max: 1.90e-07, Min: -1.90e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.85e-01, Min: -4.85e-01\nEpoch 100, LR: 0.5000, Train Loss: 2.3005, Train Accuracy: 11.58%, Val Loss: 2.3016, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.95e-09, Max: 1.89e-07, Min: -1.89e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.85e-01, Min: -4.85e-01\nEpoch 101, LR: 0.5000, Train Loss: 2.3005, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.89e-09, Max: 1.88e-07, Min: -1.88e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.35e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 102, LR: 0.2500, Train Loss: 2.3004, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.86e-09, Max: 1.87e-07, Min: -1.87e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.35e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 103, LR: 0.2500, Train Loss: 2.3004, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.83e-09, Max: 1.86e-07, Min: -1.86e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.35e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 104, LR: 0.2500, Train Loss: 2.3004, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.80e-09, Max: 1.86e-07, Min: -1.86e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.35e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 105, LR: 0.2500, Train Loss: 2.3004, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.77e-09, Max: 1.85e-07, Min: -1.85e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.35e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 106, LR: 0.2500, Train Loss: 2.3004, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.74e-09, Max: 1.84e-07, Min: -1.84e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.35e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 107, LR: 0.2500, Train Loss: 2.3004, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.71e-09, Max: 1.84e-07, Min: -1.84e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.35e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 108, LR: 0.2500, Train Loss: 2.3004, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.68e-09, Max: 1.83e-07, Min: -1.83e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.35e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 109, LR: 0.2500, Train Loss: 2.3004, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.65e-09, Max: 1.82e-07, Min: -1.82e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.35e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 110, LR: 0.2500, Train Loss: 2.3004, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.61e-09, Max: 1.82e-07, Min: -1.82e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.35e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 111, LR: 0.2500, Train Loss: 2.3004, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.58e-09, Max: 1.81e-07, Min: -1.81e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.35e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 112, LR: 0.2500, Train Loss: 2.3004, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.55e-09, Max: 1.80e-07, Min: -1.80e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.35e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 113, LR: 0.2500, Train Loss: 2.3004, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.52e-09, Max: 1.80e-07, Min: -1.80e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.35e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 114, LR: 0.2500, Train Loss: 2.3004, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.49e-09, Max: 1.79e-07, Min: -1.79e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.35e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 115, LR: 0.2500, Train Loss: 2.3004, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.46e-09, Max: 1.78e-07, Min: -1.78e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.35e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 116, LR: 0.2500, Train Loss: 2.3004, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.43e-09, Max: 1.78e-07, Min: -1.78e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.35e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 117, LR: 0.2500, Train Loss: 2.3004, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.40e-09, Max: 1.77e-07, Min: -1.77e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.35e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 118, LR: 0.2500, Train Loss: 2.3004, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.37e-09, Max: 1.76e-07, Min: -1.76e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.35e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 119, LR: 0.2500, Train Loss: 2.3004, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.34e-09, Max: 1.76e-07, Min: -1.76e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.35e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 120, LR: 0.2500, Train Loss: 2.3004, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.31e-09, Max: 1.75e-07, Min: -1.75e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.35e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 121, LR: 0.2500, Train Loss: 2.3004, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.28e-09, Max: 1.74e-07, Min: -1.74e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.35e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 122, LR: 0.2500, Train Loss: 2.3004, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.25e-09, Max: 1.74e-07, Min: -1.74e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.35e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 123, LR: 0.2500, Train Loss: 2.3003, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.22e-09, Max: 1.73e-07, Min: -1.73e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.35e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 124, LR: 0.2500, Train Loss: 2.3003, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.19e-09, Max: 1.72e-07, Min: -1.72e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.35e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 125, LR: 0.2500, Train Loss: 2.3003, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.16e-09, Max: 1.72e-07, Min: -1.72e-07\nLayer 1 Gradients - Mean Magnitude: 7.33e-03, Max: 3.94e-02, Min: -3.94e-02\nLayer 2 Gradients - Mean Magnitude: 1.35e-02, Max: 3.28e-01, Min: -3.28e-01\nLayer 3 Gradients - Mean Magnitude: 7.35e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 126, LR: 0.2500, Train Loss: 2.3003, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.13e-09, Max: 1.71e-07, Min: -1.71e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.35e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 127, LR: 0.2500, Train Loss: 2.3003, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.10e-09, Max: 1.70e-07, Min: -1.70e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.35e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 128, LR: 0.2500, Train Loss: 2.3003, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.07e-09, Max: 1.70e-07, Min: -1.70e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.35e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 129, LR: 0.2500, Train Loss: 2.3003, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.04e-09, Max: 1.69e-07, Min: -1.69e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.36e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 130, LR: 0.2500, Train Loss: 2.3003, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.00e-09, Max: 1.68e-07, Min: -1.68e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.36e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 131, LR: 0.2500, Train Loss: 2.3003, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 7.97e-09, Max: 1.68e-07, Min: -1.68e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.36e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 132, LR: 0.2500, Train Loss: 2.3003, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 7.94e-09, Max: 1.67e-07, Min: -1.67e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.36e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 133, LR: 0.2500, Train Loss: 2.3003, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 7.91e-09, Max: 1.66e-07, Min: -1.66e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.36e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 134, LR: 0.2500, Train Loss: 2.3003, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 7.88e-09, Max: 1.66e-07, Min: -1.66e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.36e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 135, LR: 0.2500, Train Loss: 2.3003, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 7.85e-09, Max: 1.65e-07, Min: -1.65e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.36e-02, Max: 2.42e-01, Min: -2.42e-01\nEpoch 136, LR: 0.2500, Train Loss: 2.3003, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 7.82e-09, Max: 1.64e-07, Min: -1.64e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.36e-02, Max: 2.41e-01, Min: -2.41e-01\nEpoch 137, LR: 0.2500, Train Loss: 2.3003, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 7.79e-09, Max: 1.64e-07, Min: -1.64e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.36e-02, Max: 2.41e-01, Min: -2.41e-01\nEpoch 138, LR: 0.2500, Train Loss: 2.3003, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 7.76e-09, Max: 1.63e-07, Min: -1.63e-07\nLayer 1 Gradients - Mean Magnitude: 8.23e-05, Max: 3.72e-04, Min: -3.72e-04\nLayer 2 Gradients - Mean Magnitude: 1.35e-02, Max: 3.28e-01, Min: -3.28e-01\nLayer 3 Gradients - Mean Magnitude: 7.36e-02, Max: 2.41e-01, Min: -2.41e-01\nEpoch 139, LR: 0.2500, Train Loss: 2.3003, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 7.73e-09, Max: 1.62e-07, Min: -1.62e-07\nLayer 1 Gradients - Mean Magnitude: 9.72e-03, Max: 1.05e-01, Min: -1.05e-01\nLayer 2 Gradients - Mean Magnitude: 1.11e-02, Max: 5.61e-01, Min: -9.73e-01\nLayer 3 Gradients - Mean Magnitude: 7.36e-02, Max: 2.41e-01, Min: -2.41e-01\nEpoch 140, LR: 0.2500, Train Loss: 2.3003, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 7.70e-09, Max: 1.62e-07, Min: -1.62e-07\nLayer 1 Gradients - Mean Magnitude: 2.26e-02, Max: 1.11e-01, Min: -1.11e-01\nLayer 2 Gradients - Mean Magnitude: 1.36e-02, Max: 3.84e-01, Min: -6.30e-01\nLayer 3 Gradients - Mean Magnitude: 7.36e-02, Max: 2.41e-01, Min: -2.41e-01\nEpoch 141, LR: 0.2500, Train Loss: 2.3003, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 7.67e-09, Max: 1.61e-07, Min: -1.61e-07\nLayer 1 Gradients - Mean Magnitude: 3.41e-02, Max: 1.55e-01, Min: -1.55e-01\nLayer 2 Gradients - Mean Magnitude: 1.42e-02, Max: 3.48e-01, Min: -3.86e-01\nLayer 3 Gradients - Mean Magnitude: 7.36e-02, Max: 2.41e-01, Min: -2.41e-01\nEpoch 142, LR: 0.2500, Train Loss: 2.3003, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 7.64e-09, Max: 1.60e-07, Min: -1.60e-07\nLayer 1 Gradients - Mean Magnitude: 2.04e-02, Max: 1.07e-01, Min: -1.07e-01\nLayer 2 Gradients - Mean Magnitude: 1.38e-02, Max: 4.68e-01, Min: -4.72e-01\nLayer 3 Gradients - Mean Magnitude: 7.36e-02, Max: 2.41e-01, Min: -2.41e-01\nEpoch 143, LR: 0.2500, Train Loss: 2.3003, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 7.61e-09, Max: 1.60e-07, Min: -1.60e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.36e-02, Max: 2.41e-01, Min: -2.41e-01\nEpoch 144, LR: 0.2500, Train Loss: 2.3003, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 7.58e-09, Max: 1.59e-07, Min: -1.59e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.36e-02, Max: 2.41e-01, Min: -2.41e-01\nEpoch 145, LR: 0.2500, Train Loss: 2.3003, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 7.55e-09, Max: 1.58e-07, Min: -1.58e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.36e-02, Max: 2.41e-01, Min: -2.41e-01\nEpoch 146, LR: 0.2500, Train Loss: 2.3003, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 7.52e-09, Max: 1.58e-07, Min: -1.58e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.36e-02, Max: 2.41e-01, Min: -2.41e-01\nEpoch 147, LR: 0.2500, Train Loss: 2.3002, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 7.48e-09, Max: 1.57e-07, Min: -1.57e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.36e-02, Max: 2.41e-01, Min: -2.41e-01\nEpoch 148, LR: 0.2500, Train Loss: 2.3002, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 7.45e-09, Max: 1.56e-07, Min: -1.56e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 7.36e-02, Max: 2.41e-01, Min: -2.41e-01\nEpoch 149, LR: 0.2500, Train Loss: 2.3002, Train Accuracy: 11.58%, Val Loss: 2.3015, Val Accuracy: 11.35%\nEarly stopping triggered at epoch 149\n","output_type":"stream"}],"execution_count":76},{"id":"a07797c4-b96a-4051-9f5a-b2da19ed0cd8","cell_type":"code","source":"(model_RRAM.w>1)*1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:19:43.520192Z","iopub.execute_input":"2025-01-27T07:19:43.520603Z","iopub.status.idle":"2025-01-27T07:19:43.528078Z","shell.execute_reply.started":"2025-01-27T07:19:43.520565Z","shell.execute_reply":"2025-01-27T07:19:43.527262Z"}},"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"tensor([[0, 0, 0,  ..., 0, 1, 0],\n        [1, 0, 1,  ..., 0, 1, 1],\n        [0, 1, 0,  ..., 1, 0, 0],\n        ...,\n        [1, 0, 0,  ..., 0, 0, 0],\n        [0, 1, 1,  ..., 1, 1, 1],\n        [0, 1, 1,  ..., 0, 0, 1]], device='cuda:0')"},"metadata":{}}],"execution_count":78},{"id":"8dc53685-ceb0-4b1c-a03f-04ca24416013","cell_type":"markdown","source":"### Complete Training","metadata":{}},{"id":"75e71bbb-9193-4010-8f9e-70ef9154c911","cell_type":"code","source":"history_RRAM = {\n    \"train_loss\": [],\n    \"train_accuracy\": [],\n    \"val_loss\": [],\n    \"val_accuracy\": []\n}\ncriterion = nn.CrossEntropyLoss().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:19:43.962167Z","iopub.execute_input":"2025-01-27T07:19:43.962369Z","iopub.status.idle":"2025-01-27T07:19:43.965913Z","shell.execute_reply.started":"2025-01-27T07:19:43.962352Z","shell.execute_reply":"2025-01-27T07:19:43.965143Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":79},{"id":"a9ed1842-f618-4562-8e82-c5355ca39f90","cell_type":"code","source":"lr = params_RRAM[\"lr\"]\nnum_epochs = params_RRAM[\"epochs\"]\n# lr = 0.1\n# num_epochs = 200\n\npatience = 10\nmin_val_loss = float('inf')\nstagnant_epochs = 0\n\nfor epoch in range(num_epochs):\n    if epoch == 0:\n        lr /= 25\n    elif epoch <= 2:\n        lr *= 5\n\n    model_RRAM.train()\n    model_RRAM.to(device)\n\n    train_loss = 0\n    train_accuracy = 0\n    total_samples = 0\n\n    for inputs, labels in train_loader:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        outputs = model_RRAM(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        model_RRAM.backprop(lr)\n\n        train_loss += loss.item() * inputs.size(0)\n        _, predictions = torch.max(outputs, dim=1)\n        train_accuracy += (predictions == labels).sum().item()\n        total_samples += inputs.size(0)\n\n    train_loss /= total_samples\n    train_accuracy = (train_accuracy / total_samples) * 100\n\n    # Evaluate on the test set\n    model_RRAM.eval()\n    test_loss = 0\n    test_accuracy = 0\n    total_test_samples = 0\n\n    with torch.no_grad():\n        for test_inputs, test_labels in test_loader:\n            test_inputs = test_inputs.to(device)\n            test_labels = test_labels.to(device)\n\n            test_outputs = model_RRAM(test_inputs)\n            loss = criterion(test_outputs, test_labels)\n\n            test_loss += loss.item() * test_inputs.size(0)\n            _, test_predictions = torch.max(test_outputs, dim=1)\n            test_accuracy += (test_predictions == test_labels).sum().item()\n            total_test_samples += test_inputs.size(0)\n\n    test_loss /= total_test_samples\n    test_accuracy = (test_accuracy / total_test_samples) * 100\n\n    history_RRAM[\"train_loss\"].append(train_loss)\n    history_RRAM[\"train_accuracy\"].append(train_accuracy)\n    history_RRAM[\"val_loss\"].append(test_loss)\n    history_RRAM[\"val_accuracy\"].append(test_accuracy)\n\n    if test_accuracy > val_best:\n        val_best = test_accuracy\n        torch.save(model_RRAM.state_dict(), f\"Best_model.pth\")\n        with open(f\"Best_Val_Accuracy.txt\", \"w\") as f: \n            f.write(f\"{val_best:.6f}\")\n        with open(f\"Best_Params.txt\", \"w\") as f: \n            f.write(f\"{params_RRAM}\")\n        print(f\"Model saved with Validation Accuracy: {val_best:.6f}\")\n\n    print(f\"Epoch {epoch + 1}, LR: {lr:.4f}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, \"\n              f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n\n    if test_loss < min_val_loss:\n        min_val_loss = test_loss\n        stagnant_epochs = 0\n    else:\n        stagnant_epochs += 1\n\n    if stagnant_epochs >= patience:\n        print(f\"Early stopping triggered at epoch {epoch + 1}\")\n        break\n\n    if (epoch) % (10) == 0 and epoch != 0:\n        lr /= 2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:19:44.179770Z","iopub.execute_input":"2025-01-27T07:19:44.179993Z","iopub.status.idle":"2025-01-27T07:24:52.579894Z","shell.execute_reply.started":"2025-01-27T07:19:44.179974Z","shell.execute_reply":"2025-01-27T07:24:52.578743Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Original Grads - Mean Magnitude: 1.13e-08, Max: 1.92e-07, Min: -1.92e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.25e-02, Max: 3.32e-02, Min: -3.32e-02\nOriginal Grads - Mean Magnitude: 8.58e-09, Max: 2.06e-07, Min: -2.06e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.12e-02, Max: 4.21e-02, Min: -4.21e-02\nOriginal Grads - Mean Magnitude: 1.18e-08, Max: 2.48e-07, Min: -2.48e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.20e-02, Max: 3.94e-02, Min: -3.94e-02\nOriginal Grads - Mean Magnitude: 7.65e-09, Max: 1.60e-07, Min: -1.60e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.11e-02, Max: 3.62e-02, Min: -3.62e-02\nOriginal Grads - Mean Magnitude: 1.30e-08, Max: 2.30e-07, Min: -2.30e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.17e-02, Max: 3.22e-02, Min: -3.22e-02\nOriginal Grads - Mean Magnitude: 9.19e-09, Max: 2.02e-07, Min: -2.02e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.09e-02, Max: 3.75e-02, Min: -3.75e-02\nOriginal Grads - Mean Magnitude: 4.80e-09, Max: 1.00e-07, Min: -1.00e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.20e-02, Max: 3.89e-02, Min: -3.89e-02\nOriginal Grads - Mean Magnitude: 1.02e-08, Max: 1.98e-07, Min: -1.98e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.17e-02, Max: 3.54e-02, Min: -3.54e-02\nOriginal Grads - Mean Magnitude: 9.38e-09, Max: 1.80e-07, Min: -1.80e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.18e-02, Max: 3.56e-02, Min: -3.56e-02\nOriginal Grads - Mean Magnitude: 1.01e-08, Max: 2.39e-07, Min: -2.39e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.09e-02, Max: 4.03e-02, Min: -4.03e-02\nEpoch 1, LR: 0.0400, Train Loss: 2.3014, Train Accuracy: 11.24%, Test Loss: 2.3015, Test Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.07e-08, Max: 1.64e-07, Min: -1.64e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 6.12e-02, Max: 1.46e-01, Min: -1.46e-01\nOriginal Grads - Mean Magnitude: 6.90e-09, Max: 1.25e-07, Min: -1.25e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 5.75e-02, Max: 1.63e-01, Min: -1.63e-01\nOriginal Grads - Mean Magnitude: 6.94e-09, Max: 1.25e-07, Min: -1.25e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 6.12e-02, Max: 1.72e-01, Min: -1.72e-01\nOriginal Grads - Mean Magnitude: 1.23e-08, Max: 2.23e-07, Min: -2.23e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 5.59e-02, Max: 1.59e-01, Min: -1.59e-01\nOriginal Grads - Mean Magnitude: 1.47e-08, Max: 2.69e-07, Min: -2.69e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 6.33e-02, Max: 1.81e-01, Min: -1.81e-01\nOriginal Grads - Mean Magnitude: 1.03e-08, Max: 2.04e-07, Min: -2.04e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 5.74e-02, Max: 1.78e-01, Min: -1.78e-01\nOriginal Grads - Mean Magnitude: 1.13e-08, Max: 2.28e-07, Min: -2.28e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 5.82e-02, Max: 1.84e-01, Min: -1.84e-01\nOriginal Grads - Mean Magnitude: 7.16e-09, Max: 1.80e-07, Min: -1.80e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 5.41e-02, Max: 2.12e-01, Min: -2.12e-01\nOriginal Grads - Mean Magnitude: 1.01e-08, Max: 1.47e-07, Min: -1.47e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 6.42e-02, Max: 1.47e-01, Min: -1.47e-01\nOriginal Grads - Mean Magnitude: 9.34e-09, Max: 2.19e-07, Min: -2.19e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 5.57e-02, Max: 2.04e-01, Min: -2.04e-01\nEpoch 2, LR: 0.2000, Train Loss: 2.3014, Train Accuracy: 11.24%, Test Loss: 2.3015, Test Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 6.40e-09, Max: 1.58e-07, Min: -1.58e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.67e-01, Max: 1.03e+00, Min: -1.03e+00\nOriginal Grads - Mean Magnitude: 1.31e-08, Max: 2.37e-07, Min: -2.37e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.03e-01, Max: 8.55e-01, Min: -8.55e-01\nOriginal Grads - Mean Magnitude: 8.58e-09, Max: 2.26e-07, Min: -2.26e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.59e-01, Max: 1.07e+00, Min: -1.07e+00\nOriginal Grads - Mean Magnitude: 1.52e-08, Max: 3.32e-07, Min: -3.32e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.94e-01, Max: 1.00e+00, Min: -1.00e+00\nOriginal Grads - Mean Magnitude: 6.60e-09, Max: 1.34e-07, Min: -1.34e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.94e-01, Max: 9.30e-01, Min: -9.30e-01\nOriginal Grads - Mean Magnitude: 1.10e-08, Max: 1.57e-07, Min: -1.57e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.05e-01, Max: 6.77e-01, Min: -6.77e-01\nOriginal Grads - Mean Magnitude: 6.64e-09, Max: 1.71e-07, Min: -1.71e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.70e-01, Max: 1.09e+00, Min: -1.09e+00\nOriginal Grads - Mean Magnitude: 9.29e-09, Max: 1.68e-07, Min: -1.68e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.07e-01, Max: 8.69e-01, Min: -8.69e-01\nOriginal Grads - Mean Magnitude: 8.95e-09, Max: 2.12e-07, Min: -2.12e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.66e-01, Max: 9.86e-01, Min: -9.86e-01\nOriginal Grads - Mean Magnitude: 7.86e-09, Max: 1.34e-07, Min: -1.34e-07\nLayer 1 Gradients - Mean Magnitude: 2.31e-02, Max: 8.93e-02, Min: -8.93e-02\nLayer 2 Gradients - Mean Magnitude: 5.46e-02, Max: 1.33e+00, Min: -1.33e+00\nLayer 3 Gradients - Mean Magnitude: 3.07e-01, Max: 8.17e-01, Min: -8.17e-01\nEpoch 3, LR: 1.0000, Train Loss: 2.3014, Train Accuracy: 11.24%, Test Loss: 2.3014, Test Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 7.88e-09, Max: 1.56e-07, Min: -1.56e-07\nLayer 1 Gradients - Mean Magnitude: 1.32e-01, Max: 7.26e-01, Min: -7.26e-01\nLayer 2 Gradients - Mean Magnitude: 5.51e-02, Max: 1.91e+00, Min: -1.82e+00\nLayer 3 Gradients - Mean Magnitude: 2.96e-01, Max: 9.16e-01, Min: -9.16e-01\nOriginal Grads - Mean Magnitude: 6.28e-09, Max: 1.18e-07, Min: -1.18e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.94e-01, Max: 8.64e-01, Min: -8.64e-01\nOriginal Grads - Mean Magnitude: 1.11e-08, Max: 2.44e-07, Min: -2.44e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.73e-01, Max: 9.41e-01, Min: -9.41e-01\nOriginal Grads - Mean Magnitude: 9.27e-09, Max: 1.43e-07, Min: -1.43e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.17e-01, Max: 7.65e-01, Min: -7.65e-01\nOriginal Grads - Mean Magnitude: 1.69e-08, Max: 2.31e-07, Min: -2.31e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.33e-01, Max: 7.09e-01, Min: -7.09e-01\nOriginal Grads - Mean Magnitude: 5.60e-09, Max: 1.47e-07, Min: -1.47e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.39e-01, Max: 9.77e-01, Min: -9.77e-01\nOriginal Grads - Mean Magnitude: 8.51e-09, Max: 2.40e-07, Min: -2.40e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.51e-01, Max: 1.10e+00, Min: -1.10e+00\nOriginal Grads - Mean Magnitude: 8.96e-09, Max: 1.85e-07, Min: -1.85e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.95e-01, Max: 9.51e-01, Min: -9.51e-01\nOriginal Grads - Mean Magnitude: 7.18e-09, Max: 1.22e-07, Min: -1.22e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.20e-01, Max: 8.47e-01, Min: -8.47e-01\nOriginal Grads - Mean Magnitude: 5.37e-09, Max: 1.03e-07, Min: -1.03e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.75e-01, Max: 8.19e-01, Min: -8.19e-01\nEpoch 4, LR: 1.0000, Train Loss: 2.3013, Train Accuracy: 11.24%, Test Loss: 2.3014, Test Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 1.08e-08, Max: 1.56e-07, Min: -1.56e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.05e-01, Max: 6.91e-01, Min: -6.91e-01\nOriginal Grads - Mean Magnitude: 7.66e-09, Max: 1.39e-07, Min: -1.39e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.17e-01, Max: 9.00e-01, Min: -9.00e-01\nOriginal Grads - Mean Magnitude: 8.60e-09, Max: 1.77e-07, Min: -1.77e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.77e-01, Max: 8.90e-01, Min: -8.90e-01\nOriginal Grads - Mean Magnitude: 1.14e-08, Max: 1.83e-07, Min: -1.83e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.98e-01, Max: 7.45e-01, Min: -7.45e-01\nOriginal Grads - Mean Magnitude: 1.24e-08, Max: 1.90e-07, Min: -1.90e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.08e-01, Max: 7.37e-01, Min: -7.37e-01\nOriginal Grads - Mean Magnitude: 1.43e-08, Max: 2.20e-07, Min: -2.20e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.06e-01, Max: 7.34e-01, Min: -7.34e-01\nOriginal Grads - Mean Magnitude: 1.07e-08, Max: 2.20e-07, Min: -2.20e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.87e-01, Max: 9.23e-01, Min: -9.23e-01\nOriginal Grads - Mean Magnitude: 8.30e-09, Max: 1.57e-07, Min: -1.57e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 8.64e-01, Min: -8.64e-01\nOriginal Grads - Mean Magnitude: 1.18e-08, Max: 2.43e-07, Min: -2.43e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.83e-01, Max: 9.09e-01, Min: -9.09e-01\nOriginal Grads - Mean Magnitude: 9.73e-09, Max: 2.07e-07, Min: -2.07e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.78e-01, Max: 9.24e-01, Min: -9.24e-01\nEpoch 5, LR: 1.0000, Train Loss: 2.3013, Train Accuracy: 11.24%, Test Loss: 2.3013, Test Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 9.80e-09, Max: 1.28e-07, Min: -1.28e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.23e-01, Max: 6.60e-01, Min: -6.60e-01\nOriginal Grads - Mean Magnitude: 1.09e-08, Max: 3.20e-07, Min: -3.20e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.63e-01, Max: 1.21e+00, Min: -1.21e+00\nOriginal Grads - Mean Magnitude: 1.05e-08, Max: 2.12e-07, Min: -2.12e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.02e-01, Max: 9.53e-01, Min: -9.53e-01\nOriginal Grads - Mean Magnitude: 1.03e-08, Max: 1.94e-07, Min: -1.94e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.02e-01, Max: 8.94e-01, Min: -8.94e-01\nOriginal Grads - Mean Magnitude: 7.07e-09, Max: 9.71e-08, Min: -9.71e-08\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.30e-01, Max: 7.08e-01, Min: -7.08e-01\nOriginal Grads - Mean Magnitude: 1.06e-08, Max: 1.32e-07, Min: -1.32e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.34e-01, Max: 6.49e-01, Min: -6.49e-01\nOriginal Grads - Mean Magnitude: 6.15e-09, Max: 1.71e-07, Min: -1.71e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.54e-01, Max: 1.10e+00, Min: -1.10e+00\nOriginal Grads - Mean Magnitude: 1.12e-08, Max: 1.92e-07, Min: -1.92e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.09e-01, Max: 8.29e-01, Min: -8.29e-01\nOriginal Grads - Mean Magnitude: 6.63e-09, Max: 1.54e-07, Min: -1.54e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.55e-01, Max: 9.22e-01, Min: -9.22e-01\nOriginal Grads - Mean Magnitude: 8.72e-09, Max: 1.74e-07, Min: -1.74e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 9.15e-01, Min: -9.15e-01\nEpoch 6, LR: 1.0000, Train Loss: 2.3013, Train Accuracy: 11.24%, Test Loss: 2.3013, Test Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 7.91e-09, Max: 1.87e-07, Min: -1.87e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.78e-01, Max: 1.02e+00, Min: -1.02e+00\nOriginal Grads - Mean Magnitude: 7.03e-09, Max: 8.61e-08, Min: -8.61e-08\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.29e-01, Max: 6.30e-01, Min: -6.30e-01\nOriginal Grads - Mean Magnitude: 7.31e-09, Max: 1.12e-07, Min: -1.12e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.07e-01, Max: 7.35e-01, Min: -7.35e-01\nOriginal Grads - Mean Magnitude: 1.10e-08, Max: 2.91e-07, Min: -2.91e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.77e-01, Max: 1.14e+00, Min: -1.14e+00\nOriginal Grads - Mean Magnitude: 8.39e-09, Max: 2.22e-07, Min: -2.22e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.65e-01, Max: 1.10e+00, Min: -1.10e+00\nOriginal Grads - Mean Magnitude: 8.57e-09, Max: 1.26e-07, Min: -1.26e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.04e-01, Max: 6.96e-01, Min: -6.96e-01\nOriginal Grads - Mean Magnitude: 9.91e-09, Max: 1.35e-07, Min: -1.35e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.22e-01, Max: 6.84e-01, Min: -6.84e-01\nOriginal Grads - Mean Magnitude: 1.12e-08, Max: 2.30e-07, Min: -2.30e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.90e-01, Max: 9.31e-01, Min: -9.31e-01\nOriginal Grads - Mean Magnitude: 1.08e-08, Max: 1.49e-07, Min: -1.49e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.19e-01, Max: 6.88e-01, Min: -6.88e-01\nOriginal Grads - Mean Magnitude: 8.32e-09, Max: 1.37e-07, Min: -1.37e-07\nLayer 1 Gradients - Mean Magnitude: 1.38e-02, Max: 3.22e-01, Min: -3.22e-01\nLayer 2 Gradients - Mean Magnitude: 5.46e-02, Max: 1.30e+00, Min: -1.30e+00\nLayer 3 Gradients - Mean Magnitude: 2.97e-01, Max: 7.64e-01, Min: -7.64e-01\nEpoch 7, LR: 1.0000, Train Loss: 2.3013, Train Accuracy: 11.24%, Test Loss: 2.3012, Test Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.89e-09, Max: 2.00e-07, Min: -2.00e-07\nLayer 1 Gradients - Mean Magnitude: 5.20e-02, Max: 1.72e+00, Min: -1.72e+00\nLayer 2 Gradients - Mean Magnitude: 5.62e-02, Max: 1.51e+00, Min: -1.61e+00\nLayer 3 Gradients - Mean Magnitude: 2.76e-01, Max: 9.71e-01, Min: -9.71e-01\nOriginal Grads - Mean Magnitude: 1.11e-08, Max: 1.74e-07, Min: -1.74e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.10e-01, Max: 7.61e-01, Min: -7.61e-01\nOriginal Grads - Mean Magnitude: 7.55e-09, Max: 1.41e-07, Min: -1.41e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.99e-01, Max: 8.72e-01, Min: -8.72e-01\nOriginal Grads - Mean Magnitude: 7.48e-09, Max: 1.05e-07, Min: -1.05e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.23e-01, Max: 7.09e-01, Min: -7.09e-01\nOriginal Grads - Mean Magnitude: 9.00e-09, Max: 1.84e-07, Min: -1.84e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.88e-01, Max: 9.19e-01, Min: -9.19e-01\nOriginal Grads - Mean Magnitude: 4.86e-09, Max: 1.09e-07, Min: -1.09e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.81e-01, Max: 9.84e-01, Min: -9.84e-01\nOriginal Grads - Mean Magnitude: 8.11e-09, Max: 2.03e-07, Min: -2.03e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.67e-01, Max: 1.04e+00, Min: -1.04e+00\nOriginal Grads - Mean Magnitude: 1.01e-08, Max: 1.52e-07, Min: -1.52e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.16e-01, Max: 7.44e-01, Min: -7.44e-01\nOriginal Grads - Mean Magnitude: 8.61e-09, Max: 1.71e-07, Min: -1.71e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.02e-01, Max: 9.39e-01, Min: -9.39e-01\nOriginal Grads - Mean Magnitude: 5.57e-09, Max: 1.10e-07, Min: -1.10e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.80e-01, Max: 8.61e-01, Min: -8.61e-01\nEpoch 8, LR: 1.0000, Train Loss: 2.3012, Train Accuracy: 11.24%, Test Loss: 2.3012, Test Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 7.70e-09, Max: 1.29e-07, Min: -1.29e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.15e-01, Max: 8.24e-01, Min: -8.24e-01\nOriginal Grads - Mean Magnitude: 4.78e-09, Max: 1.30e-07, Min: -1.30e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.57e-01, Max: 1.10e+00, Min: -1.10e+00\nOriginal Grads - Mean Magnitude: 8.07e-09, Max: 1.36e-07, Min: -1.36e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.99e-01, Max: 7.88e-01, Min: -7.88e-01\nOriginal Grads - Mean Magnitude: 5.59e-09, Max: 8.77e-08, Min: -8.77e-08\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.10e-01, Max: 7.60e-01, Min: -7.60e-01\nOriginal Grads - Mean Magnitude: 9.07e-09, Max: 1.74e-07, Min: -1.74e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.88e-01, Max: 8.62e-01, Min: -8.62e-01\nOriginal Grads - Mean Magnitude: 7.95e-09, Max: 2.02e-07, Min: -2.02e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.62e-01, Max: 1.04e+00, Min: -1.04e+00\nOriginal Grads - Mean Magnitude: 6.23e-09, Max: 1.84e-07, Min: -1.84e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.45e-01, Max: 1.13e+00, Min: -1.13e+00\nOriginal Grads - Mean Magnitude: 8.54e-09, Max: 1.42e-07, Min: -1.42e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.12e-01, Max: 8.09e-01, Min: -8.09e-01\nOriginal Grads - Mean Magnitude: 6.31e-09, Max: 8.24e-08, Min: -8.24e-08\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.29e-01, Max: 6.72e-01, Min: -6.72e-01\nOriginal Grads - Mean Magnitude: 1.05e-08, Max: 1.55e-07, Min: -1.55e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.12e-01, Max: 7.20e-01, Min: -7.20e-01\nEpoch 9, LR: 1.0000, Train Loss: 2.3012, Train Accuracy: 11.24%, Test Loss: 2.3012, Test Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 6.53e-09, Max: 1.29e-07, Min: -1.29e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.86e-01, Max: 8.83e-01, Min: -8.83e-01\nOriginal Grads - Mean Magnitude: 9.97e-09, Max: 2.20e-07, Min: -2.20e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.97e-01, Max: 1.02e+00, Min: -1.02e+00\nOriginal Grads - Mean Magnitude: 1.09e-08, Max: 1.97e-07, Min: -1.97e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.10e-01, Max: 8.78e-01, Min: -8.78e-01\nOriginal Grads - Mean Magnitude: 6.20e-09, Max: 9.82e-08, Min: -9.82e-08\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.19e-01, Max: 7.90e-01, Min: -7.90e-01\nOriginal Grads - Mean Magnitude: 6.92e-09, Max: 1.44e-07, Min: -1.44e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.66e-01, Max: 8.66e-01, Min: -8.66e-01\nOriginal Grads - Mean Magnitude: 3.31e-09, Max: 6.32e-08, Min: -6.32e-08\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.93e-01, Max: 8.75e-01, Min: -8.75e-01\nOriginal Grads - Mean Magnitude: 5.56e-09, Max: 9.73e-08, Min: -9.73e-08\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.00e-01, Max: 8.20e-01, Min: -8.20e-01\nOriginal Grads - Mean Magnitude: 9.74e-09, Max: 2.09e-07, Min: -2.09e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.83e-01, Max: 9.50e-01, Min: -9.50e-01\nOriginal Grads - Mean Magnitude: 9.92e-09, Max: 1.49e-07, Min: -1.49e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.09e-01, Max: 7.24e-01, Min: -7.24e-01\nOriginal Grads - Mean Magnitude: 8.55e-09, Max: 1.80e-07, Min: -1.80e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.86e-01, Max: 9.44e-01, Min: -9.44e-01\nEpoch 10, LR: 1.0000, Train Loss: 2.3012, Train Accuracy: 11.24%, Test Loss: 2.3011, Test Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 7.19e-09, Max: 1.18e-07, Min: -1.18e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.07e-01, Max: 7.87e-01, Min: -7.87e-01\nOriginal Grads - Mean Magnitude: 8.16e-09, Max: 1.63e-07, Min: -1.63e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.15e-01, Max: 9.80e-01, Min: -9.80e-01\nOriginal Grads - Mean Magnitude: 7.40e-09, Max: 1.16e-07, Min: -1.16e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.16e-01, Max: 7.72e-01, Min: -7.72e-01\nOriginal Grads - Mean Magnitude: 9.17e-09, Max: 1.41e-07, Min: -1.41e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.05e-01, Max: 7.34e-01, Min: -7.34e-01\nOriginal Grads - Mean Magnitude: 9.61e-09, Max: 1.48e-07, Min: -1.48e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.17e-01, Max: 7.62e-01, Min: -7.62e-01\nOriginal Grads - Mean Magnitude: 5.69e-09, Max: 1.11e-07, Min: -1.11e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.01e-01, Max: 9.17e-01, Min: -9.17e-01\nOriginal Grads - Mean Magnitude: 9.35e-09, Max: 2.84e-07, Min: -2.84e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.55e-01, Max: 1.21e+00, Min: -1.21e+00\nOriginal Grads - Mean Magnitude: 1.21e-08, Max: 2.63e-07, Min: -2.63e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.99e-01, Max: 1.01e+00, Min: -1.01e+00\nOriginal Grads - Mean Magnitude: 6.75e-09, Max: 1.18e-07, Min: -1.18e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 3.16e-01, Max: 8.61e-01, Min: -8.61e-01\nOriginal Grads - Mean Magnitude: 8.15e-09, Max: 1.51e-07, Min: -1.51e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 2.98e-01, Max: 8.61e-01, Min: -8.61e-01\nEpoch 11, LR: 1.0000, Train Loss: 2.3012, Train Accuracy: 11.24%, Test Loss: 2.3011, Test Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 7.18e-09, Max: 1.89e-07, Min: -1.89e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.36e-01, Max: 5.61e-01, Min: -5.61e-01\nOriginal Grads - Mean Magnitude: 5.49e-09, Max: 9.51e-08, Min: -9.51e-08\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.58e-01, Max: 4.29e-01, Min: -4.29e-01\nOriginal Grads - Mean Magnitude: 9.27e-09, Max: 1.80e-07, Min: -1.80e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.46e-01, Max: 4.43e-01, Min: -4.43e-01\nOriginal Grads - Mean Magnitude: 7.31e-09, Max: 1.74e-07, Min: -1.74e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.42e-01, Max: 5.27e-01, Min: -5.27e-01\nOriginal Grads - Mean Magnitude: 7.10e-09, Max: 1.92e-07, Min: -1.92e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.30e-01, Max: 5.49e-01, Min: -5.49e-01\nOriginal Grads - Mean Magnitude: 8.07e-09, Max: 1.29e-07, Min: -1.29e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.49e-01, Max: 3.74e-01, Min: -3.74e-01\nOriginal Grads - Mean Magnitude: 1.01e-08, Max: 1.59e-07, Min: -1.59e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.45e-01, Max: 3.58e-01, Min: -3.58e-01\nOriginal Grads - Mean Magnitude: 9.70e-09, Max: 2.44e-07, Min: -2.44e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.27e-01, Max: 4.99e-01, Min: -4.99e-01\nOriginal Grads - Mean Magnitude: 4.93e-09, Max: 1.45e-07, Min: -1.45e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.27e-01, Max: 5.84e-01, Min: -5.84e-01\nOriginal Grads - Mean Magnitude: 6.26e-09, Max: 1.54e-07, Min: -1.54e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.30e-01, Max: 4.97e-01, Min: -4.97e-01\nEpoch 12, LR: 0.5000, Train Loss: 2.3012, Train Accuracy: 11.24%, Test Loss: 2.3011, Test Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 7.46e-09, Max: 1.90e-07, Min: -1.90e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.35e-01, Max: 5.40e-01, Min: -5.40e-01\nOriginal Grads - Mean Magnitude: 6.66e-09, Max: 1.29e-07, Min: -1.29e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.34e-01, Max: 4.05e-01, Min: -4.05e-01\nOriginal Grads - Mean Magnitude: 1.10e-08, Max: 1.90e-07, Min: -1.90e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.53e-01, Max: 4.12e-01, Min: -4.12e-01\nOriginal Grads - Mean Magnitude: 9.97e-09, Max: 1.40e-07, Min: -1.40e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.58e-01, Max: 3.46e-01, Min: -3.46e-01\nOriginal Grads - Mean Magnitude: 8.16e-09, Max: 2.27e-07, Min: -2.27e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.30e-01, Max: 5.65e-01, Min: -5.65e-01\nOriginal Grads - Mean Magnitude: 5.97e-09, Max: 1.12e-07, Min: -1.12e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.32e-01, Min: -4.32e-01\nOriginal Grads - Mean Magnitude: 1.03e-08, Max: 2.48e-07, Min: -2.48e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.34e-01, Max: 5.05e-01, Min: -5.05e-01\nOriginal Grads - Mean Magnitude: 1.01e-08, Max: 1.92e-07, Min: -1.92e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.44e-01, Max: 4.28e-01, Min: -4.28e-01\nOriginal Grads - Mean Magnitude: 8.18e-09, Max: 2.27e-07, Min: -2.27e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.36e-01, Max: 5.90e-01, Min: -5.90e-01\nOriginal Grads - Mean Magnitude: 8.08e-09, Max: 1.45e-07, Min: -1.45e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.11e-01, Min: -4.11e-01\nEpoch 13, LR: 0.5000, Train Loss: 2.3012, Train Accuracy: 11.24%, Test Loss: 2.3011, Test Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 9.28e-09, Max: 1.57e-07, Min: -1.57e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.55e-01, Max: 4.12e-01, Min: -4.12e-01\nOriginal Grads - Mean Magnitude: 8.22e-09, Max: 1.85e-07, Min: -1.85e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.45e-01, Max: 5.10e-01, Min: -5.10e-01\nOriginal Grads - Mean Magnitude: 8.24e-09, Max: 1.37e-07, Min: -1.37e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.58e-01, Max: 4.10e-01, Min: -4.10e-01\nOriginal Grads - Mean Magnitude: 6.30e-09, Max: 1.10e-07, Min: -1.10e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 4.00e-01, Min: -4.00e-01\nOriginal Grads - Mean Magnitude: 1.01e-08, Max: 1.51e-07, Min: -1.51e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.56e-01, Max: 3.66e-01, Min: -3.66e-01\nOriginal Grads - Mean Magnitude: 8.59e-09, Max: 1.44e-07, Min: -1.44e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.53e-01, Max: 3.99e-01, Min: -3.99e-01\nOriginal Grads - Mean Magnitude: 1.03e-08, Max: 1.39e-07, Min: -1.39e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.65e-01, Max: 3.49e-01, Min: -3.49e-01\nOriginal Grads - Mean Magnitude: 6.11e-09, Max: 1.22e-07, Min: -1.22e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.42e-01, Max: 4.42e-01, Min: -4.42e-01\nOriginal Grads - Mean Magnitude: 7.84e-09, Max: 1.40e-07, Min: -1.40e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.49e-01, Max: 4.13e-01, Min: -4.13e-01\nOriginal Grads - Mean Magnitude: 1.13e-08, Max: 1.71e-07, Min: -1.71e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.55e-01, Max: 3.69e-01, Min: -3.69e-01\nEpoch 14, LR: 0.5000, Train Loss: 2.3012, Train Accuracy: 11.24%, Test Loss: 2.3011, Test Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 9.89e-09, Max: 1.53e-07, Min: -1.53e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.61e-01, Max: 3.89e-01, Min: -3.89e-01\nOriginal Grads - Mean Magnitude: 6.31e-09, Max: 1.16e-07, Min: -1.16e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.49e-01, Max: 4.28e-01, Min: -4.28e-01\nOriginal Grads - Mean Magnitude: 9.36e-09, Max: 1.67e-07, Min: -1.67e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.54e-01, Max: 4.29e-01, Min: -4.29e-01\nOriginal Grads - Mean Magnitude: 1.14e-08, Max: 1.98e-07, Min: -1.98e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.54e-01, Max: 4.20e-01, Min: -4.20e-01\nOriginal Grads - Mean Magnitude: 6.83e-09, Max: 2.30e-07, Min: -2.30e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.17e-01, Max: 6.13e-01, Min: -6.13e-01\nOriginal Grads - Mean Magnitude: 7.68e-09, Max: 2.32e-07, Min: -2.32e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.24e-01, Max: 5.83e-01, Min: -5.83e-01\nOriginal Grads - Mean Magnitude: 7.15e-09, Max: 1.36e-07, Min: -1.36e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.51e-01, Max: 4.48e-01, Min: -4.48e-01\nOriginal Grads - Mean Magnitude: 9.83e-09, Max: 2.00e-07, Min: -2.00e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.45e-01, Max: 4.58e-01, Min: -4.58e-01\nOriginal Grads - Mean Magnitude: 9.32e-09, Max: 1.82e-07, Min: -1.82e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.52e-01, Max: 4.65e-01, Min: -4.65e-01\nOriginal Grads - Mean Magnitude: 7.83e-09, Max: 1.37e-07, Min: -1.37e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.52e-01, Max: 4.16e-01, Min: -4.16e-01\nEpoch 15, LR: 0.5000, Train Loss: 2.3012, Train Accuracy: 11.24%, Test Loss: 2.3011, Test Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 9.59e-09, Max: 2.68e-07, Min: -2.68e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.33e-01, Max: 5.81e-01, Min: -5.81e-01\nOriginal Grads - Mean Magnitude: 6.82e-09, Max: 1.86e-07, Min: -1.86e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.29e-01, Max: 5.47e-01, Min: -5.47e-01\nOriginal Grads - Mean Magnitude: 4.76e-09, Max: 1.41e-07, Min: -1.41e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.26e-01, Max: 5.83e-01, Min: -5.83e-01\nOriginal Grads - Mean Magnitude: 8.57e-09, Max: 1.68e-07, Min: -1.68e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.51e-01, Max: 4.62e-01, Min: -4.62e-01\nOriginal Grads - Mean Magnitude: 4.48e-09, Max: 1.03e-07, Min: -1.03e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.34e-01, Max: 4.81e-01, Min: -4.81e-01\nOriginal Grads - Mean Magnitude: 9.90e-09, Max: 1.78e-07, Min: -1.78e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.43e-01, Max: 4.01e-01, Min: -4.01e-01\nOriginal Grads - Mean Magnitude: 6.31e-09, Max: 1.33e-07, Min: -1.33e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.43e-01, Max: 4.70e-01, Min: -4.70e-01\nOriginal Grads - Mean Magnitude: 8.91e-09, Max: 1.85e-07, Min: -1.85e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.44e-01, Max: 4.67e-01, Min: -4.67e-01\nOriginal Grads - Mean Magnitude: 6.12e-09, Max: 9.92e-08, Min: -9.92e-08\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.55e-01, Max: 3.94e-01, Min: -3.94e-01\nOriginal Grads - Mean Magnitude: 9.62e-09, Max: 1.40e-07, Min: -1.40e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.57e-01, Max: 3.58e-01, Min: -3.58e-01\nEpoch 16, LR: 0.5000, Train Loss: 2.3012, Train Accuracy: 11.24%, Test Loss: 2.3011, Test Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.51e-09, Max: 2.15e-07, Min: -2.15e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.40e-01, Max: 5.53e-01, Min: -5.53e-01\nOriginal Grads - Mean Magnitude: 8.70e-09, Max: 2.00e-07, Min: -2.00e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.45e-01, Max: 5.20e-01, Min: -5.20e-01\nOriginal Grads - Mean Magnitude: 1.05e-08, Max: 2.30e-07, Min: -2.30e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.41e-01, Max: 4.83e-01, Min: -4.83e-01\nOriginal Grads - Mean Magnitude: 1.02e-08, Max: 1.71e-07, Min: -1.71e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.59e-01, Max: 4.13e-01, Min: -4.13e-01\nOriginal Grads - Mean Magnitude: 8.40e-09, Max: 2.51e-07, Min: -2.51e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.27e-01, Max: 5.91e-01, Min: -5.91e-01\nOriginal Grads - Mean Magnitude: 5.54e-09, Max: 1.37e-07, Min: -1.37e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.40e-01, Max: 5.42e-01, Min: -5.42e-01\nOriginal Grads - Mean Magnitude: 1.03e-08, Max: 2.80e-07, Min: -2.80e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.31e-01, Max: 5.53e-01, Min: -5.53e-01\nOriginal Grads - Mean Magnitude: 7.45e-09, Max: 1.16e-07, Min: -1.16e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.54e-01, Max: 3.75e-01, Min: -3.75e-01\nOriginal Grads - Mean Magnitude: 9.27e-09, Max: 1.54e-07, Min: -1.54e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.47e-01, Max: 3.82e-01, Min: -3.82e-01\nOriginal Grads - Mean Magnitude: 9.15e-09, Max: 2.53e-07, Min: -2.53e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.22e-01, Max: 5.28e-01, Min: -5.28e-01\nEpoch 17, LR: 0.5000, Train Loss: 2.3012, Train Accuracy: 11.24%, Test Loss: 2.3011, Test Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 8.01e-09, Max: 1.41e-07, Min: -1.41e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.52e-01, Max: 4.18e-01, Min: -4.18e-01\nOriginal Grads - Mean Magnitude: 7.30e-09, Max: 1.28e-07, Min: -1.28e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.50e-01, Max: 4.09e-01, Min: -4.09e-01\nOriginal Grads - Mean Magnitude: 1.14e-08, Max: 1.64e-07, Min: -1.64e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.57e-01, Max: 3.51e-01, Min: -3.51e-01\nOriginal Grads - Mean Magnitude: 9.34e-09, Max: 1.89e-07, Min: -1.89e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.41e-01, Max: 4.47e-01, Min: -4.47e-01\nOriginal Grads - Mean Magnitude: 7.26e-09, Max: 1.44e-07, Min: -1.44e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.49e-01, Max: 4.61e-01, Min: -4.61e-01\nOriginal Grads - Mean Magnitude: 6.93e-09, Max: 1.41e-07, Min: -1.41e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.38e-01, Max: 4.41e-01, Min: -4.41e-01\nOriginal Grads - Mean Magnitude: 9.29e-09, Max: 1.70e-07, Min: -1.70e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.48e-01, Max: 4.24e-01, Min: -4.24e-01\nOriginal Grads - Mean Magnitude: 7.29e-09, Max: 1.27e-07, Min: -1.27e-07\nLayer 1 Gradients - Mean Magnitude: 8.33e-04, Max: 4.01e-03, Min: -4.01e-03\nLayer 2 Gradients - Mean Magnitude: 2.70e-02, Max: 6.57e-01, Min: -6.57e-01\nLayer 3 Gradients - Mean Magnitude: 1.45e-01, Max: 3.95e-01, Min: -3.95e-01\nOriginal Grads - Mean Magnitude: 9.27e-09, Max: 1.61e-07, Min: -1.61e-07\nLayer 1 Gradients - Mean Magnitude: 6.12e-02, Max: 2.52e-01, Min: -2.52e-01\nLayer 2 Gradients - Mean Magnitude: 2.82e-02, Max: 7.81e-01, Min: -7.27e-01\nLayer 3 Gradients - Mean Magnitude: 1.50e-01, Max: 4.08e-01, Min: -4.08e-01\nOriginal Grads - Mean Magnitude: 1.36e-08, Max: 2.34e-07, Min: -2.34e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.56e-01, Max: 4.19e-01, Min: -4.19e-01\nEpoch 18, LR: 0.5000, Train Loss: 2.3012, Train Accuracy: 11.24%, Test Loss: 2.3011, Test Accuracy: 11.35%\nOriginal Grads - Mean Magnitude: 7.08e-09, Max: 1.38e-07, Min: -1.38e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.50e-01, Max: 4.57e-01, Min: -4.57e-01\nOriginal Grads - Mean Magnitude: 8.42e-09, Max: 1.40e-07, Min: -1.40e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.59e-01, Max: 4.13e-01, Min: -4.13e-01\nOriginal Grads - Mean Magnitude: 1.01e-08, Max: 1.88e-07, Min: -1.88e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.55e-01, Max: 4.52e-01, Min: -4.52e-01\nOriginal Grads - Mean Magnitude: 1.17e-08, Max: 1.66e-07, Min: -1.66e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.63e-01, Max: 3.62e-01, Min: -3.62e-01\nOriginal Grads - Mean Magnitude: 6.73e-09, Max: 1.57e-07, Min: -1.57e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.45e-01, Max: 5.29e-01, Min: -5.29e-01\nOriginal Grads - Mean Magnitude: 1.26e-08, Max: 1.83e-07, Min: -1.83e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.54e-01, Max: 3.50e-01, Min: -3.50e-01\nOriginal Grads - Mean Magnitude: 8.41e-09, Max: 1.81e-07, Min: -1.81e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.44e-01, Max: 4.86e-01, Min: -4.86e-01\nOriginal Grads - Mean Magnitude: 7.06e-09, Max: 1.51e-07, Min: -1.51e-07\nLayer 1 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 2 Gradients - Mean Magnitude: 0.00e+00, Max: 0.00e+00, Min: 0.00e+00\nLayer 3 Gradients - Mean Magnitude: 1.39e-01, Max: 4.64e-01, Min: -4.64e-01\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-80-a79998a92a6e>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-9356b22d4fb3>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise_std\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":80},{"id":"070a2ef8-94c5-49ca-bd35-3ed75ae2327b","cell_type":"code","source":"plot_history(history_RRAM, num_epochs, \"RRAM\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:24:52.580332Z","iopub.status.idle":"2025-01-27T07:24:52.580595Z","shell.execute_reply":"2025-01-27T07:24:52.580492Z"}},"outputs":[],"execution_count":null},{"id":"cedc935c-7196-4221-b4aa-783ce1e195f3","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom itertools import combinations\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define the recurrent encoding layer\nclass RecurrentEncodingLayer(nn.Module):\n    def __init__(self, input_size=784, recurrence_size=32, data_in=28, crossbar=(64, 64), V_1=0.1, V_0=-0.1, \n                 R_INV_1=5e3, G_ON=6e-5, G_OFF=3e-6, zeta=100, V_IN = 1):\n        super(RecurrentEncodingLayer, self).__init__()\n        \n        self.input_size = input_size\n        self.recurrence_size = recurrence_size\n        self.data_in = data_in\n        self.first_bias = crossbar[0] - recurrence_size - data_in\n        self.r_passes = input_size // data_in\n\n        self.V_1 = torch.tensor(V_1, device=device)\n        self.V_0 = torch.tensor(V_0, device=device)\n        self.R_INV_1 = torch.tensor(R_INV_1, device=device)\n        self.G_ON = torch.tensor(G_ON, device=device)\n        self.G_OFF = torch.tensor(G_OFF, device=device)\n        self.zeta = torch.tensor(zeta, device=device)\n        self.V_IN = torch.tensor(V_IN, device=device)\n\n        self.w = nn.Parameter(torch.empty(crossbar, device=device))\n        nn.init.xavier_uniform_(self.w)\n\n    def INV_AMP(self, x, R_INV):\n        return -self.V_IN * torch.tanh(R_INV * x / self.V_IN)\n\n    def SOFT_BIN(self, x):\n        return ((self.G_ON - self.G_OFF) * torch.sigmoid(x * self.zeta) + self.G_OFF)\n\n    def PREPROCESS(self, img):\n        return (self.V_1 - self.V_0) * img.view(img.size(0), -1).to(device) + self.V_0\n\n    def forward(self, img):\n        img = self.PREPROCESS(img)\n        g = self.SOFT_BIN(self.w)\n\n        feedback = self.PREPROCESS(torch.zeros((img.shape[0], self.recurrence_size), device=device))\n        bias = self.PREPROCESS(torch.tensor([1, -1] * (self.first_bias // 2), device=device).float().unsqueeze(0).repeat(img.shape[0], 1))\n\n        for r_pass in range(self.r_passes):\n            x = torch.cat((feedback, bias, img[:, r_pass * self.data_in:(r_pass + 1) * self.data_in]), dim=1)\n            x = F.linear(x, g[-self.recurrence_size:, :], bias=None)\n            feedback = self.INV_AMP(x, self.R_INV_1)\n        \n        return feedback\n\n# Contrastive loss function\nclass ContrastiveLoss(nn.Module):\n    def __init__(self, margin=2.0):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, embedding1, embedding2, label):\n        distance = F.pairwise_distance(embedding1, embedding2)\n        loss = (1 - label) * distance.pow(2) + label * F.relu(self.margin - distance).pow(2)\n        return loss.mean()\n\n# Load MNIST dataset\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\ntrain_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n# Initialize model, loss, and optimizer\nmodel = RecurrentEncodingLayer().to(device)\ncriterion = ContrastiveLoss(margin=2.0)\noptimizer = optim.Adam(model.parameters(), lr=100.0)\n\n# Function to create pairs and labels\ndef create_pairs(images, labels):\n    pairs = []\n    pair_labels = []\n    for i, j in combinations(range(len(labels)), 2):\n        pairs.append((images[i], images[j]))\n        pair_labels.append(0 if labels[i] == labels[j] else 1)  # 0 = same, 1 = different\n    return pairs, pair_labels\n\n# Training loop\nepochs = 20\nfor epoch in range(epochs):\n    epoch_loss = 0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        \n        pairs, pair_labels = create_pairs(images, labels)\n        \n        img1, img2 = zip(*pairs)\n        img1 = torch.stack(img1).to(device)\n        img2 = torch.stack(img2).to(device)\n        pair_labels = torch.tensor(pair_labels, dtype=torch.float32).to(device)\n\n        optimizer.zero_grad()\n        embedding1 = model(img1)\n        embedding2 = model(img2)\n\n        loss = criterion(embedding1, embedding2, pair_labels)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n\nprint(\"Training complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:16:52.451805Z","iopub.execute_input":"2025-01-27T08:16:52.452093Z"}},"outputs":[],"execution_count":null},{"id":"18eb9a00-69d5-4b69-906f-2cdf348d4b9e","cell_type":"code","source":"model(img1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c93e55bc-1bae-4483-a3cf-01cde188dc69","cell_type":"markdown","source":"## Testing","metadata":{}},{"id":"af52e53e-15ed-441f-8412-d94c5d6a0bdb","cell_type":"markdown","source":"### Current Model","metadata":{}},{"id":"ad288f0b-0ec9-415d-96cc-a4ecec7aa5ce","cell_type":"markdown","source":"## PWL Generation\n\nLet's assume that we will program the two crossbars with seperate PWLs. That is, during programming, we will cut the Inverting Amplifier stages with a pass transistor and connect the programming lines with a pass transistor. First array has 16 Top PWLs and 8 Bottom PWLs. Second array has 8 Top PWLs and 4 Bottom PWLs. And then once the programming switch is toggled to inference mode, only the 16 Top PWLs are to be changed. Let's also generate a PWL for that too.\n\nIn the code below, we will first maintain tuples for each PWL that holds what the voltage should be. And then we will write a function that will take there and space pulses of the given voltage that are 100us apart from other and have an ON duration of 100us ","metadata":{}},{"id":"58d5cebb-fc99-4a00-87a1-e183838c9a64","cell_type":"code","source":"WL_FC1 = [list() for i in range(16)]\nBL_FC1 = [list() for i in range(8)]\nWL_FC2 = [list() for i in range(8)]\nBL_FC2 = [list() for i in range(4)]\nMode = []\nMode_B = []\n\nV_WRITE = 1.5\nV_READ = 0.1\nV_mode = 1.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:06:30.025111Z","iopub.status.idle":"2025-01-27T07:06:30.025421Z","shell.execute_reply":"2025-01-27T07:06:30.025280Z"}},"outputs":[],"execution_count":null},{"id":"c647afda-e1a7-47a7-b634-212ec5527be2","cell_type":"markdown","source":"#### Fully Connected Weights 1","metadata":{}},{"id":"03334172-89f6-4d73-9461-6ac922bdb6ea","cell_type":"code","source":"target = (model_RRAM_best.w1>0).int()\ntarget","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:06:30.026323Z","iopub.status.idle":"2025-01-27T07:06:30.026606Z","shell.execute_reply":"2025-01-27T07:06:30.026494Z"}},"outputs":[],"execution_count":null},{"id":"49287afe-8437-40aa-8825-93f2cffe3a66","cell_type":"code","source":"for ind_i, i in enumerate(target):\n    for ind_j, j in enumerate(i):\n        if j==1: WL_FC1[ind_j].append(V_WRITE)\n        else: WL_FC1[ind_j].append(V_WRITE/3)\n    for ind_k in range(len(target)): \n        if ind_k==ind_i: BL_FC1[ind_i].append(0)\n        else: BL_FC1[ind_k].append(2*V_WRITE/3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:06:30.027602Z","iopub.status.idle":"2025-01-27T07:06:30.027979Z","shell.execute_reply":"2025-01-27T07:06:30.027814Z"}},"outputs":[],"execution_count":null},{"id":"be77131c-005d-4f06-8a70-f40d267eb67e","cell_type":"markdown","source":"#### Fully Connected Weights 2","metadata":{}},{"id":"9adc2b24-ffb0-4470-8eae-3830221cb33d","cell_type":"code","source":"target = (model_RRAM_best.w2>0).int()\ntarget","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:06:30.028957Z","iopub.status.idle":"2025-01-27T07:06:30.029380Z","shell.execute_reply":"2025-01-27T07:06:30.029190Z"}},"outputs":[],"execution_count":null},{"id":"3f7ca037-0e2f-49a3-98b3-b2e0b93f8b79","cell_type":"code","source":"for ind_i, i in enumerate(target):\n    for ind_j, j in enumerate(i):\n        if j==1: WL_FC2[ind_j].append(V_WRITE)\n        else: WL_FC2[ind_j].append(V_WRITE/3)\n    for ind_k in range(len(target)): \n        if ind_k==ind_i: BL_FC2[ind_i].append(0)\n        else: BL_FC2[ind_k].append(2*V_WRITE/3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:06:30.030346Z","iopub.status.idle":"2025-01-27T07:06:30.030747Z","shell.execute_reply":"2025-01-27T07:06:30.030582Z"}},"outputs":[],"execution_count":null},{"id":"a602368f-07a2-4beb-90e3-d782ca717fea","cell_type":"markdown","source":"#### Filling Out Programming Mode","metadata":{}},{"id":"c4aa24d2-0d95-4ff4-8dfd-5c0800d9de11","cell_type":"code","source":"WL_FC1 = [i + [0,0] for i in WL_FC1]\nBL_FC1 = [i + [0,0] for i in BL_FC1]\nwhile(len(WL_FC2[0]) < len(WL_FC1[0])):\n    WL_FC2 = [i + [0,] for i in WL_FC2]\n    BL_FC2 = [i + [0,] for i in BL_FC2]\nMode.extend([V_mode]*(len(WL_FC1[0])-1) + [-V_mode])\nMode_B.extend([-V_mode]*(len(WL_FC1[0])-1) + [V_mode])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:06:30.031645Z","iopub.status.idle":"2025-01-27T07:06:30.032038Z","shell.execute_reply":"2025-01-27T07:06:30.031867Z"}},"outputs":[],"execution_count":null},{"id":"f953198a-a107-428c-9624-3c712086e922","cell_type":"code","source":"print(WL_FC1[0])\nprint(BL_FC1[0])\nprint(WL_FC2[0])\nprint(BL_FC2[0])\nprint(Mode)\nprint(Mode_B)\nprint(len(WL_FC1[0]), len(BL_FC1[0]), len(WL_FC2[0]), len(BL_FC2[0]), len(Mode), len(Mode_B)) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:06:30.032829Z","iopub.status.idle":"2025-01-27T07:06:30.033199Z","shell.execute_reply":"2025-01-27T07:06:30.033039Z"}},"outputs":[],"execution_count":null},{"id":"92339f84-e8d6-4280-92af-e5215aa129a0","cell_type":"markdown","source":"### Inference: Loading the Testing Set","metadata":{}},{"id":"b97dacd2-42bb-49f0-8ca9-e0f02ca4cc58","cell_type":"code","source":"val_inputs[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:06:30.034018Z","iopub.status.idle":"2025-01-27T07:06:30.034401Z","shell.execute_reply":"2025-01-27T07:06:30.034229Z"}},"outputs":[],"execution_count":null},{"id":"2336c3ab-2181-4ee7-b1c9-0a97a3700248","cell_type":"code","source":"V_1 = 0.1\nV_0 = -0.1\ninclude_testing = True\ninclude_every = 4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:06:30.037054Z","iopub.status.idle":"2025-01-27T07:06:30.037350Z","shell.execute_reply":"2025-01-27T07:06:30.037246Z"}},"outputs":[],"execution_count":null},{"id":"6a1cfba2-3243-4062-9ef6-4cd309622c73","cell_type":"code","source":"if include_testing:\n    for i in val_inputs[::include_every]:\n        i = i.flatten()\n        for ind, j in enumerate(i):\n            WL_FC1[ind].append(V_1 if j==1 else V_0)\n        BL_FC1 = [i + [0,] for i in BL_FC1]\n        WL_FC2 = [i + [0,] for i in WL_FC2]\n        BL_FC2 = [i + [0,] for i in BL_FC2]\n        Mode = Mode + [-V_mode,]\n        Mode_B = Mode_B + [V_mode,]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:06:30.038216Z","iopub.status.idle":"2025-01-27T07:06:30.038556Z","shell.execute_reply":"2025-01-27T07:06:30.038398Z"}},"outputs":[],"execution_count":null},{"id":"b9dd97b8-fb1b-4634-9ae4-3a6fe05d0a2c","cell_type":"markdown","source":"### PWL Convertion","metadata":{}},{"id":"c98af5ff-8f34-4472-89fc-e308d98d4072","cell_type":"code","source":"def pwl(l):\n    t = 0\n    res = \"pwl(time, 0us, 0V\"\n    for i in l:\n        res += f\", {t+5}us, {i:.2f}V, {t+100}us, {i:.2f}V, {t+105}us, 0V, {t+200}us, 0V\"\n        t+=200\n    res += \")\"\n    return res","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:06:30.039691Z","iopub.status.idle":"2025-01-27T07:06:30.039997Z","shell.execute_reply":"2025-01-27T07:06:30.039870Z"}},"outputs":[],"execution_count":null},{"id":"54850dbc-1317-44a2-9093-c5a4e048094c","cell_type":"code","source":"pwl_data = []\n\nfor ind, i in enumerate(WL_FC1):\n    pwl_data.append({\"Signal\": f\"WL_FC1_{ind}\", \"Index\": ind, \"PWL\": pwl(i)})\nfor ind, i in enumerate(BL_FC1):\n    pwl_data.append({\"Signal\": f\"BL_FC1_{ind}\", \"Index\": ind, \"PWL\": pwl(i)})\nfor ind, i in enumerate(WL_FC2):\n    pwl_data.append({\"Signal\": f\"WL_FC2_{ind}\", \"Index\": ind, \"PWL\": pwl(i)})\nfor ind, i in enumerate(BL_FC2):\n    pwl_data.append({\"Signal\": f\"BL_FC2_{ind}\", \"Index\": ind, \"PWL\": pwl(i)})\npwl_data.append({\"Signal\": \"Mode\", \"Index\": \"\", \"PWL\": pwl(Mode)})\npwl_data.append({\"Signal\": \"Mode_b\", \"Index\": \"\", \"PWL\": pwl(Mode_B)})\n\npwl_data = pd.DataFrame(pwl_data)\npwl_data.to_csv(\"pwl_data.csv\", index=False)\npwl_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:06:30.040976Z","iopub.status.idle":"2025-01-27T07:06:30.041360Z","shell.execute_reply":"2025-01-27T07:06:30.041197Z"}},"outputs":[],"execution_count":null},{"id":"03e13866-8cd8-4b8e-a8d0-a5c69e951174","cell_type":"markdown","source":"#### Testing Accuracy on 160 Images\nADS isn't allowing PWLs longer than 160 Images, so let's check software accuracy for the same too","metadata":{}},{"id":"fd4e4e97-5ecd-4c41-9035-4a51015d684e","cell_type":"code","source":"test(model_RRAM_best, val_inputs[::4], val_labels[::4])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:06:30.042430Z","iopub.status.idle":"2025-01-27T07:06:30.042831Z","shell.execute_reply":"2025-01-27T07:06:30.042666Z"}},"outputs":[],"execution_count":null},{"id":"7cf44b69-14ac-4fdd-a2b5-b9c2cc361e7f","cell_type":"code","source":"test(model_RRAM_best, train_inputs, train_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:06:30.043594Z","iopub.status.idle":"2025-01-27T07:06:30.043966Z","shell.execute_reply":"2025-01-27T07:06:30.043803Z"}},"outputs":[],"execution_count":null},{"id":"1079074f-8781-43af-b52e-fb3581ef8931","cell_type":"markdown","source":"## Simulation Data from ADS","metadata":{}},{"id":"e0d523a2-f99f-46bb-93e0-fead2768da75","cell_type":"code","source":"simu = pd.read_csv(\"Testing_160_Images.csv\")\nsimu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:06:30.044735Z","iopub.status.idle":"2025-01-27T07:06:30.045049Z","shell.execute_reply":"2025-01-27T07:06:30.044899Z"}},"outputs":[],"execution_count":null},{"id":"fd96e845-0c19-470b-8910-6b45df78022c","cell_type":"code","source":"def remove_units(value):\n    return float(value.replace('E', 'e').split('V')[0].replace('sec', ''))\n\nsimu['time'] = simu['time'].apply(remove_units)\nfor col in ['A', 'X', 'V', 'T']:\n    simu[col] = simu[col].apply(remove_units)\nsimu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:06:30.045928Z","iopub.status.idle":"2025-01-27T07:06:30.046249Z","shell.execute_reply":"2025-01-27T07:06:30.046131Z"}},"outputs":[],"execution_count":null},{"id":"223576d6-266c-49bd-aaca-ff3444617f09","cell_type":"markdown","source":"We just need one sample every 0.1ms samples of these starting from 2.050ms to 33.850ms","metadata":{}},{"id":"a13d6e19-daa6-481f-8ad2-11a357dc20d3","cell_type":"code","source":"t_stamps = np.arange(2.05e-3, 33.9e-3, 0.2e-3)\nt_stamps.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:06:30.048195Z","iopub.status.idle":"2025-01-27T07:06:30.048560Z","shell.execute_reply":"2025-01-27T07:06:30.048386Z"}},"outputs":[],"execution_count":null},{"id":"f5548dd9-e376-4bd1-b9d2-484f4358f700","cell_type":"code","source":"sampled = []\nwindow = 0.02e-3\n\nfor t in t_stamps:\n    filtered = simu[(simu['time'] >= t - window) & (simu['time'] <= t + window)]\n    \n    avg_A = filtered['A'].mean()\n    avg_X = filtered['X'].mean()\n    avg_V = filtered['V'].mean()\n    avg_T = filtered['T'].mean()\n    \n    sampled.append({\n        'Image Index': t,\n        'A': avg_A,\n        'X': avg_X,\n        'V': avg_V,\n        'T': avg_T\n    })\n\nsampled = pd.DataFrame(sampled)\nsampled","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:06:30.049373Z","iopub.status.idle":"2025-01-27T07:06:30.049700Z","shell.execute_reply":"2025-01-27T07:06:30.049556Z"}},"outputs":[],"execution_count":null},{"id":"096828ea-b6be-4839-8b7d-4e0f5a515a7e","cell_type":"code","source":"def get_max_column(row):\n    return row[['A', 'X', 'V', 'T']].idxmax()\nsampled['Predicted Class'] = sampled.apply(get_max_column, axis=1)\nsampled.to_csv(\"Sampled_Results.csv\", index=False)\nsampled","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:06:30.050862Z","iopub.status.idle":"2025-01-27T07:06:30.051187Z","shell.execute_reply":"2025-01-27T07:06:30.051047Z"}},"outputs":[],"execution_count":null},{"id":"d1143c06-5631-4c79-9965-0cb937929706","cell_type":"code","source":"ground_truth = ['A']*40 + ['X']*40 + ['V']*40 + ['T']*40\ncorrect_predictions = sampled['Predicted Class'] == ground_truth\naccuracy = correct_predictions.sum() / len(ground_truth)\nprint(accuracy*100,end=\"%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:06:30.052054Z","iopub.status.idle":"2025-01-27T07:06:30.052395Z","shell.execute_reply":"2025-01-27T07:06:30.052227Z"}},"outputs":[],"execution_count":null},{"id":"918b4552-b0d3-4768-bfe7-9b874959d42c","cell_type":"code","source":"plt.figure(figsize=(7, 3.5))\n\nplt.scatter(sampled.index, sampled['A'], color='red', label='A_pred', s=30, marker='o')  # Red dots for A\nplt.scatter(sampled.index, sampled['X'], color='blue', label='X_pred', s=30, marker='o')  # Blue dots for X\nplt.scatter(sampled.index, sampled['T'], color='green', label='T_pred', s=30, marker='o')  # Green dots for T\nplt.scatter(sampled.index, sampled['V'], color='orange', label='V_pred', s=30, marker='o')  # Orange dots for V\n\nplt.xlabel('Image Index')\nplt.ylabel('Predicted Voltages (V)')\nplt.legend()\n\nplt.axvline(x=40, color='gray', linestyle='--', linewidth=2)\nplt.axvline(x=80, color='gray', linestyle='--', linewidth=2)\nplt.axvline(x=120, color='gray', linestyle='--', linewidth=2)\n\nplt.text(20, plt.ylim()[1]*(-0.8), 'A', fontsize=15, color='black', ha='center')\nplt.text(60, plt.ylim()[1]*0.8, 'X', fontsize=15, color='black', ha='center')\nplt.text(100, plt.ylim()[1]*0.8, 'V', fontsize=15, color='black', ha='center')\nplt.text(140, plt.ylim()[1]*(-0.8), 'T', fontsize=15, color='black', ha='center')\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T07:06:30.053187Z","iopub.status.idle":"2025-01-27T07:06:30.053504Z","shell.execute_reply":"2025-01-27T07:06:30.053365Z"}},"outputs":[],"execution_count":null},{"id":"e86f31ef-ad69-472f-9217-bdde6f8ab1e8","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}