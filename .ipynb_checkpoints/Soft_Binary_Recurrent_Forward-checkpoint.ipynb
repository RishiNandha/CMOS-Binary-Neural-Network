{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "141370fc-54a4-4a6b-afcf-a7677dc6dc87",
   "metadata": {
    "id": "141370fc-54a4-4a6b-afcf-a7677dc6dc87"
   },
   "source": [
    "# Soft Binary Neural Network with Recurrent Crossbar Recycling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508058d8-e23a-4c29-aad7-c2b233d621c9",
   "metadata": {
    "id": "508058d8-e23a-4c29-aad7-c2b233d621c9"
   },
   "source": [
    "## Imports and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a70e539-1dc9-4e36-9c9f-18fbdaeede1f",
   "metadata": {
    "id": "9a70e539-1dc9-4e36-9c9f-18fbdaeede1f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d406d54c-db27-4536-a8c1-f46437f6fb71",
   "metadata": {
    "id": "d406d54c-db27-4536-a8c1-f46437f6fb71"
   },
   "outputs": [],
   "source": [
    "def plot_history(history, num_epochs, element):\n",
    "    epochs = range(len(history[list(history.keys())[0]]))\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    ax1.plot(epochs, history[\"train_loss\"], label=\"Train Loss\", color=\"blue\")\n",
    "    ax1.plot(epochs, history[\"val_loss\"], label=\"Validation Loss\", color=\"red\")\n",
    "    ax1.set_xlabel(\"Epochs\", fontsize=14)\n",
    "    ax1.set_ylabel(\"Loss\", fontsize=14, color=\"blue\")\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=\"blue\")\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(epochs, history[\"train_accuracy\"], label=\"Train Accuracy\", color=\"green\")\n",
    "    ax2.plot(epochs, history[\"val_accuracy\"], label=\"Validation Accuracy\", color=\"orange\")\n",
    "    ax2.set_ylabel(\"Accuracy (%)\", fontsize=14, color=\"green\")\n",
    "    ax2.tick_params(axis=\"y\", labelcolor=\"green\")\n",
    "    ax2.legend(loc=\"upper right\")\n",
    "\n",
    "    plt.title(f\"Training and Validation Metrics for {element}\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c658023-75df-4754-a617-a8ba6d08d068",
   "metadata": {
    "id": "7c658023-75df-4754-a617-a8ba6d08d068"
   },
   "outputs": [],
   "source": [
    "def test(model, test_loader, class_names=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = (total_correct / total_samples) * 100\n",
    "\n",
    "    print(f\"Validation Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5e8ced-6bb5-445a-9b04-bc268eec917e",
   "metadata": {
    "id": "be5e8ced-6bb5-445a-9b04-bc268eec917e"
   },
   "source": [
    "### MNIST Handwritten Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3bbad17-7067-4f06-8755-012646ca9567",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3bbad17-7067-4f06-8755-012646ca9567",
    "outputId": "f1798d9d-41cf-4c7c-810b-01a97006c6d8"
   },
   "outputs": [],
   "source": [
    "class BinarizeAndAddNoiseTransform:\n",
    "    def __init__(self, threshold, noise_std):\n",
    "        self.threshold = threshold\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = transforms.ToTensor()(img).to(device)\n",
    "        img = (img > self.threshold).float()\n",
    "        img = img[:,1:-1, 1:-1]\n",
    "        noise = torch.randn(img.size(), device=device) * self.noise_std\n",
    "        noisy_img = img + noise\n",
    "        return noisy_img\n",
    "\n",
    "binary_noise_transform = transforms.Compose([\n",
    "    BinarizeAndAddNoiseTransform(threshold=0.48, noise_std=0.05)\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=binary_noise_transform)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1000, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=binary_noise_transform)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=10000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc40653e-ebac-4013-9ecb-27dfe1370edd",
   "metadata": {
    "id": "bc40653e-ebac-4013-9ecb-27dfe1370edd"
   },
   "outputs": [],
   "source": [
    "# Get a subset of the dataset\n",
    "train_in, train_lab = next(iter(train_loader))\n",
    "val_in, val_lab = next(iter(test_loader))\n",
    "\n",
    "# Move data to the appropriate device\n",
    "train_in, train_lab = train_in.to(device), train_lab.to(device)\n",
    "val_in, val_lab = val_in.to(device), val_lab.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d76599e3-3d87-4d1d-bd5f-41ca0adab18f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "d76599e3-3d87-4d1d-bd5f-41ca0adab18f",
    "outputId": "1b72a9c5-d410-4fd7-d135-0495307455db"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1, 26, 26])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=10, figsize=(15, 4))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(train_in[i].cpu().squeeze(), cmap='gray')\n",
    "    ax.set_title(f\"Label: {train_lab[i].item()}\", fontsize=10)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "train_in.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b44def-df85-406b-87e8-fbc5b4f7fe7a",
   "metadata": {
    "id": "d1b44def-df85-406b-87e8-fbc5b4f7fe7a"
   },
   "source": [
    "## Custom Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "391dfb3d-1a2b-42d7-9ff7-e3f0e831d50a",
   "metadata": {
    "id": "391dfb3d-1a2b-42d7-9ff7-e3f0e831d50a"
   },
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def tensor_stats(tensor, name=\"Tensor\"):\n",
    "    tensor = tensor.to(device)\n",
    "    mean_magnitude = tensor.abs().mean().item()\n",
    "    print(f\"{name} - Mean Magnitude: {mean_magnitude:.2e}, Max: {tensor.max().item():.2e}, Min: {tensor.min().item():.2e}\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SoftBinaryRecurrentForwardNetwork(nn.Module):\n",
    "    def __init__(self, scaling, G_ON, G_OFF, V_INV, R_INV, V_1, V_0, zeta, initial_factor, crossbar=(64,64),\n",
    "                 input_size=676, encoding_size=4, output_size=10, data_in=52, bin_active=True,\n",
    "                 monitor_volts=False, monitor_grads=True, monitor_latents=False, dropout=0.01,\n",
    "                 int_lr=0.01, int_norm=True, temperature_1 = 500, temperature_2 = 10000,monitor_annealing=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.w = nn.Parameter(initial_factor * torch.empty(crossbar, device=device))\n",
    "        nn.init.xavier_uniform_(self.w)\n",
    "        self.w.data = (initial_factor*(self.w.data))\n",
    "\n",
    "        self.G_ON, self.G_OFF = torch.tensor(G_ON, device=device)*scaling, torch.tensor(G_OFF, device=device)*scaling\n",
    "        self.V_INV, self.R_INV = torch.tensor(V_INV, device=device), torch.tensor(R_INV, device=device)\n",
    "        self.V_1, self.V_0 = torch.tensor(V_1, device=device), torch.tensor(V_0, device=device)\n",
    "\n",
    "        self.crossbar_in, self.crossbar_out = crossbar\n",
    "        self.encoding, self.data_in, self.output_size = encoding_size, data_in, output_size\n",
    "        self.r_passes = input_size // data_in\n",
    "\n",
    "        self.first_bias = (crossbar[0] - data_in) % encoding_size\n",
    "        self.extra_final = crossbar[1] - self.encoding * self.r_passes - output_size\n",
    "        self.final_bias = (crossbar[0] - self.encoding * self.r_passes - self.extra_final) % (self.extra_final + self.encoding)\n",
    "\n",
    "        self.feed_repeats = (crossbar[0] - data_in)//encoding_size\n",
    "        self.final_repeats = (crossbar[0] - self.encoding * self.r_passes - self.extra_final)//(self.extra_final + self.encoding)\n",
    "\n",
    "        self.zeta, self.int_lr = torch.tensor(zeta, device=device), torch.tensor(int_lr, device=device)\n",
    "        self.bin_active, self.int_norm = bin_active, int_norm\n",
    "        self.monitor_volts, self.monitor_grads, self.monitor_latents = monitor_volts, monitor_grads, monitor_latents\n",
    "        self.monitor_annealing = monitor_annealing\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.temperature_1 = temperature_1\n",
    "        self.temperature_2 = temperature_2\n",
    "        self.device = device\n",
    "\n",
    "    def INV_AMP(self, x, R_INV):\n",
    "        return -self.V_INV * torch.tanh(R_INV * x / self.V_INV)\n",
    "\n",
    "    def SOFT_BIN(self, x):\n",
    "        if self.bin_active: return ((self.G_ON - self.G_OFF) * torch.sigmoid(x * self.zeta) + self.G_OFF)\n",
    "        else: return self.G_ON * x * self.zeta * 0.4\n",
    "\n",
    "    def PREPROCESS(self, img):\n",
    "        return (self.V_1 - self.V_0) * img.to(device) + self.V_0\n",
    "\n",
    "    def ANNEALER(self):\n",
    "        prob = torch.exp(torch.tensor(-1.0, device=self.device) / self.temperature_1)\n",
    "        prob = torch.clamp(prob, min=1e-3, max=1)\n",
    "\n",
    "        rand_vals = torch.rand((64, 64), device=self.device)\n",
    "        annealed_mask = torch.where(rand_vals < prob, -1, torch.where(rand_vals < 2 * prob, 0, 1))\n",
    "\n",
    "        return annealed_mask\n",
    "\n",
    "    def forward(self, img):\n",
    "        # Preprocessing: Two States of input (V_ON and V_OFF)\n",
    "        img = self.PREPROCESS(img.view(img.size(0), -1))\n",
    "        bias = self.PREPROCESS(((-1) ** torch.arange(self.first_bias, device=device)).repeat(img.shape[0], 1))\n",
    "        bias2 = self.PREPROCESS(((-1) ** torch.arange(self.final_bias, device=device)).repeat(img.shape[0], 1))\n",
    "\n",
    "        # RRAM Soft Binarization\n",
    "        g = self.SOFT_BIN(self.w)\n",
    "        if self.monitor_latents: tensor_stats(self.w, \"Latent Weights:\")\n",
    "\n",
    "        # Recurrent Encoding Layer\n",
    "        out1size = self.crossbar_out - self.output_size\n",
    "        feedback = torch.zeros((img.shape[0], self.encoding*self.feed_repeats), device=device)\n",
    "        out1 = torch.zeros((img.shape[0], out1size), device = device)\n",
    "\n",
    "        for r_pass in range(self.r_passes - 1):\n",
    "            ind_s, ind_f = self.crossbar_out - (r_pass+1)*self.encoding, self.crossbar_out - (r_pass)*self.encoding\n",
    "            ind_a, ind_b = out1size - (r_pass+1)*self.encoding, out1size - (r_pass)*self.encoding\n",
    "\n",
    "            x = torch.cat((feedback, bias, img[:, r_pass * self.data_in:(r_pass + 1) * self.data_in]), dim=1)\n",
    "            x = F.linear(x, g[ind_s:ind_f, : ], bias=None)\n",
    "\n",
    "            out1[:, ind_a:ind_b] = self.INV_AMP(x, self.R_INV)\n",
    "            if self.monitor_volts: tensor_stats(feedback, f\"Voltages in Recurrent Stage after pass {r_pass}\")\n",
    "\n",
    "            feedback = out1[:, ind_a:ind_b].repeat(1,self.feed_repeats)\n",
    "\n",
    "        else:\n",
    "            r_pass += 1\n",
    "            ind_s, ind_f = self.crossbar_out - (r_pass+1)*self.encoding - self.extra_final, self.crossbar_out - (r_pass)*self.encoding\n",
    "            ind_a, ind_b = out1size - (r_pass+1)*self.encoding - self.extra_final, out1size - (r_pass)*self.encoding\n",
    "\n",
    "            x = torch.cat((feedback, bias, img[:, r_pass * self.data_in:(r_pass + 1) * self.data_in]), dim=1)\n",
    "            x = F.linear(x, g[-(r_pass+1)*self.encoding - self.extra_final:-r_pass*self.encoding, : ], bias=None)\n",
    "\n",
    "            out1[:, ind_a:ind_b] = self.INV_AMP(x, self.R_INV)\n",
    "            if self.monitor_volts: tensor_stats(out1, f\"All Voltages in Recurrent Stage\")\n",
    "\n",
    "            feedback = out1[:, ind_a:ind_b].repeat(1,self.final_repeats)\n",
    "\n",
    "        x = torch.cat((feedback, bias2, out1), dim = 1)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Feature Extraction Layer\n",
    "        x = F.linear(x, g[:self.output_size, : ], bias=None)\n",
    "        x = self.INV_AMP(x, self.R_INV)\n",
    "        if self.monitor_volts: tensor_stats(x, f\"Voltages after h_layer {h_pass}\")\n",
    "\n",
    "        return x\n",
    "\n",
    "    def backprop(self, ext_lr):\n",
    "        with torch.no_grad():\n",
    "            if self.w.grad is not None:\n",
    "                grad = self.w.grad.to(device)\n",
    "                for i in range(grad.shape[0]):\n",
    "                    if self.int_norm:\n",
    "                        grad[i] = self.int_lr * grad[i] / (torch.norm(grad[i]) + 1e-20)\n",
    "                    grad[i] = ext_lr * grad[i]\n",
    "                if self.monitor_grads: tensor_stats(grad, \"Gradients\")\n",
    "                self.w -= grad\n",
    "                self.w.grad.zero_()\n",
    "\n",
    "    def anneal(self, inputs, labels, decay1, decay2):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.forward(inputs)\n",
    "            old_loss = criterion(outputs, labels).item() * inputs.size(0)\n",
    "            old_w = self.w.data.clone()\n",
    "\n",
    "            self.w.data = self.w.data * self.ANNEALER()\n",
    "            outputs = self.forward(inputs)\n",
    "            new_loss = criterion(outputs, labels).item() * inputs.size(0)\n",
    "\n",
    "            acceptance_prob = torch.exp(torch.tensor(-(new_loss - old_loss) / self.temperature_2, device=self.device))\n",
    "            if self.monitor_annealing: print(\"Old & New Losses\", old_loss, new_loss,\"Probab:\", acceptance_prob)\n",
    "            if new_loss < old_loss or torch.rand(1, device=self.device) < acceptance_prob:\n",
    "                if self.monitor_annealing: print(\"Annealed weights accepted\")\n",
    "            else:\n",
    "                self.w.data = old_w\n",
    "\n",
    "            self.temperature_1 *= decay1\n",
    "            self.temperature_2 *= decay2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845d51da-8368-4c97-89a2-9fc1374f408b",
   "metadata": {
    "id": "845d51da-8368-4c97-89a2-9fc1374f408b"
   },
   "source": [
    "## Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d332b5ba-a9e0-4e8a-bff9-3176267bef00",
   "metadata": {
    "id": "d332b5ba-a9e0-4e8a-bff9-3176267bef00"
   },
   "outputs": [],
   "source": [
    "params_RRAM = {\n",
    "    \"scaling\": 5,\n",
    "    \"G_ON\": 6e-5,\n",
    "    \"G_OFF\": 2.88e-6,\n",
    "    \"V_INV\": 0.6,\n",
    "    \"R_INV\": 1000.0,\n",
    "    \"V_1\": 0.1,\n",
    "    \"V_0\": -0.1,\n",
    "    \"zeta\": 10.0,\n",
    "    \"initial_factor\": 0.01,\n",
    "    \"crossbar\": (64, 64),\n",
    "    \"input_size\": 676,\n",
    "    \"encoding_size\": 4,\n",
    "    \"output_size\": 10,\n",
    "    \"data_in\": 52,\n",
    "    \"bin_active\": True,\n",
    "    \"monitor_volts\": False,\n",
    "    \"monitor_grads\": False,\n",
    "    \"monitor_latents\": False,\n",
    "    \"dropout\": 0.1,\n",
    "    \"int_lr\": 0.01,\n",
    "    \"int_norm\": True,\n",
    "    \"ext_lr\": 500,\n",
    "    \"epochs\": 1000,\n",
    "    \"temperature_1\": 0.2,\n",
    "    \"temperature_2\": 5000,\n",
    "    \"monitor_annealing\": True\n",
    "}\n",
    "\n",
    "\n",
    "model_params = {k: v for k, v in params_RRAM.items() if k not in [\"noise_std\", \"batch_size\", \"lr\", \"epochs\",\"ext_lr\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83a6271e-f7f2-4be1-8a71-b162ad2055e9",
   "metadata": {
    "id": "83a6271e-f7f2-4be1-8a71-b162ad2055e9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_RRAM = SoftBinaryRecurrentForwardNetwork(**model_params).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d1155-be9e-4a7a-9a74-a1d608188350",
   "metadata": {
    "id": "241d1155-be9e-4a7a-9a74-a1d608188350"
   },
   "source": [
    "## Training:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c0b7d2-7c19-4b3e-8856-76d2e9f32792",
   "metadata": {
    "id": "25c0b7d2-7c19-4b3e-8856-76d2e9f32792"
   },
   "source": [
    "### Training to a subset of Dataset First\n",
    "\n",
    "This is just to see if the model is backpropagating before putting in into the full training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efd13def-9751-4e0e-89bf-666a98444d02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "efd13def-9751-4e0e-89bf-666a98444d02",
    "outputId": "76bd894d-87a5-429b-bb31-d646e8921856"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old & New Losses 2299.3550300598145 2302.7737140655518 Probab: tensor(0.9993, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 1, LR: 20.0000, Train Loss: 2.3026, Train Accuracy: 7.80%, Temperatures:(0.99, 4950.00)\n",
      "Old & New Losses 2478.7044525146484 2303.753614425659 Probab: tensor(1.0360, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 2, LR: 100.0000, Train Loss: 2.3028, Train Accuracy: 10.60%, Temperatures:(0.98, 4900.50)\n",
      "Old & New Losses 2473.375082015991 2306.154727935791 Probab: tensor(1.0347, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 3, LR: 500.0000, Train Loss: 2.3037, Train Accuracy: 10.10%, Temperatures:(0.97, 4851.49)\n",
      "Old & New Losses 2303.93123626709 2307.117223739624 Probab: tensor(0.9993, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 4, LR: 500.0000, Train Loss: 2.3084, Train Accuracy: 11.10%, Temperatures:(0.96, 4802.98)\n",
      "Old & New Losses 2291.8834686279297 2303.3580780029297 Probab: tensor(0.9976, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 5, LR: 500.0000, Train Loss: 2.3081, Train Accuracy: 11.50%, Temperatures:(0.95, 4754.95)\n",
      "Old & New Losses 2331.8240642547607 2308.379888534546 Probab: tensor(1.0049, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 6, LR: 500.0000, Train Loss: 2.3023, Train Accuracy: 9.70%, Temperatures:(0.94, 4707.40)\n",
      "Old & New Losses 2284.7912311553955 2306.5671920776367 Probab: tensor(0.9954, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 7, LR: 500.0000, Train Loss: 2.3063, Train Accuracy: 11.10%, Temperatures:(0.93, 4660.33)\n",
      "Old & New Losses 2315.9849643707275 2302.7424812316895 Probab: tensor(1.0028, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 8, LR: 500.0000, Train Loss: 2.3077, Train Accuracy: 8.00%, Temperatures:(0.92, 4613.72)\n",
      "Old & New Losses 2369.9190616607666 2299.06964302063 Probab: tensor(1.0155, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 9, LR: 500.0000, Train Loss: 2.3031, Train Accuracy: 9.90%, Temperatures:(0.91, 4567.59)\n",
      "Old & New Losses 2274.1341590881348 2297.264814376831 Probab: tensor(0.9949, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 10, LR: 500.0000, Train Loss: 2.2995, Train Accuracy: 10.70%, Temperatures:(0.90, 4521.91)\n",
      "Old & New Losses 2248.910427093506 2302.5426864624023 Probab: tensor(0.9882, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 11, LR: 500.0000, Train Loss: 2.2976, Train Accuracy: 10.70%, Temperatures:(0.90, 4476.69)\n",
      "Old & New Losses 2463.0203247070312 2302.461624145508 Probab: tensor(1.0365, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 12, LR: 500.0000, Train Loss: 2.3045, Train Accuracy: 9.80%, Temperatures:(0.89, 4431.92)\n",
      "Old & New Losses 2297.114372253418 2302.914619445801 Probab: tensor(0.9987, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 13, LR: 500.0000, Train Loss: 2.3027, Train Accuracy: 10.10%, Temperatures:(0.88, 4387.61)\n",
      "Old & New Losses 2334.7246646881104 2305.8505058288574 Probab: tensor(1.0066, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 14, LR: 500.0000, Train Loss: 2.3029, Train Accuracy: 6.00%, Temperatures:(0.87, 4343.73)\n",
      "Old & New Losses 2301.6796112060547 2302.0336627960205 Probab: tensor(0.9999, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 15, LR: 500.0000, Train Loss: 2.3063, Train Accuracy: 11.30%, Temperatures:(0.86, 4300.29)\n",
      "Old & New Losses 2320.8706378936768 2301.2866973876953 Probab: tensor(1.0046, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 16, LR: 500.0000, Train Loss: 2.3026, Train Accuracy: 10.10%, Temperatures:(0.85, 4257.29)\n",
      "Old & New Losses 2313.1229877471924 2302.5200366973877 Probab: tensor(1.0025, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 17, LR: 500.0000, Train Loss: 2.3029, Train Accuracy: 9.60%, Temperatures:(0.84, 4214.72)\n",
      "Old & New Losses 2399.4033336639404 2303.7428855895996 Probab: tensor(1.0230, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 18, LR: 500.0000, Train Loss: 2.3031, Train Accuracy: 7.90%, Temperatures:(0.83, 4172.57)\n",
      "Old & New Losses 2382.341146469116 2300.9214401245117 Probab: tensor(1.0197, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 19, LR: 500.0000, Train Loss: 2.3018, Train Accuracy: 9.30%, Temperatures:(0.83, 4130.84)\n",
      "Old & New Losses 2298.3829975128174 2299.8859882354736 Probab: tensor(0.9996, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 20, LR: 500.0000, Train Loss: 2.2998, Train Accuracy: 10.70%, Temperatures:(0.82, 4089.53)\n",
      "Old & New Losses 2255.059242248535 2300.642490386963 Probab: tensor(0.9889, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 21, LR: 500.0000, Train Loss: 2.3009, Train Accuracy: 10.90%, Temperatures:(0.81, 4048.64)\n",
      "Old & New Losses 2273.0045318603516 2297.628164291382 Probab: tensor(0.9939, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 22, LR: 500.0000, Train Loss: 2.3022, Train Accuracy: 11.40%, Temperatures:(0.80, 4008.15)\n",
      "Old & New Losses 2290.3666496276855 2302.4375438690186 Probab: tensor(0.9970, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 23, LR: 500.0000, Train Loss: 2.2994, Train Accuracy: 11.10%, Temperatures:(0.79, 3968.07)\n",
      "Old & New Losses 2302.9727935791016 2302.7896881103516 Probab: tensor(1.0000, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 24, LR: 500.0000, Train Loss: 2.3026, Train Accuracy: 11.00%, Temperatures:(0.79, 3928.39)\n",
      "Old & New Losses 2273.8938331604004 2297.3649501800537 Probab: tensor(0.9940, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 25, LR: 500.0000, Train Loss: 2.3021, Train Accuracy: 12.80%, Temperatures:(0.78, 3889.11)\n",
      "Old & New Losses 2391.8488025665283 2301.1345863342285 Probab: tensor(1.0236, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 26, LR: 500.0000, Train Loss: 2.2983, Train Accuracy: 10.70%, Temperatures:(0.77, 3850.22)\n",
      "Old & New Losses 2319.4053173065186 2304.320812225342 Probab: tensor(1.0039, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 27, LR: 500.0000, Train Loss: 2.3024, Train Accuracy: 11.20%, Temperatures:(0.76, 3811.71)\n",
      "Old & New Losses 2285.9654426574707 2301.649332046509 Probab: tensor(0.9959, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 28, LR: 500.0000, Train Loss: 2.3043, Train Accuracy: 12.60%, Temperatures:(0.75, 3773.60)\n",
      "Old & New Losses 2272.5143432617188 2301.041841506958 Probab: tensor(0.9925, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 29, LR: 500.0000, Train Loss: 2.3020, Train Accuracy: 11.80%, Temperatures:(0.75, 3735.86)\n",
      "Old & New Losses 2286.8971824645996 2297.2795963287354 Probab: tensor(0.9972, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 30, LR: 500.0000, Train Loss: 2.3011, Train Accuracy: 10.60%, Temperatures:(0.74, 3698.50)\n",
      "Old & New Losses 2324.3112564086914 2299.131393432617 Probab: tensor(1.0068, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 31, LR: 500.0000, Train Loss: 2.2968, Train Accuracy: 10.60%, Temperatures:(0.73, 3661.52)\n",
      "Old & New Losses 2326.355457305908 2300.3005981445312 Probab: tensor(1.0071, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 32, LR: 500.0000, Train Loss: 2.3009, Train Accuracy: 11.20%, Temperatures:(0.72, 3624.90)\n",
      "Old & New Losses 2286.616802215576 2302.6037216186523 Probab: tensor(0.9956, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 33, LR: 500.0000, Train Loss: 2.3016, Train Accuracy: 11.20%, Temperatures:(0.72, 3588.65)\n",
      "Old & New Losses 2292.1738624572754 2306.560754776001 Probab: tensor(0.9960, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 34, LR: 500.0000, Train Loss: 2.3023, Train Accuracy: 8.80%, Temperatures:(0.71, 3552.77)\n",
      "Old & New Losses 2291.2392616271973 2301.51629447937 Probab: tensor(0.9971, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 35, LR: 500.0000, Train Loss: 2.3060, Train Accuracy: 10.50%, Temperatures:(0.70, 3517.24)\n",
      "Old & New Losses 2319.002151489258 2302.8547763824463 Probab: tensor(1.0046, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 36, LR: 500.0000, Train Loss: 2.3021, Train Accuracy: 9.00%, Temperatures:(0.70, 3482.07)\n",
      "Old & New Losses 2286.273241043091 2302.75821685791 Probab: tensor(0.9953, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 37, LR: 500.0000, Train Loss: 2.3036, Train Accuracy: 10.40%, Temperatures:(0.69, 3447.25)\n",
      "Old & New Losses 2271.342992782593 2302.757740020752 Probab: tensor(0.9909, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 38, LR: 500.0000, Train Loss: 2.3027, Train Accuracy: 11.60%, Temperatures:(0.68, 3412.77)\n",
      "Old & New Losses 2285.083770751953 2300.7450103759766 Probab: tensor(0.9954, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 39, LR: 500.0000, Train Loss: 2.3023, Train Accuracy: 10.40%, Temperatures:(0.68, 3378.65)\n",
      "Old & New Losses 2288.680076599121 2301.8596172332764 Probab: tensor(0.9961, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 40, LR: 500.0000, Train Loss: 2.3020, Train Accuracy: 9.20%, Temperatures:(0.67, 3344.86)\n",
      "Old & New Losses 2302.708625793457 2299.4484901428223 Probab: tensor(1.0010, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 41, LR: 500.0000, Train Loss: 2.3034, Train Accuracy: 9.30%, Temperatures:(0.66, 3311.41)\n",
      "Old & New Losses 2329.2126655578613 2301.819324493408 Probab: tensor(1.0083, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 42, LR: 500.0000, Train Loss: 2.2987, Train Accuracy: 14.20%, Temperatures:(0.66, 3278.30)\n",
      "Old & New Losses 2289.036273956299 2295.9890365600586 Probab: tensor(0.9979, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 43, LR: 500.0000, Train Loss: 2.3014, Train Accuracy: 7.90%, Temperatures:(0.65, 3245.51)\n",
      "Old & New Losses 2281.5206050872803 2302.450656890869 Probab: tensor(0.9936, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 44, LR: 500.0000, Train Loss: 2.2963, Train Accuracy: 10.70%, Temperatures:(0.64, 3213.06)\n",
      "Old & New Losses 2332.150936126709 2300.649881362915 Probab: tensor(1.0099, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 45, LR: 500.0000, Train Loss: 2.3015, Train Accuracy: 11.80%, Temperatures:(0.64, 3180.93)\n",
      "Old & New Losses 2343.761920928955 2291.5847301483154 Probab: tensor(1.0165, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 46, LR: 500.0000, Train Loss: 2.3005, Train Accuracy: 11.70%, Temperatures:(0.63, 3149.12)\n",
      "Old & New Losses 2314.018726348877 2326.5395164489746 Probab: tensor(0.9960, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 47, LR: 500.0000, Train Loss: 2.2962, Train Accuracy: 11.30%, Temperatures:(0.62, 3117.63)\n",
      "Old & New Losses 2280.714511871338 2297.0457077026367 Probab: tensor(0.9948, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 48, LR: 500.0000, Train Loss: 2.3237, Train Accuracy: 9.40%, Temperatures:(0.62, 3086.45)\n",
      "Old & New Losses 2316.4565563201904 2301.6765117645264 Probab: tensor(1.0048, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 49, LR: 500.0000, Train Loss: 2.2950, Train Accuracy: 10.60%, Temperatures:(0.61, 3055.59)\n",
      "Old & New Losses 2305.4122924804688 2295.3739166259766 Probab: tensor(1.0033, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 50, LR: 500.0000, Train Loss: 2.3009, Train Accuracy: 10.30%, Temperatures:(0.61, 3025.03)\n",
      "Old & New Losses 2273.6668586730957 2300.4603385925293 Probab: tensor(0.9912, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 51, LR: 500.0000, Train Loss: 2.2963, Train Accuracy: 11.30%, Temperatures:(0.60, 2994.78)\n",
      "Old & New Losses 2327.9190063476562 2303.104877471924 Probab: tensor(1.0083, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 52, LR: 250.0000, Train Loss: 2.2989, Train Accuracy: 9.70%, Temperatures:(0.59, 2964.83)\n",
      "Old & New Losses 2280.2510261535645 2300.8370399475098 Probab: tensor(0.9931, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 53, LR: 250.0000, Train Loss: 2.3014, Train Accuracy: 11.10%, Temperatures:(0.59, 2935.18)\n",
      "Old & New Losses 2438.054084777832 2301.382064819336 Probab: tensor(1.0477, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 54, LR: 250.0000, Train Loss: 2.3007, Train Accuracy: 11.50%, Temperatures:(0.58, 2905.83)\n",
      "Old & New Losses 2283.4081649780273 2298.3105182647705 Probab: tensor(0.9949, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 55, LR: 250.0000, Train Loss: 2.2982, Train Accuracy: 9.80%, Temperatures:(0.58, 2876.77)\n",
      "Old & New Losses 2319.6284770965576 2299.2796897888184 Probab: tensor(1.0071, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 56, LR: 250.0000, Train Loss: 2.2994, Train Accuracy: 10.90%, Temperatures:(0.57, 2848.01)\n",
      "Old & New Losses 2274.5015621185303 2291.203498840332 Probab: tensor(0.9942, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 57, LR: 250.0000, Train Loss: 2.2997, Train Accuracy: 9.50%, Temperatures:(0.56, 2819.53)\n",
      "Old & New Losses 2240.009307861328 2293.43318939209 Probab: tensor(0.9812, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 58, LR: 250.0000, Train Loss: 2.2876, Train Accuracy: 10.60%, Temperatures:(0.56, 2791.33)\n",
      "Old & New Losses 2380.429267883301 2302.267074584961 Probab: tensor(1.0284, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 59, LR: 250.0000, Train Loss: 2.2931, Train Accuracy: 15.60%, Temperatures:(0.55, 2763.42)\n",
      "Old & New Losses 2369.317054748535 2305.166721343994 Probab: tensor(1.0235, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 60, LR: 250.0000, Train Loss: 2.3000, Train Accuracy: 12.10%, Temperatures:(0.55, 2735.78)\n",
      "Old & New Losses 2282.8898429870605 2298.637628555298 Probab: tensor(0.9943, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 61, LR: 250.0000, Train Loss: 2.3052, Train Accuracy: 8.80%, Temperatures:(0.54, 2708.43)\n",
      "Old & New Losses 2424.180507659912 2302.297592163086 Probab: tensor(1.0460, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 62, LR: 250.0000, Train Loss: 2.2986, Train Accuracy: 11.40%, Temperatures:(0.54, 2681.34)\n",
      "Old & New Losses 2319.035053253174 2299.1576194763184 Probab: tensor(1.0074, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 63, LR: 250.0000, Train Loss: 2.3015, Train Accuracy: 10.30%, Temperatures:(0.53, 2654.53)\n",
      "Old & New Losses 2289.623975753784 2298.579692840576 Probab: tensor(0.9966, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 64, LR: 250.0000, Train Loss: 2.2960, Train Accuracy: 11.50%, Temperatures:(0.53, 2627.98)\n",
      "Old & New Losses 2271.791696548462 2297.349214553833 Probab: tensor(0.9903, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 65, LR: 250.0000, Train Loss: 2.3004, Train Accuracy: 11.20%, Temperatures:(0.52, 2601.70)\n",
      "Old & New Losses 2267.488956451416 2298.4023094177246 Probab: tensor(0.9882, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 66, LR: 250.0000, Train Loss: 2.2963, Train Accuracy: 10.90%, Temperatures:(0.52, 2575.69)\n",
      "Old & New Losses 2348.8969802856445 2298.9346981048584 Probab: tensor(1.0196, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 67, LR: 250.0000, Train Loss: 2.2951, Train Accuracy: 12.40%, Temperatures:(0.51, 2549.93)\n",
      "Old & New Losses 2302.3152351379395 2296.8573570251465 Probab: tensor(1.0021, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 68, LR: 250.0000, Train Loss: 2.3009, Train Accuracy: 10.40%, Temperatures:(0.50, 2524.43)\n",
      "Old & New Losses 2421.6251373291016 2288.4771823883057 Probab: tensor(1.0542, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 69, LR: 250.0000, Train Loss: 2.2967, Train Accuracy: 11.90%, Temperatures:(0.50, 2499.19)\n",
      "Old & New Losses 2423.537492752075 2352.482795715332 Probab: tensor(1.0288, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 70, LR: 250.0000, Train Loss: 2.2880, Train Accuracy: 13.30%, Temperatures:(0.49, 2474.19)\n",
      "Old & New Losses 2313.163995742798 2311.2974166870117 Probab: tensor(1.0008, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 71, LR: 250.0000, Train Loss: 2.3484, Train Accuracy: 9.90%, Temperatures:(0.49, 2449.45)\n",
      "Old & New Losses 2289.18194770813 2305.771827697754 Probab: tensor(0.9933, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 72, LR: 250.0000, Train Loss: 2.3140, Train Accuracy: 9.40%, Temperatures:(0.48, 2424.96)\n",
      "Old & New Losses 2329.289436340332 2293.9395904541016 Probab: tensor(1.0147, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 73, LR: 250.0000, Train Loss: 2.3112, Train Accuracy: 11.60%, Temperatures:(0.48, 2400.71)\n",
      "Old & New Losses 2255.021333694458 2293.788433074951 Probab: tensor(0.9840, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 74, LR: 250.0000, Train Loss: 2.2938, Train Accuracy: 11.80%, Temperatures:(0.48, 2376.70)\n",
      "Old & New Losses 2318.9120292663574 2288.839340209961 Probab: tensor(1.0127, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 75, LR: 250.0000, Train Loss: 2.2924, Train Accuracy: 12.70%, Temperatures:(0.47, 2352.93)\n",
      "Old & New Losses 2394.0939903259277 2290.6720638275146 Probab: tensor(1.0449, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 76, LR: 250.0000, Train Loss: 2.2872, Train Accuracy: 9.60%, Temperatures:(0.47, 2329.40)\n",
      "Old & New Losses 2459.6261978149414 2288.6600494384766 Probab: tensor(1.0762, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 77, LR: 250.0000, Train Loss: 2.2893, Train Accuracy: 11.20%, Temperatures:(0.46, 2306.11)\n",
      "Old & New Losses 2429.940938949585 2293.50209236145 Probab: tensor(1.0609, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 78, LR: 250.0000, Train Loss: 2.2895, Train Accuracy: 11.70%, Temperatures:(0.46, 2283.05)\n",
      "Old & New Losses 2308.0339431762695 2296.342372894287 Probab: tensor(1.0051, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 79, LR: 250.0000, Train Loss: 2.2952, Train Accuracy: 13.90%, Temperatures:(0.45, 2260.22)\n",
      "Old & New Losses 2263.4522914886475 2293.718099594116 Probab: tensor(0.9867, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 80, LR: 250.0000, Train Loss: 2.2910, Train Accuracy: 11.20%, Temperatures:(0.45, 2237.62)\n",
      "Old & New Losses 2256.164073944092 2295.659065246582 Probab: tensor(0.9825, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 81, LR: 250.0000, Train Loss: 2.2917, Train Accuracy: 12.80%, Temperatures:(0.44, 2215.24)\n",
      "Old & New Losses 2272.670030593872 2292.6735877990723 Probab: tensor(0.9910, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 82, LR: 250.0000, Train Loss: 2.2934, Train Accuracy: 10.60%, Temperatures:(0.44, 2193.09)\n",
      "Old & New Losses 2235.4893684387207 2288.222312927246 Probab: tensor(0.9762, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 83, LR: 250.0000, Train Loss: 2.2914, Train Accuracy: 11.30%, Temperatures:(0.43, 2171.16)\n",
      "Old & New Losses 2303.8384914398193 2276.9083976745605 Probab: tensor(1.0125, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 84, LR: 250.0000, Train Loss: 2.2883, Train Accuracy: 12.50%, Temperatures:(0.43, 2149.45)\n",
      "Old & New Losses 2286.2415313720703 2288.525342941284 Probab: tensor(0.9989, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 85, LR: 250.0000, Train Loss: 2.2761, Train Accuracy: 11.00%, Temperatures:(0.43, 2127.95)\n",
      "Old & New Losses 2288.4230613708496 2296.086072921753 Probab: tensor(0.9964, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 86, LR: 250.0000, Train Loss: 2.2870, Train Accuracy: 9.50%, Temperatures:(0.42, 2106.67)\n",
      "Old & New Losses 2291.496992111206 2296.79274559021 Probab: tensor(0.9975, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 87, LR: 250.0000, Train Loss: 2.2952, Train Accuracy: 9.80%, Temperatures:(0.42, 2085.60)\n",
      "Old & New Losses 2280.6992530822754 2294.252395629883 Probab: tensor(0.9935, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 88, LR: 250.0000, Train Loss: 2.2964, Train Accuracy: 10.10%, Temperatures:(0.41, 2064.75)\n",
      "Old & New Losses 2287.3306274414062 2292.1974658966064 Probab: tensor(0.9976, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 89, LR: 250.0000, Train Loss: 2.2941, Train Accuracy: 10.90%, Temperatures:(0.41, 2044.10)\n",
      "Old & New Losses 2279.3965339660645 2290.9183502197266 Probab: tensor(0.9944, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 90, LR: 250.0000, Train Loss: 2.2926, Train Accuracy: 10.10%, Temperatures:(0.40, 2023.66)\n",
      "Old & New Losses 2278.7110805511475 2292.861223220825 Probab: tensor(0.9930, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 91, LR: 250.0000, Train Loss: 2.2916, Train Accuracy: 9.60%, Temperatures:(0.40, 2003.42)\n",
      "Old & New Losses 2299.387216567993 2301.10239982605 Probab: tensor(0.9991, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 92, LR: 250.0000, Train Loss: 2.2889, Train Accuracy: 11.00%, Temperatures:(0.40, 1983.39)\n",
      "Old & New Losses 2284.480094909668 2294.405460357666 Probab: tensor(0.9950, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 93, LR: 250.0000, Train Loss: 2.2988, Train Accuracy: 9.50%, Temperatures:(0.39, 1963.56)\n",
      "Old & New Losses 2277.927875518799 2299.621105194092 Probab: tensor(0.9890, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 94, LR: 250.0000, Train Loss: 2.2927, Train Accuracy: 10.00%, Temperatures:(0.39, 1943.92)\n",
      "Old & New Losses 2282.4742794036865 2300.048351287842 Probab: tensor(0.9910, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 95, LR: 250.0000, Train Loss: 2.2985, Train Accuracy: 9.30%, Temperatures:(0.38, 1924.48)\n",
      "Old & New Losses 2299.2072105407715 2295.8858013153076 Probab: tensor(1.0017, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 96, LR: 250.0000, Train Loss: 2.3001, Train Accuracy: 9.30%, Temperatures:(0.38, 1905.24)\n",
      "Old & New Losses 2280.4031372070312 2287.88423538208 Probab: tensor(0.9961, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 97, LR: 250.0000, Train Loss: 2.2942, Train Accuracy: 12.40%, Temperatures:(0.38, 1886.18)\n",
      "Old & New Losses 2276.322841644287 2290.585994720459 Probab: tensor(0.9925, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 98, LR: 250.0000, Train Loss: 2.2908, Train Accuracy: 11.50%, Temperatures:(0.37, 1867.32)\n",
      "Old & New Losses 2274.2395401000977 2294.9788570404053 Probab: tensor(0.9890, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 99, LR: 250.0000, Train Loss: 2.2851, Train Accuracy: 11.90%, Temperatures:(0.37, 1848.65)\n",
      "Old & New Losses 2269.97709274292 2286.648988723755 Probab: tensor(0.9910, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 100, LR: 250.0000, Train Loss: 2.2931, Train Accuracy: 11.80%, Temperatures:(0.37, 1830.16)\n",
      "Old & New Losses 2267.997980117798 2281.1944484710693 Probab: tensor(0.9928, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 101, LR: 250.0000, Train Loss: 2.2835, Train Accuracy: 11.30%, Temperatures:(0.36, 1811.86)\n",
      "Old & New Losses 2261.017322540283 2274.811267852783 Probab: tensor(0.9924, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 102, LR: 125.0000, Train Loss: 2.2814, Train Accuracy: 12.10%, Temperatures:(0.36, 1793.74)\n",
      "Old & New Losses 2268.3887481689453 2291.975498199463 Probab: tensor(0.9869, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 103, LR: 125.0000, Train Loss: 2.2781, Train Accuracy: 10.50%, Temperatures:(0.36, 1775.80)\n",
      "Old & New Losses 2278.3310413360596 2305.035352706909 Probab: tensor(0.9851, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 104, LR: 125.0000, Train Loss: 2.2926, Train Accuracy: 10.50%, Temperatures:(0.35, 1758.05)\n",
      "Old & New Losses 2282.5000286102295 2292.7215099334717 Probab: tensor(0.9942, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 105, LR: 125.0000, Train Loss: 2.3079, Train Accuracy: 9.90%, Temperatures:(0.35, 1740.47)\n",
      "Old & New Losses 2273.019790649414 2297.1630096435547 Probab: tensor(0.9862, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 106, LR: 125.0000, Train Loss: 2.2903, Train Accuracy: 9.70%, Temperatures:(0.34, 1723.06)\n",
      "Old & New Losses 2269.0296173095703 2282.8049659729004 Probab: tensor(0.9920, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 107, LR: 125.0000, Train Loss: 2.2962, Train Accuracy: 11.50%, Temperatures:(0.34, 1705.83)\n",
      "Old & New Losses 2248.363733291626 2289.722204208374 Probab: tensor(0.9760, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 108, LR: 125.0000, Train Loss: 2.2852, Train Accuracy: 14.30%, Temperatures:(0.34, 1688.77)\n",
      "Old & New Losses 2259.2363357543945 2283.8611602783203 Probab: tensor(0.9855, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 109, LR: 125.0000, Train Loss: 2.2879, Train Accuracy: 10.30%, Temperatures:(0.33, 1671.88)\n",
      "Old & New Losses 2230.2656173706055 2245.9473609924316 Probab: tensor(0.9907, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 110, LR: 125.0000, Train Loss: 2.2840, Train Accuracy: 13.40%, Temperatures:(0.33, 1655.17)\n",
      "Old & New Losses 2343.7185287475586 2252.655029296875 Probab: tensor(1.0566, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 111, LR: 125.0000, Train Loss: 2.2476, Train Accuracy: 16.40%, Temperatures:(0.33, 1638.61)\n",
      "Old & New Losses 2242.250680923462 2248.246192932129 Probab: tensor(0.9963, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 112, LR: 125.0000, Train Loss: 2.2508, Train Accuracy: 22.10%, Temperatures:(0.32, 1622.23)\n",
      "Old & New Losses 2205.176830291748 2209.2339992523193 Probab: tensor(0.9975, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 113, LR: 125.0000, Train Loss: 2.2467, Train Accuracy: 18.00%, Temperatures:(0.32, 1606.01)\n",
      "Old & New Losses 2234.0426445007324 2208.348035812378 Probab: tensor(1.0161, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 114, LR: 125.0000, Train Loss: 2.2150, Train Accuracy: 19.20%, Temperatures:(0.32, 1589.95)\n",
      "Old & New Losses 2225.900411605835 2244.097948074341 Probab: tensor(0.9886, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 115, LR: 125.0000, Train Loss: 2.2116, Train Accuracy: 17.30%, Temperatures:(0.31, 1574.05)\n",
      "Old & New Losses 2149.0962505340576 2257.9586505889893 Probab: tensor(0.9332, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 116, LR: 125.0000, Train Loss: 2.2410, Train Accuracy: 15.60%, Temperatures:(0.31, 1558.31)\n",
      "Old & New Losses 2172.6760864257812 2249.2079734802246 Probab: tensor(0.9521, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 117, LR: 125.0000, Train Loss: 2.2530, Train Accuracy: 17.60%, Temperatures:(0.31, 1542.72)\n",
      "Old & New Losses 2160.064220428467 2232.8875064849854 Probab: tensor(0.9539, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 118, LR: 125.0000, Train Loss: 2.2440, Train Accuracy: 16.70%, Temperatures:(0.31, 1527.30)\n",
      "Old & New Losses 2189.0485286712646 2232.4163913726807 Probab: tensor(0.9720, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 119, LR: 125.0000, Train Loss: 2.2332, Train Accuracy: 15.80%, Temperatures:(0.30, 1512.02)\n",
      "Old & New Losses 2212.33868598938 2213.0727767944336 Probab: tensor(0.9995, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 120, LR: 125.0000, Train Loss: 2.2351, Train Accuracy: 17.30%, Temperatures:(0.30, 1496.90)\n",
      "Old & New Losses 2230.16095161438 2266.2694454193115 Probab: tensor(0.9762, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 121, LR: 125.0000, Train Loss: 2.2114, Train Accuracy: 19.80%, Temperatures:(0.30, 1481.93)\n",
      "Old & New Losses 2180.6435585021973 2253.774642944336 Probab: tensor(0.9518, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 122, LR: 125.0000, Train Loss: 2.2599, Train Accuracy: 16.00%, Temperatures:(0.29, 1467.11)\n",
      "Old & New Losses 2171.6716289520264 2204.8778533935547 Probab: tensor(0.9776, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 123, LR: 125.0000, Train Loss: 2.2486, Train Accuracy: 16.00%, Temperatures:(0.29, 1452.44)\n",
      "Old & New Losses 2190.711736679077 2181.3480854034424 Probab: tensor(1.0065, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 124, LR: 125.0000, Train Loss: 2.2101, Train Accuracy: 20.40%, Temperatures:(0.29, 1437.92)\n",
      "Old & New Losses 2175.029993057251 2198.5812187194824 Probab: tensor(0.9838, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 125, LR: 125.0000, Train Loss: 2.1782, Train Accuracy: 19.20%, Temperatures:(0.28, 1423.54)\n",
      "Old & New Losses 2119.6842193603516 2138.1442546844482 Probab: tensor(0.9871, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 126, LR: 125.0000, Train Loss: 2.2049, Train Accuracy: 16.90%, Temperatures:(0.28, 1409.30)\n",
      "Old & New Losses 2165.912628173828 2132.504463195801 Probab: tensor(1.0240, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 127, LR: 125.0000, Train Loss: 2.1392, Train Accuracy: 22.80%, Temperatures:(0.28, 1395.21)\n",
      "Old & New Losses 2165.9388542175293 2245.3527450561523 Probab: tensor(0.9447, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 128, LR: 125.0000, Train Loss: 2.1272, Train Accuracy: 20.10%, Temperatures:(0.28, 1381.26)\n",
      "Old & New Losses 2170.219659805298 2147.9859352111816 Probab: tensor(1.0162, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 129, LR: 125.0000, Train Loss: 2.2455, Train Accuracy: 13.10%, Temperatures:(0.27, 1367.45)\n",
      "Old & New Losses 2218.583106994629 2157.054901123047 Probab: tensor(1.0460, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 130, LR: 125.0000, Train Loss: 2.1392, Train Accuracy: 19.80%, Temperatures:(0.27, 1353.77)\n",
      "Old & New Losses 2154.7019481658936 2253.6025047302246 Probab: tensor(0.9295, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 131, LR: 125.0000, Train Loss: 2.1622, Train Accuracy: 19.20%, Temperatures:(0.27, 1340.23)\n",
      "Old & New Losses 2149.306297302246 2223.640203475952 Probab: tensor(0.9460, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 132, LR: 125.0000, Train Loss: 2.2531, Train Accuracy: 14.50%, Temperatures:(0.27, 1326.83)\n",
      "Old & New Losses 2152.4863243103027 2147.1824645996094 Probab: tensor(1.0040, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 133, LR: 125.0000, Train Loss: 2.2295, Train Accuracy: 18.10%, Temperatures:(0.26, 1313.56)\n",
      "Old & New Losses 2202.08477973938 2196.897506713867 Probab: tensor(1.0040, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 134, LR: 125.0000, Train Loss: 2.1577, Train Accuracy: 18.10%, Temperatures:(0.26, 1300.43)\n",
      "Old & New Losses 2148.894786834717 2185.9254837036133 Probab: tensor(0.9719, device='cuda:0')\n",
      "Epoch 135, LR: 125.0000, Train Loss: 2.2040, Train Accuracy: 17.00%, Temperatures:(0.26, 1287.42)\n",
      "Old & New Losses 2117.4662113189697 2142.857313156128 Probab: tensor(0.9805, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 136, LR: 125.0000, Train Loss: 2.1491, Train Accuracy: 18.30%, Temperatures:(0.25, 1274.55)\n",
      "Old & New Losses 2153.130531311035 2212.958574295044 Probab: tensor(0.9541, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 137, LR: 125.0000, Train Loss: 2.1413, Train Accuracy: 21.00%, Temperatures:(0.25, 1261.80)\n",
      "Old & New Losses 2130.685567855835 2140.9127712249756 Probab: tensor(0.9919, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 138, LR: 125.0000, Train Loss: 2.2174, Train Accuracy: 19.50%, Temperatures:(0.25, 1249.19)\n",
      "Old & New Losses 2121.9964027404785 2145.6034183502197 Probab: tensor(0.9813, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 139, LR: 125.0000, Train Loss: 2.1403, Train Accuracy: 20.10%, Temperatures:(0.25, 1236.69)\n",
      "Old & New Losses 2113.4238243103027 2124.0549087524414 Probab: tensor(0.9914, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 140, LR: 125.0000, Train Loss: 2.1488, Train Accuracy: 20.70%, Temperatures:(0.24, 1224.33)\n",
      "Old & New Losses 2115.875720977783 2104.808568954468 Probab: tensor(1.0091, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 141, LR: 125.0000, Train Loss: 2.1244, Train Accuracy: 21.30%, Temperatures:(0.24, 1212.08)\n",
      "Old & New Losses 2093.4112071990967 2089.489698410034 Probab: tensor(1.0032, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 142, LR: 125.0000, Train Loss: 2.1037, Train Accuracy: 21.50%, Temperatures:(0.24, 1199.96)\n",
      "Old & New Losses 2102.3037433624268 2111.3297939300537 Probab: tensor(0.9925, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 143, LR: 125.0000, Train Loss: 2.0880, Train Accuracy: 19.70%, Temperatures:(0.24, 1187.96)\n",
      "Old & New Losses 2064.473867416382 2116.5761947631836 Probab: tensor(0.9571, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 144, LR: 125.0000, Train Loss: 2.1106, Train Accuracy: 26.40%, Temperatures:(0.24, 1176.08)\n",
      "Old & New Losses 2088.850736618042 2098.8073348999023 Probab: tensor(0.9916, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 145, LR: 125.0000, Train Loss: 2.1163, Train Accuracy: 21.30%, Temperatures:(0.23, 1164.32)\n",
      "Old & New Losses 2100.841283798218 2084.2530727386475 Probab: tensor(1.0143, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 146, LR: 125.0000, Train Loss: 2.0980, Train Accuracy: 23.70%, Temperatures:(0.23, 1152.68)\n",
      "Old & New Losses 2096.665143966675 2094.8486328125 Probab: tensor(1.0016, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 147, LR: 125.0000, Train Loss: 2.0839, Train Accuracy: 19.90%, Temperatures:(0.23, 1141.15)\n",
      "Old & New Losses 2058.394432067871 2073.5526084899902 Probab: tensor(0.9868, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 148, LR: 125.0000, Train Loss: 2.0922, Train Accuracy: 26.00%, Temperatures:(0.23, 1129.74)\n",
      "Old & New Losses 2057.5833320617676 2066.0617351531982 Probab: tensor(0.9925, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 149, LR: 125.0000, Train Loss: 2.0793, Train Accuracy: 25.80%, Temperatures:(0.22, 1118.44)\n",
      "Old & New Losses 2044.8522567749023 2072.8278160095215 Probab: tensor(0.9753, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 150, LR: 125.0000, Train Loss: 2.0639, Train Accuracy: 28.30%, Temperatures:(0.22, 1107.26)\n",
      "Old & New Losses 2111.5307807922363 2131.845235824585 Probab: tensor(0.9818, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 151, LR: 125.0000, Train Loss: 2.0637, Train Accuracy: 21.80%, Temperatures:(0.22, 1096.19)\n",
      "Old & New Losses 2054.8079013824463 2066.701889038086 Probab: tensor(0.9892, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 152, LR: 62.5000, Train Loss: 2.1298, Train Accuracy: 19.40%, Temperatures:(0.22, 1085.22)\n",
      "Old & New Losses 2047.2607612609863 2070.4805850982666 Probab: tensor(0.9788, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 153, LR: 62.5000, Train Loss: 2.0695, Train Accuracy: 19.90%, Temperatures:(0.21, 1074.37)\n",
      "Old & New Losses 2039.2365455627441 2056.8206310272217 Probab: tensor(0.9838, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 154, LR: 62.5000, Train Loss: 2.0725, Train Accuracy: 22.40%, Temperatures:(0.21, 1063.63)\n",
      "Old & New Losses 2044.034719467163 2060.948371887207 Probab: tensor(0.9842, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 155, LR: 62.5000, Train Loss: 2.0496, Train Accuracy: 23.70%, Temperatures:(0.21, 1052.99)\n",
      "Old & New Losses 2050.731897354126 2050.361394882202 Probab: tensor(1.0004, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 156, LR: 62.5000, Train Loss: 2.0603, Train Accuracy: 27.10%, Temperatures:(0.21, 1042.46)\n",
      "Old & New Losses 2030.9782028198242 2042.6061153411865 Probab: tensor(0.9889, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 157, LR: 62.5000, Train Loss: 2.0525, Train Accuracy: 25.20%, Temperatures:(0.21, 1032.04)\n",
      "Old & New Losses 2047.1935272216797 2042.3808097839355 Probab: tensor(1.0047, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 158, LR: 62.5000, Train Loss: 2.0467, Train Accuracy: 28.20%, Temperatures:(0.20, 1021.72)\n",
      "Old & New Losses 2036.8237495422363 2051.778554916382 Probab: tensor(0.9855, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 159, LR: 62.5000, Train Loss: 2.0433, Train Accuracy: 26.70%, Temperatures:(0.20, 1011.50)\n",
      "Old & New Losses 2057.760715484619 2055.849552154541 Probab: tensor(1.0019, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 160, LR: 62.5000, Train Loss: 2.0476, Train Accuracy: 28.80%, Temperatures:(0.20, 1001.39)\n",
      "Old & New Losses 2040.849208831787 2103.168249130249 Probab: tensor(0.9397, device='cuda:0')\n",
      "Epoch 161, LR: 62.5000, Train Loss: 2.0548, Train Accuracy: 27.30%, Temperatures:(0.20, 991.37)\n",
      "Old & New Losses 2033.829689025879 2042.2534942626953 Probab: tensor(0.9915, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 162, LR: 62.5000, Train Loss: 2.0479, Train Accuracy: 27.30%, Temperatures:(0.20, 981.46)\n",
      "Old & New Losses 2042.2859191894531 2040.365219116211 Probab: tensor(1.0020, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 163, LR: 62.5000, Train Loss: 2.0356, Train Accuracy: 30.20%, Temperatures:(0.19, 971.64)\n",
      "Old & New Losses 2028.099536895752 2031.799554824829 Probab: tensor(0.9962, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 164, LR: 62.5000, Train Loss: 2.0367, Train Accuracy: 31.50%, Temperatures:(0.19, 961.93)\n",
      "Old & New Losses 2037.7469062805176 2049.3218898773193 Probab: tensor(0.9880, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 165, LR: 62.5000, Train Loss: 2.0288, Train Accuracy: 31.00%, Temperatures:(0.19, 952.31)\n",
      "Old & New Losses 2022.8850841522217 2041.1744117736816 Probab: tensor(0.9810, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 166, LR: 62.5000, Train Loss: 2.0467, Train Accuracy: 28.80%, Temperatures:(0.19, 942.78)\n",
      "Old & New Losses 2028.4819602966309 2033.076524734497 Probab: tensor(0.9951, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 167, LR: 62.5000, Train Loss: 2.0357, Train Accuracy: 28.90%, Temperatures:(0.19, 933.36)\n",
      "Old & New Losses 2035.2344512939453 2056.448221206665 Probab: tensor(0.9775, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 168, LR: 62.5000, Train Loss: 2.0352, Train Accuracy: 31.90%, Temperatures:(0.18, 924.02)\n",
      "Old & New Losses 2022.4604606628418 2030.5638313293457 Probab: tensor(0.9913, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 169, LR: 62.5000, Train Loss: 2.0472, Train Accuracy: 29.20%, Temperatures:(0.18, 914.78)\n",
      "Old & New Losses 2025.7229804992676 2085.416316986084 Probab: tensor(0.9368, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 170, LR: 62.5000, Train Loss: 2.0258, Train Accuracy: 33.60%, Temperatures:(0.18, 905.63)\n",
      "Old & New Losses 2036.6921424865723 2056.403398513794 Probab: tensor(0.9785, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 171, LR: 62.5000, Train Loss: 2.0866, Train Accuracy: 30.00%, Temperatures:(0.18, 896.58)\n",
      "Old & New Losses 2030.4961204528809 2033.184289932251 Probab: tensor(0.9970, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 172, LR: 62.5000, Train Loss: 2.0516, Train Accuracy: 27.70%, Temperatures:(0.18, 887.61)\n",
      "Old & New Losses 2032.0625305175781 2036.9961261749268 Probab: tensor(0.9945, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 173, LR: 62.5000, Train Loss: 2.0398, Train Accuracy: 31.90%, Temperatures:(0.18, 878.74)\n",
      "Old & New Losses 2026.364803314209 2026.2179374694824 Probab: tensor(1.0002, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 174, LR: 62.5000, Train Loss: 2.0383, Train Accuracy: 29.00%, Temperatures:(0.17, 869.95)\n",
      "Old & New Losses 2025.8824825286865 2028.8033485412598 Probab: tensor(0.9966, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 175, LR: 62.5000, Train Loss: 2.0223, Train Accuracy: 32.70%, Temperatures:(0.17, 861.25)\n",
      "Old & New Losses 2019.6709632873535 2016.0775184631348 Probab: tensor(1.0042, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 176, LR: 62.5000, Train Loss: 2.0276, Train Accuracy: 30.30%, Temperatures:(0.17, 852.64)\n",
      "Old & New Losses 2014.0931606292725 2022.796392440796 Probab: tensor(0.9898, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 177, LR: 62.5000, Train Loss: 2.0199, Train Accuracy: 33.00%, Temperatures:(0.17, 844.11)\n",
      "Old & New Losses 2022.2454071044922 2038.9385223388672 Probab: tensor(0.9804, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 178, LR: 62.5000, Train Loss: 2.0207, Train Accuracy: 29.00%, Temperatures:(0.17, 835.67)\n",
      "Old & New Losses 2017.1644687652588 2018.9414024353027 Probab: tensor(0.9979, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 179, LR: 62.5000, Train Loss: 2.0368, Train Accuracy: 32.50%, Temperatures:(0.17, 827.31)\n",
      "Old & New Losses 2023.939609527588 2015.2111053466797 Probab: tensor(1.0106, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 180, LR: 62.5000, Train Loss: 2.0154, Train Accuracy: 32.10%, Temperatures:(0.16, 819.04)\n",
      "Old & New Losses 2015.1007175445557 2027.9951095581055 Probab: tensor(0.9844, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 181, LR: 62.5000, Train Loss: 2.0157, Train Accuracy: 35.00%, Temperatures:(0.16, 810.85)\n",
      "Old & New Losses 2008.2082748413086 2012.016773223877 Probab: tensor(0.9953, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 182, LR: 62.5000, Train Loss: 2.0282, Train Accuracy: 31.80%, Temperatures:(0.16, 802.74)\n",
      "Old & New Losses 2013.81516456604 2015.1121616363525 Probab: tensor(0.9984, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 183, LR: 62.5000, Train Loss: 2.0152, Train Accuracy: 34.70%, Temperatures:(0.16, 794.71)\n",
      "Old & New Losses 2012.4387741088867 2011.4073753356934 Probab: tensor(1.0013, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 184, LR: 62.5000, Train Loss: 2.0145, Train Accuracy: 33.40%, Temperatures:(0.16, 786.77)\n",
      "Old & New Losses 2008.9125633239746 2006.5686702728271 Probab: tensor(1.0030, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 185, LR: 62.5000, Train Loss: 2.0130, Train Accuracy: 35.40%, Temperatures:(0.16, 778.90)\n",
      "Old & New Losses 2022.0062732696533 2026.87406539917 Probab: tensor(0.9938, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 186, LR: 62.5000, Train Loss: 2.0059, Train Accuracy: 34.40%, Temperatures:(0.15, 771.11)\n",
      "Old & New Losses 1994.2032098770142 1998.669147491455 Probab: tensor(0.9942, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 187, LR: 62.5000, Train Loss: 2.0270, Train Accuracy: 31.80%, Temperatures:(0.15, 763.40)\n",
      "Old & New Losses 2025.5167484283447 2030.3750038146973 Probab: tensor(0.9937, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 188, LR: 62.5000, Train Loss: 1.9972, Train Accuracy: 34.80%, Temperatures:(0.15, 755.76)\n",
      "Old & New Losses 1989.929437637329 1994.7751760482788 Probab: tensor(0.9936, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 189, LR: 62.5000, Train Loss: 2.0317, Train Accuracy: 33.80%, Temperatures:(0.15, 748.21)\n",
      "Old & New Losses 2006.8678855895996 2009.2856884002686 Probab: tensor(0.9968, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 190, LR: 62.5000, Train Loss: 1.9930, Train Accuracy: 35.60%, Temperatures:(0.15, 740.72)\n",
      "Old & New Losses 1984.7936630249023 1984.4977855682373 Probab: tensor(1.0004, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 191, LR: 62.5000, Train Loss: 2.0095, Train Accuracy: 35.50%, Temperatures:(0.15, 733.32)\n",
      "Old & New Losses 2004.5785903930664 2008.9752674102783 Probab: tensor(0.9940, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 192, LR: 62.5000, Train Loss: 1.9840, Train Accuracy: 36.00%, Temperatures:(0.15, 725.98)\n",
      "Old & New Losses 1980.4760217666626 1981.539011001587 Probab: tensor(0.9985, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 193, LR: 62.5000, Train Loss: 2.0125, Train Accuracy: 37.30%, Temperatures:(0.14, 718.72)\n",
      "Old & New Losses 1974.2701053619385 1993.7244653701782 Probab: tensor(0.9733, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 194, LR: 62.5000, Train Loss: 1.9800, Train Accuracy: 38.30%, Temperatures:(0.14, 711.54)\n",
      "Old & New Losses 1971.5702533721924 1985.937476158142 Probab: tensor(0.9800, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 195, LR: 62.5000, Train Loss: 1.9932, Train Accuracy: 34.90%, Temperatures:(0.14, 704.42)\n",
      "Old & New Losses 1973.7579822540283 1970.8671569824219 Probab: tensor(1.0041, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 196, LR: 62.5000, Train Loss: 1.9804, Train Accuracy: 37.10%, Temperatures:(0.14, 697.38)\n",
      "Old & New Losses 1970.5415964126587 1964.782953262329 Probab: tensor(1.0083, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 197, LR: 62.5000, Train Loss: 1.9774, Train Accuracy: 37.10%, Temperatures:(0.14, 690.40)\n",
      "Old & New Losses 1967.381238937378 1973.820686340332 Probab: tensor(0.9907, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 198, LR: 62.5000, Train Loss: 1.9723, Train Accuracy: 36.70%, Temperatures:(0.14, 683.50)\n",
      "Old & New Losses 1963.0348682403564 1965.437650680542 Probab: tensor(0.9965, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 199, LR: 62.5000, Train Loss: 1.9712, Train Accuracy: 37.30%, Temperatures:(0.14, 676.67)\n",
      "Old & New Losses 1960.6095552444458 1961.8057012557983 Probab: tensor(0.9982, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 200, LR: 62.5000, Train Loss: 1.9662, Train Accuracy: 40.40%, Temperatures:(0.13, 669.90)\n",
      "Old & New Losses 1963.2103443145752 1963.4336233139038 Probab: tensor(0.9997, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 201, LR: 62.5000, Train Loss: 1.9639, Train Accuracy: 41.20%, Temperatures:(0.13, 663.20)\n",
      "Old & New Losses 1958.0475091934204 1963.245153427124 Probab: tensor(0.9922, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 202, LR: 31.2500, Train Loss: 1.9604, Train Accuracy: 39.50%, Temperatures:(0.13, 656.57)\n",
      "Old & New Losses 1960.9344005584717 1960.3685140609741 Probab: tensor(1.0009, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 203, LR: 31.2500, Train Loss: 1.9646, Train Accuracy: 36.10%, Temperatures:(0.13, 650.00)\n",
      "Old & New Losses 1960.9193801879883 1960.0489139556885 Probab: tensor(1.0013, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 204, LR: 31.2500, Train Loss: 1.9578, Train Accuracy: 37.60%, Temperatures:(0.13, 643.50)\n",
      "Old & New Losses 1960.1362943649292 1962.5850915908813 Probab: tensor(0.9962, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 205, LR: 31.2500, Train Loss: 1.9647, Train Accuracy: 35.60%, Temperatures:(0.13, 637.07)\n",
      "Old & New Losses 1957.6737880706787 1959.6606492996216 Probab: tensor(0.9969, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 206, LR: 31.2500, Train Loss: 1.9570, Train Accuracy: 36.80%, Temperatures:(0.13, 630.70)\n",
      "Old & New Losses 1955.254316329956 1959.7114324569702 Probab: tensor(0.9930, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 207, LR: 31.2500, Train Loss: 1.9603, Train Accuracy: 39.60%, Temperatures:(0.12, 624.39)\n",
      "Old & New Losses 1950.3819942474365 1956.2567472457886 Probab: tensor(0.9906, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 208, LR: 31.2500, Train Loss: 1.9613, Train Accuracy: 44.10%, Temperatures:(0.12, 618.15)\n",
      "Old & New Losses 1950.8295059204102 1950.189232826233 Probab: tensor(1.0010, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 209, LR: 31.2500, Train Loss: 1.9551, Train Accuracy: 41.10%, Temperatures:(0.12, 611.96)\n",
      "Old & New Losses 1951.3866901397705 1960.3923559188843 Probab: tensor(0.9854, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 210, LR: 31.2500, Train Loss: 1.9544, Train Accuracy: 43.00%, Temperatures:(0.12, 605.84)\n",
      "Old & New Losses 1948.6674070358276 1946.5285539627075 Probab: tensor(1.0035, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 211, LR: 31.2500, Train Loss: 1.9645, Train Accuracy: 43.20%, Temperatures:(0.12, 599.79)\n",
      "Old & New Losses 1956.7394256591797 1957.0072889328003 Probab: tensor(0.9996, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 212, LR: 31.2500, Train Loss: 1.9528, Train Accuracy: 41.30%, Temperatures:(0.12, 593.79)\n",
      "Old & New Losses 1943.4382915496826 1948.1990337371826 Probab: tensor(0.9920, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 213, LR: 31.2500, Train Loss: 1.9587, Train Accuracy: 42.00%, Temperatures:(0.12, 587.85)\n",
      "Old & New Losses 1946.8519687652588 1947.0868110656738 Probab: tensor(0.9996, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 214, LR: 31.2500, Train Loss: 1.9424, Train Accuracy: 44.40%, Temperatures:(0.12, 581.97)\n",
      "Old & New Losses 1942.3247575759888 1944.9806213378906 Probab: tensor(0.9954, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 215, LR: 31.2500, Train Loss: 1.9494, Train Accuracy: 44.20%, Temperatures:(0.12, 576.15)\n",
      "Old & New Losses 1942.171573638916 1939.3970966339111 Probab: tensor(1.0048, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 216, LR: 31.2500, Train Loss: 1.9471, Train Accuracy: 46.90%, Temperatures:(0.11, 570.39)\n",
      "Old & New Losses 1942.2770738601685 1944.3707466125488 Probab: tensor(0.9963, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 217, LR: 31.2500, Train Loss: 1.9476, Train Accuracy: 47.40%, Temperatures:(0.11, 564.69)\n",
      "Old & New Losses 1942.371129989624 1945.6737041473389 Probab: tensor(0.9942, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 218, LR: 31.2500, Train Loss: 1.9461, Train Accuracy: 46.00%, Temperatures:(0.11, 559.04)\n",
      "Old & New Losses 1938.3763074874878 1939.0932321548462 Probab: tensor(0.9987, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 219, LR: 31.2500, Train Loss: 1.9413, Train Accuracy: 47.50%, Temperatures:(0.11, 553.45)\n",
      "Old & New Losses 1941.9420957565308 1939.4093751907349 Probab: tensor(1.0046, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 220, LR: 31.2500, Train Loss: 1.9387, Train Accuracy: 46.60%, Temperatures:(0.11, 547.91)\n",
      "Old & New Losses 1930.2451610565186 1938.056468963623 Probab: tensor(0.9858, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 221, LR: 31.2500, Train Loss: 1.9383, Train Accuracy: 48.10%, Temperatures:(0.11, 542.44)\n",
      "Old & New Losses 1932.5488805770874 1936.9946718215942 Probab: tensor(0.9918, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 222, LR: 31.2500, Train Loss: 1.9375, Train Accuracy: 47.40%, Temperatures:(0.11, 537.01)\n",
      "Old & New Losses 1933.5155487060547 1933.9559078216553 Probab: tensor(0.9992, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 223, LR: 31.2500, Train Loss: 1.9306, Train Accuracy: 46.10%, Temperatures:(0.11, 531.64)\n",
      "Old & New Losses 1925.8623123168945 1939.8680925369263 Probab: tensor(0.9740, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 224, LR: 31.2500, Train Loss: 1.9291, Train Accuracy: 45.60%, Temperatures:(0.11, 526.32)\n",
      "Old & New Losses 1926.7691373825073 1942.9513216018677 Probab: tensor(0.9697, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 225, LR: 31.2500, Train Loss: 1.9394, Train Accuracy: 45.90%, Temperatures:(0.10, 521.06)\n",
      "Old & New Losses 1929.6090602874756 1930.7184219360352 Probab: tensor(0.9979, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 226, LR: 31.2500, Train Loss: 1.9395, Train Accuracy: 44.40%, Temperatures:(0.10, 515.85)\n",
      "Old & New Losses 1924.7994422912598 1927.1496534347534 Probab: tensor(0.9955, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 227, LR: 31.2500, Train Loss: 1.9358, Train Accuracy: 45.90%, Temperatures:(0.10, 510.69)\n",
      "Old & New Losses 1929.7776222229004 1933.2540035247803 Probab: tensor(0.9932, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 228, LR: 31.2500, Train Loss: 1.9267, Train Accuracy: 48.10%, Temperatures:(0.10, 505.59)\n",
      "Old & New Losses 1926.347255706787 1927.6905059814453 Probab: tensor(0.9973, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 229, LR: 31.2500, Train Loss: 1.9325, Train Accuracy: 44.90%, Temperatures:(0.10, 500.53)\n",
      "Old & New Losses 1939.198613166809 1934.8366260528564 Probab: tensor(1.0088, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 230, LR: 31.2500, Train Loss: 1.9312, Train Accuracy: 42.70%, Temperatures:(0.10, 495.52)\n",
      "Old & New Losses 1923.3663082122803 1925.303339958191 Probab: tensor(0.9961, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 231, LR: 31.2500, Train Loss: 1.9344, Train Accuracy: 40.20%, Temperatures:(0.10, 490.57)\n",
      "Old & New Losses 1930.5579662322998 1933.8903427124023 Probab: tensor(0.9932, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 232, LR: 31.2500, Train Loss: 1.9297, Train Accuracy: 45.90%, Temperatures:(0.10, 485.66)\n",
      "Old & New Losses 1924.8182773590088 1928.6123514175415 Probab: tensor(0.9922, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 233, LR: 31.2500, Train Loss: 1.9336, Train Accuracy: 42.50%, Temperatures:(0.10, 480.81)\n",
      "Old & New Losses 1931.9063425064087 1929.0001392364502 Probab: tensor(1.0061, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 234, LR: 31.2500, Train Loss: 1.9257, Train Accuracy: 45.70%, Temperatures:(0.10, 476.00)\n",
      "Old & New Losses 1930.3730726242065 1930.8750629425049 Probab: tensor(0.9989, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 235, LR: 31.2500, Train Loss: 1.9305, Train Accuracy: 42.80%, Temperatures:(0.09, 471.24)\n",
      "Old & New Losses 1927.4647235870361 1928.3559322357178 Probab: tensor(0.9981, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 236, LR: 31.2500, Train Loss: 1.9309, Train Accuracy: 46.80%, Temperatures:(0.09, 466.53)\n",
      "Old & New Losses 1931.870937347412 1924.9814748764038 Probab: tensor(1.0149, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 237, LR: 31.2500, Train Loss: 1.9234, Train Accuracy: 47.50%, Temperatures:(0.09, 461.86)\n",
      "Old & New Losses 1922.0914840698242 1927.6448488235474 Probab: tensor(0.9880, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 238, LR: 31.2500, Train Loss: 1.9230, Train Accuracy: 47.00%, Temperatures:(0.09, 457.24)\n",
      "Old & New Losses 1917.9704189300537 1925.6397485733032 Probab: tensor(0.9834, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 239, LR: 31.2500, Train Loss: 1.9258, Train Accuracy: 50.20%, Temperatures:(0.09, 452.67)\n",
      "Old & New Losses 1930.6737184524536 1926.2783527374268 Probab: tensor(1.0098, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 240, LR: 31.2500, Train Loss: 1.9254, Train Accuracy: 46.30%, Temperatures:(0.09, 448.14)\n",
      "Old & New Losses 1921.2981462478638 1932.2675466537476 Probab: tensor(0.9758, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 241, LR: 31.2500, Train Loss: 1.9236, Train Accuracy: 44.40%, Temperatures:(0.09, 443.66)\n",
      "Old & New Losses 1922.9027032852173 1921.7841625213623 Probab: tensor(1.0025, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 242, LR: 31.2500, Train Loss: 1.9319, Train Accuracy: 46.30%, Temperatures:(0.09, 439.23)\n",
      "Old & New Losses 1929.0369749069214 1932.794451713562 Probab: tensor(0.9915, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 243, LR: 31.2500, Train Loss: 1.9225, Train Accuracy: 45.00%, Temperatures:(0.09, 434.83)\n",
      "Old & New Losses 1921.2040901184082 1916.1232709884644 Probab: tensor(1.0118, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 244, LR: 31.2500, Train Loss: 1.9309, Train Accuracy: 44.00%, Temperatures:(0.09, 430.48)\n",
      "Old & New Losses 1915.4908657073975 1920.9028482437134 Probab: tensor(0.9875, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 245, LR: 31.2500, Train Loss: 1.9184, Train Accuracy: 44.10%, Temperatures:(0.09, 426.18)\n",
      "Old & New Losses 1921.3998317718506 1926.0934591293335 Probab: tensor(0.9890, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 246, LR: 31.2500, Train Loss: 1.9204, Train Accuracy: 44.00%, Temperatures:(0.08, 421.92)\n",
      "Old & New Losses 1918.7856912612915 1917.6957607269287 Probab: tensor(1.0026, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 247, LR: 31.2500, Train Loss: 1.9261, Train Accuracy: 43.50%, Temperatures:(0.08, 417.70)\n",
      "Old & New Losses 1921.3924407958984 1923.880934715271 Probab: tensor(0.9941, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 248, LR: 31.2500, Train Loss: 1.9184, Train Accuracy: 40.60%, Temperatures:(0.08, 413.52)\n",
      "Old & New Losses 1919.8492765426636 1924.5221614837646 Probab: tensor(0.9888, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 249, LR: 31.2500, Train Loss: 1.9275, Train Accuracy: 42.00%, Temperatures:(0.08, 409.39)\n",
      "Old & New Losses 1917.8398847579956 1924.9433279037476 Probab: tensor(0.9828, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 250, LR: 31.2500, Train Loss: 1.9248, Train Accuracy: 39.70%, Temperatures:(0.08, 405.29)\n",
      "Old & New Losses 1918.7966585159302 1924.6559143066406 Probab: tensor(0.9856, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 251, LR: 31.2500, Train Loss: 1.9243, Train Accuracy: 42.30%, Temperatures:(0.08, 401.24)\n",
      "Old & New Losses 1916.6635274887085 1922.706127166748 Probab: tensor(0.9851, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 252, LR: 15.6250, Train Loss: 1.9188, Train Accuracy: 40.00%, Temperatures:(0.08, 397.23)\n",
      "Old & New Losses 1920.9905862808228 1923.9410161972046 Probab: tensor(0.9926, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 253, LR: 15.6250, Train Loss: 1.9223, Train Accuracy: 41.10%, Temperatures:(0.08, 393.25)\n",
      "Old & New Losses 1919.071912765503 1919.127345085144 Probab: tensor(0.9999, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 254, LR: 15.6250, Train Loss: 1.9270, Train Accuracy: 39.80%, Temperatures:(0.08, 389.32)\n",
      "Old & New Losses 1922.0142364501953 1928.194522857666 Probab: tensor(0.9843, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 255, LR: 15.6250, Train Loss: 1.9227, Train Accuracy: 41.10%, Temperatures:(0.08, 385.43)\n",
      "Old & New Losses 1919.4395542144775 1918.9928770065308 Probab: tensor(1.0012, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 256, LR: 15.6250, Train Loss: 1.9323, Train Accuracy: 40.30%, Temperatures:(0.08, 381.57)\n",
      "Old & New Losses 1927.9687404632568 1926.4063835144043 Probab: tensor(1.0041, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 257, LR: 15.6250, Train Loss: 1.9189, Train Accuracy: 45.00%, Temperatures:(0.08, 377.76)\n",
      "Old & New Losses 1920.6693172454834 1924.8034954071045 Probab: tensor(0.9891, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 258, LR: 15.6250, Train Loss: 1.9293, Train Accuracy: 41.70%, Temperatures:(0.07, 373.98)\n",
      "Old & New Losses 1927.9404878616333 1924.6690273284912 Probab: tensor(1.0088, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 259, LR: 15.6250, Train Loss: 1.9279, Train Accuracy: 43.30%, Temperatures:(0.07, 370.24)\n",
      "Old & New Losses 1921.7456579208374 1926.4323711395264 Probab: tensor(0.9874, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 260, LR: 15.6250, Train Loss: 1.9210, Train Accuracy: 45.50%, Temperatures:(0.07, 366.54)\n",
      "Old & New Losses 1924.3344068527222 1917.86789894104 Probab: tensor(1.0178, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 261, LR: 15.6250, Train Loss: 1.9208, Train Accuracy: 48.70%, Temperatures:(0.07, 362.87)\n",
      "Old & New Losses 1923.996090888977 1927.593469619751 Probab: tensor(0.9901, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 262, LR: 15.6250, Train Loss: 1.9242, Train Accuracy: 43.90%, Temperatures:(0.07, 359.25)\n",
      "Old & New Losses 1925.3687858581543 1925.234079360962 Probab: tensor(1.0004, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 263, LR: 15.6250, Train Loss: 1.9244, Train Accuracy: 45.20%, Temperatures:(0.07, 355.65)\n",
      "Old & New Losses 1927.4238348007202 1924.256443977356 Probab: tensor(1.0089, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 264, LR: 15.6250, Train Loss: 1.9235, Train Accuracy: 42.40%, Temperatures:(0.07, 352.10)\n",
      "Old & New Losses 1922.9565858840942 1932.0517778396606 Probab: tensor(0.9745, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 265, LR: 15.6250, Train Loss: 1.9256, Train Accuracy: 44.60%, Temperatures:(0.07, 348.58)\n",
      "Old & New Losses 1926.2851476669312 1925.458550453186 Probab: tensor(1.0024, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 266, LR: 15.6250, Train Loss: 1.9286, Train Accuracy: 44.10%, Temperatures:(0.07, 345.09)\n",
      "Old & New Losses 1930.9577941894531 1931.2583208084106 Probab: tensor(0.9991, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 267, LR: 15.6250, Train Loss: 1.9271, Train Accuracy: 43.50%, Temperatures:(0.07, 341.64)\n",
      "Old & New Losses 1931.53715133667 1937.2568130493164 Probab: tensor(0.9834, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 268, LR: 15.6250, Train Loss: 1.9323, Train Accuracy: 41.70%, Temperatures:(0.07, 338.22)\n",
      "Old & New Losses 1935.2120161056519 1958.4704637527466 Probab: tensor(0.9335, device='cuda:0')\n",
      "Epoch 269, LR: 15.6250, Train Loss: 1.9330, Train Accuracy: 42.50%, Temperatures:(0.07, 334.84)\n",
      "Old & New Losses 1930.471420288086 1930.7692050933838 Probab: tensor(0.9991, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 270, LR: 15.6250, Train Loss: 1.9362, Train Accuracy: 42.60%, Temperatures:(0.07, 331.49)\n",
      "Old & New Losses 1929.4939041137695 1936.7008209228516 Probab: tensor(0.9785, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 271, LR: 15.6250, Train Loss: 1.9335, Train Accuracy: 43.40%, Temperatures:(0.07, 328.18)\n",
      "Old & New Losses 1932.8635931015015 1932.6486587524414 Probab: tensor(1.0007, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 272, LR: 15.6250, Train Loss: 1.9349, Train Accuracy: 40.20%, Temperatures:(0.06, 324.89)\n",
      "Old & New Losses 1923.4471321105957 1938.9156103134155 Probab: tensor(0.9535, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 273, LR: 15.6250, Train Loss: 1.9294, Train Accuracy: 41.90%, Temperatures:(0.06, 321.65)\n",
      "Old & New Losses 1931.9323301315308 1931.0004711151123 Probab: tensor(1.0029, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 274, LR: 15.6250, Train Loss: 1.9348, Train Accuracy: 38.40%, Temperatures:(0.06, 318.43)\n",
      "Old & New Losses 1931.4810037612915 1936.2449645996094 Probab: tensor(0.9852, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 275, LR: 15.6250, Train Loss: 1.9289, Train Accuracy: 42.10%, Temperatures:(0.06, 315.25)\n",
      "Old & New Losses 1928.8755655288696 1931.993007659912 Probab: tensor(0.9902, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 276, LR: 15.6250, Train Loss: 1.9342, Train Accuracy: 39.80%, Temperatures:(0.06, 312.09)\n",
      "Old & New Losses 1931.4899444580078 1938.1057024002075 Probab: tensor(0.9790, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 277, LR: 15.6250, Train Loss: 1.9290, Train Accuracy: 42.80%, Temperatures:(0.06, 308.97)\n",
      "Old & New Losses 1928.6707639694214 1930.2115440368652 Probab: tensor(0.9950, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 278, LR: 15.6250, Train Loss: 1.9317, Train Accuracy: 40.10%, Temperatures:(0.06, 305.88)\n",
      "Old & New Losses 1924.2265224456787 1931.639552116394 Probab: tensor(0.9761, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 279, LR: 15.6250, Train Loss: 1.9282, Train Accuracy: 40.90%, Temperatures:(0.06, 302.82)\n",
      "Old & New Losses 1929.0941953659058 1942.1104192733765 Probab: tensor(0.9579, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 280, LR: 15.6250, Train Loss: 1.9260, Train Accuracy: 41.60%, Temperatures:(0.06, 299.80)\n",
      "Old & New Losses 1925.4136085510254 1924.6466159820557 Probab: tensor(1.0026, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 281, LR: 15.6250, Train Loss: 1.9362, Train Accuracy: 40.30%, Temperatures:(0.06, 296.80)\n",
      "Old & New Losses 1924.039363861084 1924.6402978897095 Probab: tensor(0.9980, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 282, LR: 15.6250, Train Loss: 1.9229, Train Accuracy: 42.70%, Temperatures:(0.06, 293.83)\n",
      "Old & New Losses 1923.0265617370605 1924.5704412460327 Probab: tensor(0.9948, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 283, LR: 15.6250, Train Loss: 1.9311, Train Accuracy: 41.50%, Temperatures:(0.06, 290.89)\n",
      "Old & New Losses 1924.8636960983276 1925.8862733840942 Probab: tensor(0.9965, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 284, LR: 15.6250, Train Loss: 1.9269, Train Accuracy: 43.10%, Temperatures:(0.06, 287.98)\n",
      "Old & New Losses 1929.2746782302856 1923.559308052063 Probab: tensor(1.0200, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 285, LR: 15.6250, Train Loss: 1.9250, Train Accuracy: 46.70%, Temperatures:(0.06, 285.10)\n",
      "Old & New Losses 1926.3392686843872 1923.5116243362427 Probab: tensor(1.0100, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 286, LR: 15.6250, Train Loss: 1.9250, Train Accuracy: 45.10%, Temperatures:(0.06, 282.25)\n",
      "Old & New Losses 1923.5148429870605 1926.7783164978027 Probab: tensor(0.9885, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 287, LR: 15.6250, Train Loss: 1.9271, Train Accuracy: 45.90%, Temperatures:(0.06, 279.43)\n",
      "Old & New Losses 1925.0982999801636 1928.8876056671143 Probab: tensor(0.9865, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 288, LR: 15.6250, Train Loss: 1.9261, Train Accuracy: 44.40%, Temperatures:(0.06, 276.63)\n",
      "Old & New Losses 1924.5678186416626 1925.1872301101685 Probab: tensor(0.9978, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 289, LR: 15.6250, Train Loss: 1.9290, Train Accuracy: 42.50%, Temperatures:(0.05, 273.87)\n",
      "Old & New Losses 1923.6807823181152 1930.734395980835 Probab: tensor(0.9746, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 290, LR: 15.6250, Train Loss: 1.9269, Train Accuracy: 44.90%, Temperatures:(0.05, 271.13)\n",
      "Old & New Losses 1927.4213314056396 1922.7111339569092 Probab: tensor(1.0175, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 291, LR: 15.6250, Train Loss: 1.9327, Train Accuracy: 43.70%, Temperatures:(0.05, 268.42)\n",
      "Old & New Losses 1928.558111190796 1929.263949394226 Probab: tensor(0.9974, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 292, LR: 15.6250, Train Loss: 1.9222, Train Accuracy: 46.50%, Temperatures:(0.05, 265.73)\n",
      "Old & New Losses 1929.9695491790771 1925.6623983383179 Probab: tensor(1.0163, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 293, LR: 15.6250, Train Loss: 1.9341, Train Accuracy: 44.10%, Temperatures:(0.05, 263.08)\n",
      "Old & New Losses 1927.9698133468628 1927.4200201034546 Probab: tensor(1.0021, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 294, LR: 15.6250, Train Loss: 1.9268, Train Accuracy: 45.20%, Temperatures:(0.05, 260.45)\n",
      "Old & New Losses 1931.4097166061401 1938.3141994476318 Probab: tensor(0.9738, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 295, LR: 15.6250, Train Loss: 1.9271, Train Accuracy: 42.50%, Temperatures:(0.05, 257.84)\n",
      "Old & New Losses 1929.1539192199707 1928.0165433883667 Probab: tensor(1.0044, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 296, LR: 15.6250, Train Loss: 1.9427, Train Accuracy: 43.60%, Temperatures:(0.05, 255.26)\n",
      "Old & New Losses 1936.5097284317017 1944.7376728057861 Probab: tensor(0.9683, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 297, LR: 15.6250, Train Loss: 1.9256, Train Accuracy: 43.80%, Temperatures:(0.05, 252.71)\n",
      "Old & New Losses 1926.6541004180908 1934.0765476226807 Probab: tensor(0.9711, device='cuda:0')\n",
      "Epoch 298, LR: 15.6250, Train Loss: 1.9418, Train Accuracy: 43.50%, Temperatures:(0.05, 250.18)\n",
      "Old & New Losses 1929.3464422225952 1928.0041456222534 Probab: tensor(1.0054, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 299, LR: 15.6250, Train Loss: 1.9309, Train Accuracy: 44.40%, Temperatures:(0.05, 247.68)\n",
      "Old & New Losses 1928.4422397613525 1926.6948699951172 Probab: tensor(1.0071, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 300, LR: 15.6250, Train Loss: 1.9295, Train Accuracy: 43.00%, Temperatures:(0.05, 245.20)\n",
      "Old & New Losses 1933.081030845642 1941.2890672683716 Probab: tensor(0.9671, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 301, LR: 15.6250, Train Loss: 1.9305, Train Accuracy: 41.70%, Temperatures:(0.05, 242.75)\n",
      "Old & New Losses 1932.112455368042 1943.1416988372803 Probab: tensor(0.9556, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 302, LR: 7.8125, Train Loss: 1.9411, Train Accuracy: 40.40%, Temperatures:(0.05, 240.32)\n",
      "Old & New Losses 1937.5231266021729 1932.3456287384033 Probab: tensor(1.0218, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 303, LR: 7.8125, Train Loss: 1.9437, Train Accuracy: 39.50%, Temperatures:(0.05, 237.92)\n",
      "Old & New Losses 1940.9990310668945 1938.7975931167603 Probab: tensor(1.0093, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 304, LR: 7.8125, Train Loss: 1.9370, Train Accuracy: 40.20%, Temperatures:(0.05, 235.54)\n",
      "Old & New Losses 1937.5187158584595 1942.6792860031128 Probab: tensor(0.9783, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 305, LR: 7.8125, Train Loss: 1.9369, Train Accuracy: 40.60%, Temperatures:(0.05, 233.19)\n",
      "Old & New Losses 1941.5581226348877 1938.2214546203613 Probab: tensor(1.0144, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 306, LR: 7.8125, Train Loss: 1.9385, Train Accuracy: 39.40%, Temperatures:(0.05, 230.86)\n",
      "Old & New Losses 1944.037675857544 1942.7303075790405 Probab: tensor(1.0057, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 307, LR: 7.8125, Train Loss: 1.9365, Train Accuracy: 41.90%, Temperatures:(0.05, 228.55)\n",
      "Old & New Losses 1938.0712509155273 1943.0201053619385 Probab: tensor(0.9786, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 308, LR: 7.8125, Train Loss: 1.9440, Train Accuracy: 42.70%, Temperatures:(0.05, 226.26)\n",
      "Old & New Losses 1939.7127628326416 1945.0222253799438 Probab: tensor(0.9768, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 309, LR: 7.8125, Train Loss: 1.9436, Train Accuracy: 41.10%, Temperatures:(0.04, 224.00)\n",
      "Old & New Losses 1943.8377618789673 1945.5333948135376 Probab: tensor(0.9925, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 310, LR: 7.8125, Train Loss: 1.9445, Train Accuracy: 43.50%, Temperatures:(0.04, 221.76)\n",
      "Old & New Losses 1949.8323202133179 1942.7316188812256 Probab: tensor(1.0325, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 311, LR: 7.8125, Train Loss: 1.9441, Train Accuracy: 44.70%, Temperatures:(0.04, 219.54)\n",
      "Old & New Losses 1942.612648010254 1949.509620666504 Probab: tensor(0.9691, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 312, LR: 7.8125, Train Loss: 1.9444, Train Accuracy: 42.00%, Temperatures:(0.04, 217.35)\n",
      "Old & New Losses 1952.4565935134888 1952.0550966262817 Probab: tensor(1.0018, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 313, LR: 7.8125, Train Loss: 1.9500, Train Accuracy: 43.30%, Temperatures:(0.04, 215.17)\n",
      "Old & New Losses 1949.0811824798584 1956.6923379898071 Probab: tensor(0.9652, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 314, LR: 7.8125, Train Loss: 1.9513, Train Accuracy: 44.70%, Temperatures:(0.04, 213.02)\n",
      "Old & New Losses 1955.5585384368896 1955.1531076431274 Probab: tensor(1.0019, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 315, LR: 7.8125, Train Loss: 1.9580, Train Accuracy: 44.00%, Temperatures:(0.04, 210.89)\n",
      "Old & New Losses 1951.6631364822388 1952.609658241272 Probab: tensor(0.9955, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 316, LR: 7.8125, Train Loss: 1.9564, Train Accuracy: 43.30%, Temperatures:(0.04, 208.78)\n",
      "Old & New Losses 1955.5854797363281 1957.4555158615112 Probab: tensor(0.9911, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 317, LR: 7.8125, Train Loss: 1.9545, Train Accuracy: 44.20%, Temperatures:(0.04, 206.69)\n",
      "Old & New Losses 1955.2620649337769 1961.6817235946655 Probab: tensor(0.9694, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 318, LR: 7.8125, Train Loss: 1.9592, Train Accuracy: 43.90%, Temperatures:(0.04, 204.63)\n",
      "Old & New Losses 1961.5795612335205 1964.605689048767 Probab: tensor(0.9853, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 319, LR: 7.8125, Train Loss: 1.9631, Train Accuracy: 41.50%, Temperatures:(0.04, 202.58)\n",
      "Old & New Losses 1957.3177099227905 1955.1304578781128 Probab: tensor(1.0109, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 320, LR: 7.8125, Train Loss: 1.9627, Train Accuracy: 40.70%, Temperatures:(0.04, 200.55)\n",
      "Old & New Losses 1957.8640460968018 1968.0973291397095 Probab: tensor(0.9503, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 321, LR: 7.8125, Train Loss: 1.9612, Train Accuracy: 38.50%, Temperatures:(0.04, 198.55)\n",
      "Old & New Losses 1962.1078968048096 1964.4042253494263 Probab: tensor(0.9885, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 322, LR: 7.8125, Train Loss: 1.9663, Train Accuracy: 40.70%, Temperatures:(0.04, 196.56)\n",
      "Old & New Losses 1969.3584442138672 1963.4664058685303 Probab: tensor(1.0304, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 323, LR: 7.8125, Train Loss: 1.9592, Train Accuracy: 39.40%, Temperatures:(0.04, 194.60)\n",
      "Old & New Losses 1963.2669687271118 1961.87162399292 Probab: tensor(1.0072, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 324, LR: 7.8125, Train Loss: 1.9621, Train Accuracy: 39.00%, Temperatures:(0.04, 192.65)\n",
      "Old & New Losses 1964.8371934890747 1962.180256843567 Probab: tensor(1.0139, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 325, LR: 7.8125, Train Loss: 1.9644, Train Accuracy: 40.10%, Temperatures:(0.04, 190.73)\n",
      "Old & New Losses 1964.4708633422852 1965.160846710205 Probab: tensor(0.9964, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 326, LR: 7.8125, Train Loss: 1.9657, Train Accuracy: 40.50%, Temperatures:(0.04, 188.82)\n",
      "Old & New Losses 1964.2584323883057 1963.3326530456543 Probab: tensor(1.0049, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 327, LR: 7.8125, Train Loss: 1.9674, Train Accuracy: 40.30%, Temperatures:(0.04, 186.93)\n",
      "Old & New Losses 1965.1930332183838 1966.0614728927612 Probab: tensor(0.9954, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 328, LR: 7.8125, Train Loss: 1.9662, Train Accuracy: 40.90%, Temperatures:(0.04, 185.06)\n",
      "Old & New Losses 1963.2412195205688 1967.3296213150024 Probab: tensor(0.9782, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 329, LR: 7.8125, Train Loss: 1.9712, Train Accuracy: 42.40%, Temperatures:(0.04, 183.21)\n",
      "Old & New Losses 1965.7031297683716 1964.7128582000732 Probab: tensor(1.0054, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 330, LR: 7.8125, Train Loss: 1.9608, Train Accuracy: 43.80%, Temperatures:(0.04, 181.38)\n",
      "Old & New Losses 1963.5916948318481 1961.9951248168945 Probab: tensor(1.0088, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 331, LR: 7.8125, Train Loss: 1.9709, Train Accuracy: 41.00%, Temperatures:(0.04, 179.56)\n",
      "Old & New Losses 1960.8759880065918 1974.9265909194946 Probab: tensor(0.9247, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 332, LR: 7.8125, Train Loss: 1.9657, Train Accuracy: 41.70%, Temperatures:(0.04, 177.77)\n",
      "Old & New Losses 1962.0381593704224 1966.9725894927979 Probab: tensor(0.9726, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 333, LR: 7.8125, Train Loss: 1.9641, Train Accuracy: 41.00%, Temperatures:(0.04, 175.99)\n",
      "Old & New Losses 1963.241457939148 1967.871069908142 Probab: tensor(0.9740, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 334, LR: 7.8125, Train Loss: 1.9697, Train Accuracy: 40.60%, Temperatures:(0.03, 174.23)\n",
      "Old & New Losses 1966.7656421661377 1968.769907951355 Probab: tensor(0.9886, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 335, LR: 7.8125, Train Loss: 1.9695, Train Accuracy: 41.10%, Temperatures:(0.03, 172.49)\n",
      "Old & New Losses 1968.7529802322388 1973.7050533294678 Probab: tensor(0.9717, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 336, LR: 7.8125, Train Loss: 1.9699, Train Accuracy: 41.60%, Temperatures:(0.03, 170.76)\n",
      "Old & New Losses 1970.1896905899048 1973.073959350586 Probab: tensor(0.9833, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 337, LR: 7.8125, Train Loss: 1.9733, Train Accuracy: 37.40%, Temperatures:(0.03, 169.06)\n",
      "Old & New Losses 1973.9165306091309 1969.2646265029907 Probab: tensor(1.0279, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 338, LR: 7.8125, Train Loss: 1.9720, Train Accuracy: 39.90%, Temperatures:(0.03, 167.37)\n",
      "Old & New Losses 1966.6825532913208 1968.7973260879517 Probab: tensor(0.9874, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 339, LR: 7.8125, Train Loss: 1.9679, Train Accuracy: 40.20%, Temperatures:(0.03, 165.69)\n",
      "Old & New Losses 1965.1572704315186 1970.2515602111816 Probab: tensor(0.9697, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 340, LR: 7.8125, Train Loss: 1.9674, Train Accuracy: 39.00%, Temperatures:(0.03, 164.03)\n",
      "Old & New Losses 1965.739369392395 1967.225432395935 Probab: tensor(0.9910, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 341, LR: 7.8125, Train Loss: 1.9707, Train Accuracy: 38.90%, Temperatures:(0.03, 162.39)\n",
      "Old & New Losses 1965.1716947555542 1965.1262760162354 Probab: tensor(1.0003, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 342, LR: 7.8125, Train Loss: 1.9653, Train Accuracy: 40.80%, Temperatures:(0.03, 160.77)\n",
      "Old & New Losses 1964.1937017440796 1962.1100425720215 Probab: tensor(1.0130, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 343, LR: 7.8125, Train Loss: 1.9667, Train Accuracy: 39.40%, Temperatures:(0.03, 159.16)\n",
      "Old & New Losses 1965.0639295578003 1966.2758111953735 Probab: tensor(0.9924, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 344, LR: 7.8125, Train Loss: 1.9696, Train Accuracy: 39.70%, Temperatures:(0.03, 157.57)\n",
      "Old & New Losses 1968.6580896377563 1963.4315967559814 Probab: tensor(1.0337, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 345, LR: 7.8125, Train Loss: 1.9674, Train Accuracy: 38.90%, Temperatures:(0.03, 156.00)\n",
      "Old & New Losses 1968.5375690460205 1965.0181531906128 Probab: tensor(1.0228, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 346, LR: 7.8125, Train Loss: 1.9696, Train Accuracy: 39.00%, Temperatures:(0.03, 154.44)\n",
      "Old & New Losses 1967.5933122634888 1977.4764776229858 Probab: tensor(0.9380, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 347, LR: 7.8125, Train Loss: 1.9622, Train Accuracy: 42.50%, Temperatures:(0.03, 152.89)\n",
      "Old & New Losses 1970.6110954284668 1978.5025119781494 Probab: tensor(0.9497, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 348, LR: 7.8125, Train Loss: 1.9735, Train Accuracy: 40.80%, Temperatures:(0.03, 151.36)\n",
      "Old & New Losses 1982.338786125183 1973.22678565979 Probab: tensor(1.0620, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 349, LR: 7.8125, Train Loss: 1.9775, Train Accuracy: 40.10%, Temperatures:(0.03, 149.85)\n",
      "Old & New Losses 1977.731466293335 1977.8215885162354 Probab: tensor(0.9994, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 350, LR: 7.8125, Train Loss: 1.9726, Train Accuracy: 38.80%, Temperatures:(0.03, 148.35)\n",
      "Old & New Losses 1970.8888530731201 1974.3009805679321 Probab: tensor(0.9773, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 351, LR: 7.8125, Train Loss: 1.9742, Train Accuracy: 42.60%, Temperatures:(0.03, 146.87)\n",
      "Old & New Losses 1973.463773727417 1976.02379322052 Probab: tensor(0.9827, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 352, LR: 3.9062, Train Loss: 1.9748, Train Accuracy: 41.70%, Temperatures:(0.03, 145.40)\n",
      "Old & New Losses 1970.2943563461304 1975.7517576217651 Probab: tensor(0.9632, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 353, LR: 3.9062, Train Loss: 1.9765, Train Accuracy: 40.50%, Temperatures:(0.03, 143.94)\n",
      "Old & New Losses 1979.7875881195068 1979.5087575912476 Probab: tensor(1.0019, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 354, LR: 3.9062, Train Loss: 1.9767, Train Accuracy: 39.70%, Temperatures:(0.03, 142.50)\n",
      "Old & New Losses 1978.4475564956665 1977.744460105896 Probab: tensor(1.0049, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 355, LR: 3.9062, Train Loss: 1.9813, Train Accuracy: 40.10%, Temperatures:(0.03, 141.08)\n",
      "Old & New Losses 1977.3651361465454 1975.0527143478394 Probab: tensor(1.0165, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 356, LR: 3.9062, Train Loss: 1.9766, Train Accuracy: 38.10%, Temperatures:(0.03, 139.67)\n",
      "Old & New Losses 1972.8184938430786 1989.9979829788208 Probab: tensor(0.8843, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 357, LR: 3.9062, Train Loss: 1.9758, Train Accuracy: 42.60%, Temperatures:(0.03, 138.27)\n",
      "Old & New Losses 1982.4494123458862 1976.826786994934 Probab: tensor(1.0415, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 358, LR: 3.9062, Train Loss: 1.9894, Train Accuracy: 40.20%, Temperatures:(0.03, 136.89)\n",
      "Old & New Losses 1975.2377271652222 1980.3731441497803 Probab: tensor(0.9632, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 359, LR: 3.9062, Train Loss: 1.9801, Train Accuracy: 42.00%, Temperatures:(0.03, 135.52)\n",
      "Old & New Losses 1978.3769845962524 1980.5947542190552 Probab: tensor(0.9838, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 360, LR: 3.9062, Train Loss: 1.9770, Train Accuracy: 41.60%, Temperatures:(0.03, 134.17)\n",
      "Old & New Losses 1974.9656915664673 1978.7335395812988 Probab: tensor(0.9723, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 361, LR: 3.9062, Train Loss: 1.9781, Train Accuracy: 41.00%, Temperatures:(0.03, 132.82)\n",
      "Old & New Losses 1983.5010766983032 1983.5501909255981 Probab: tensor(0.9996, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 362, LR: 3.9062, Train Loss: 1.9791, Train Accuracy: 40.90%, Temperatures:(0.03, 131.50)\n",
      "Old & New Losses 1978.8848161697388 1981.0876846313477 Probab: tensor(0.9834, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 363, LR: 3.9062, Train Loss: 1.9794, Train Accuracy: 40.90%, Temperatures:(0.03, 130.18)\n",
      "Old & New Losses 1980.226755142212 1985.6960773468018 Probab: tensor(0.9589, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 364, LR: 3.9062, Train Loss: 1.9843, Train Accuracy: 37.80%, Temperatures:(0.03, 128.88)\n",
      "Old & New Losses 1983.6854934692383 1990.7232522964478 Probab: tensor(0.9469, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 365, LR: 3.9062, Train Loss: 1.9884, Train Accuracy: 40.80%, Temperatures:(0.03, 127.59)\n",
      "Old & New Losses 1983.02161693573 1979.5578718185425 Probab: tensor(1.0275, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 366, LR: 3.9062, Train Loss: 1.9931, Train Accuracy: 35.70%, Temperatures:(0.03, 126.31)\n",
      "Old & New Losses 1972.8548526763916 1979.316234588623 Probab: tensor(0.9501, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 367, LR: 3.9062, Train Loss: 1.9762, Train Accuracy: 37.90%, Temperatures:(0.03, 125.05)\n",
      "Old & New Losses 1972.5134372711182 1976.098895072937 Probab: tensor(0.9717, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 368, LR: 3.9062, Train Loss: 1.9729, Train Accuracy: 40.20%, Temperatures:(0.02, 123.80)\n",
      "Old & New Losses 1974.4818210601807 1979.6644449234009 Probab: tensor(0.9590, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 369, LR: 3.9062, Train Loss: 1.9783, Train Accuracy: 37.90%, Temperatures:(0.02, 122.56)\n",
      "Old & New Losses 1976.5570163726807 1983.7981462478638 Probab: tensor(0.9426, device='cuda:0')\n",
      "Epoch 370, LR: 3.9062, Train Loss: 1.9829, Train Accuracy: 38.40%, Temperatures:(0.02, 121.34)\n",
      "Old & New Losses 1978.1047105789185 1973.72567653656 Probab: tensor(1.0367, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 371, LR: 3.9062, Train Loss: 1.9808, Train Accuracy: 37.70%, Temperatures:(0.02, 120.12)\n",
      "Old & New Losses 1975.267767906189 1977.8296947479248 Probab: tensor(0.9789, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 372, LR: 3.9062, Train Loss: 1.9782, Train Accuracy: 38.00%, Temperatures:(0.02, 118.92)\n",
      "Old & New Losses 1972.6260900497437 1974.0281105041504 Probab: tensor(0.9883, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 373, LR: 3.9062, Train Loss: 1.9774, Train Accuracy: 38.00%, Temperatures:(0.02, 117.73)\n",
      "Old & New Losses 1975.2118587493896 1976.1831760406494 Probab: tensor(0.9918, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 374, LR: 3.9062, Train Loss: 1.9724, Train Accuracy: 37.60%, Temperatures:(0.02, 116.56)\n",
      "Old & New Losses 1974.8179912567139 2002.5038719177246 Probab: tensor(0.7886, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 375, LR: 3.9062, Train Loss: 1.9716, Train Accuracy: 35.60%, Temperatures:(0.02, 115.39)\n",
      "Old & New Losses 1989.2504215240479 1986.185073852539 Probab: tensor(1.0269, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 376, LR: 3.9062, Train Loss: 2.0000, Train Accuracy: 34.30%, Temperatures:(0.02, 114.24)\n",
      "Old & New Losses 1981.1310768127441 1983.1664562225342 Probab: tensor(0.9823, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 377, LR: 3.9062, Train Loss: 1.9899, Train Accuracy: 34.10%, Temperatures:(0.02, 113.09)\n",
      "Old & New Losses 1978.402853012085 1978.8161516189575 Probab: tensor(0.9964, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 378, LR: 3.9062, Train Loss: 1.9818, Train Accuracy: 35.60%, Temperatures:(0.02, 111.96)\n",
      "Old & New Losses 1975.0570058822632 1980.0138473510742 Probab: tensor(0.9567, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 379, LR: 3.9062, Train Loss: 1.9800, Train Accuracy: 35.60%, Temperatures:(0.02, 110.84)\n",
      "Old & New Losses 1977.6580333709717 1978.3567190170288 Probab: tensor(0.9937, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 380, LR: 3.9062, Train Loss: 1.9833, Train Accuracy: 35.40%, Temperatures:(0.02, 109.73)\n",
      "Old & New Losses 1977.2028923034668 1979.1600704193115 Probab: tensor(0.9823, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 381, LR: 3.9062, Train Loss: 1.9781, Train Accuracy: 36.70%, Temperatures:(0.02, 108.64)\n",
      "Old & New Losses 1977.2963523864746 1986.2487316131592 Probab: tensor(0.9209, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 382, LR: 3.9062, Train Loss: 1.9784, Train Accuracy: 35.60%, Temperatures:(0.02, 107.55)\n",
      "Old & New Losses 1978.7741899490356 1979.6888828277588 Probab: tensor(0.9915, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 383, LR: 3.9062, Train Loss: 1.9882, Train Accuracy: 37.80%, Temperatures:(0.02, 106.48)\n",
      "Old & New Losses 1983.9868545532227 1981.998324394226 Probab: tensor(1.0189, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 384, LR: 3.9062, Train Loss: 1.9835, Train Accuracy: 37.00%, Temperatures:(0.02, 105.41)\n",
      "Old & New Losses 1980.5365800857544 1982.4320077896118 Probab: tensor(0.9822, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 385, LR: 3.9062, Train Loss: 1.9834, Train Accuracy: 38.30%, Temperatures:(0.02, 104.36)\n",
      "Old & New Losses 1978.2220125198364 1978.2241582870483 Probab: tensor(1.0000, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 386, LR: 3.9062, Train Loss: 1.9787, Train Accuracy: 37.50%, Temperatures:(0.02, 103.31)\n",
      "Old & New Losses 1978.6429405212402 1978.8920879364014 Probab: tensor(0.9976, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 387, LR: 3.9062, Train Loss: 1.9797, Train Accuracy: 35.10%, Temperatures:(0.02, 102.28)\n",
      "Old & New Losses 1978.9644479751587 1975.4430055618286 Probab: tensor(1.0350, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 388, LR: 3.9062, Train Loss: 1.9784, Train Accuracy: 37.20%, Temperatures:(0.02, 101.26)\n",
      "Old & New Losses 1971.4845418930054 1974.1872549057007 Probab: tensor(0.9737, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 389, LR: 3.9062, Train Loss: 1.9758, Train Accuracy: 37.30%, Temperatures:(0.02, 100.24)\n",
      "Old & New Losses 1973.331093788147 1975.8318662643433 Probab: tensor(0.9754, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 390, LR: 3.9062, Train Loss: 1.9768, Train Accuracy: 35.90%, Temperatures:(0.02, 99.24)\n",
      "Old & New Losses 1975.8867025375366 1979.7589778900146 Probab: tensor(0.9617, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 391, LR: 3.9062, Train Loss: 1.9723, Train Accuracy: 38.40%, Temperatures:(0.02, 98.25)\n",
      "Old & New Losses 1978.0017137527466 1980.6838035583496 Probab: tensor(0.9731, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 392, LR: 3.9062, Train Loss: 1.9801, Train Accuracy: 36.70%, Temperatures:(0.02, 97.27)\n",
      "Old & New Losses 1974.5618104934692 1978.1512022018433 Probab: tensor(0.9638, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 393, LR: 3.9062, Train Loss: 1.9763, Train Accuracy: 35.60%, Temperatures:(0.02, 96.29)\n",
      "Old & New Losses 1977.8027534484863 1981.0937643051147 Probab: tensor(0.9664, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 394, LR: 3.9062, Train Loss: 1.9781, Train Accuracy: 37.50%, Temperatures:(0.02, 95.33)\n",
      "Old & New Losses 1976.2386083602905 1986.358642578125 Probab: tensor(0.8993, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 395, LR: 3.9062, Train Loss: 1.9824, Train Accuracy: 36.00%, Temperatures:(0.02, 94.38)\n",
      "Old & New Losses 1977.2549867630005 1982.3119640350342 Probab: tensor(0.9478, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 396, LR: 3.9062, Train Loss: 1.9827, Train Accuracy: 37.60%, Temperatures:(0.02, 93.43)\n",
      "Old & New Losses 1979.7098636627197 1982.4353456497192 Probab: tensor(0.9713, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 397, LR: 3.9062, Train Loss: 1.9824, Train Accuracy: 36.70%, Temperatures:(0.02, 92.50)\n",
      "Old & New Losses 1978.939414024353 1982.9285144805908 Probab: tensor(0.9578, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 398, LR: 3.9062, Train Loss: 1.9769, Train Accuracy: 38.10%, Temperatures:(0.02, 91.58)\n",
      "Old & New Losses 1975.8613109588623 1977.0309925079346 Probab: tensor(0.9873, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 399, LR: 3.9062, Train Loss: 1.9809, Train Accuracy: 35.80%, Temperatures:(0.02, 90.66)\n",
      "Old & New Losses 1974.9146699905396 1979.7183275222778 Probab: tensor(0.9484, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 400, LR: 3.9062, Train Loss: 1.9791, Train Accuracy: 36.40%, Temperatures:(0.02, 89.75)\n",
      "Old & New Losses 1979.7618389129639 1975.2739667892456 Probab: tensor(1.0513, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 401, LR: 3.9062, Train Loss: 1.9744, Train Accuracy: 36.70%, Temperatures:(0.02, 88.86)\n",
      "Old & New Losses 1981.9295406341553 1982.1571111679077 Probab: tensor(0.9974, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 402, LR: 1.9531, Train Loss: 1.9783, Train Accuracy: 38.90%, Temperatures:(0.02, 87.97)\n",
      "Old & New Losses 1978.2050848007202 1981.7756414413452 Probab: tensor(0.9602, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 403, LR: 1.9531, Train Loss: 1.9795, Train Accuracy: 37.00%, Temperatures:(0.02, 87.09)\n",
      "Old & New Losses 1977.7073860168457 1979.9619913101196 Probab: tensor(0.9744, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 404, LR: 1.9531, Train Loss: 1.9808, Train Accuracy: 36.40%, Temperatures:(0.02, 86.22)\n",
      "Old & New Losses 1976.9213199615479 1983.0702543258667 Probab: tensor(0.9312, device='cuda:0')\n",
      "Epoch 405, LR: 1.9531, Train Loss: 1.9784, Train Accuracy: 37.10%, Temperatures:(0.02, 85.35)\n",
      "Old & New Losses 1977.802038192749 1982.212781906128 Probab: tensor(0.9496, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 406, LR: 1.9531, Train Loss: 1.9804, Train Accuracy: 38.00%, Temperatures:(0.02, 84.50)\n",
      "Old & New Losses 1973.9739894866943 1978.6759614944458 Probab: tensor(0.9459, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 407, LR: 1.9531, Train Loss: 1.9786, Train Accuracy: 37.20%, Temperatures:(0.02, 83.66)\n",
      "Old & New Losses 1970.9012508392334 1976.9340753555298 Probab: tensor(0.9304, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 408, LR: 1.9531, Train Loss: 1.9770, Train Accuracy: 37.70%, Temperatures:(0.02, 82.82)\n",
      "Old & New Losses 1978.951334953308 1989.2704486846924 Probab: tensor(0.8829, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 409, LR: 1.9531, Train Loss: 1.9798, Train Accuracy: 37.60%, Temperatures:(0.02, 81.99)\n",
      "Old & New Losses 1985.3854179382324 1987.3650074005127 Probab: tensor(0.9761, device='cuda:0')\n",
      "Epoch 410, LR: 1.9531, Train Loss: 1.9889, Train Accuracy: 36.30%, Temperatures:(0.02, 81.17)\n",
      "Old & New Losses 1981.829047203064 1987.403154373169 Probab: tensor(0.9336, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 411, LR: 1.9531, Train Loss: 1.9834, Train Accuracy: 37.20%, Temperatures:(0.02, 80.36)\n",
      "Old & New Losses 1981.4813137054443 1988.2997274398804 Probab: tensor(0.9187, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 412, LR: 1.9531, Train Loss: 1.9863, Train Accuracy: 37.40%, Temperatures:(0.02, 79.56)\n",
      "Old & New Losses 1983.0472469329834 1980.6232452392578 Probab: tensor(1.0309, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 413, LR: 1.9531, Train Loss: 1.9819, Train Accuracy: 37.50%, Temperatures:(0.02, 78.76)\n",
      "Old & New Losses 1979.9526929855347 1987.0887994766235 Probab: tensor(0.9134, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 414, LR: 1.9531, Train Loss: 1.9808, Train Accuracy: 37.50%, Temperatures:(0.02, 77.97)\n",
      "Old & New Losses 1978.295087814331 1985.2640628814697 Probab: tensor(0.9145, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 415, LR: 1.9531, Train Loss: 1.9831, Train Accuracy: 38.00%, Temperatures:(0.02, 77.19)\n",
      "Old & New Losses 1982.8470945358276 1988.0342483520508 Probab: tensor(0.9350, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 416, LR: 1.9531, Train Loss: 1.9823, Train Accuracy: 37.90%, Temperatures:(0.02, 76.42)\n",
      "Old & New Losses 1985.386848449707 1987.0147705078125 Probab: tensor(0.9789, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 417, LR: 1.9531, Train Loss: 1.9826, Train Accuracy: 37.70%, Temperatures:(0.02, 75.66)\n",
      "Old & New Losses 1987.0502948760986 1984.3473434448242 Probab: tensor(1.0364, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 418, LR: 1.9531, Train Loss: 1.9850, Train Accuracy: 38.70%, Temperatures:(0.01, 74.90)\n",
      "Old & New Losses 1986.1323833465576 1986.1091375350952 Probab: tensor(1.0003, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 419, LR: 1.9531, Train Loss: 1.9850, Train Accuracy: 37.30%, Temperatures:(0.01, 74.15)\n",
      "Old & New Losses 1988.6788129806519 1987.6775741577148 Probab: tensor(1.0136, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 420, LR: 1.9531, Train Loss: 1.9903, Train Accuracy: 38.10%, Temperatures:(0.01, 73.41)\n",
      "Old & New Losses 1986.299753189087 1988.6890649795532 Probab: tensor(0.9680, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 421, LR: 1.9531, Train Loss: 1.9913, Train Accuracy: 38.60%, Temperatures:(0.01, 72.68)\n",
      "Old & New Losses 1990.818977355957 1996.7238903045654 Probab: tensor(0.9220, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 422, LR: 1.9531, Train Loss: 1.9911, Train Accuracy: 38.40%, Temperatures:(0.01, 71.95)\n",
      "Old & New Losses 1991.5918111801147 1994.9132204055786 Probab: tensor(0.9549, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 423, LR: 1.9531, Train Loss: 1.9942, Train Accuracy: 37.70%, Temperatures:(0.01, 71.23)\n",
      "Old & New Losses 1990.6446933746338 1993.469476699829 Probab: tensor(0.9611, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 424, LR: 1.9531, Train Loss: 1.9970, Train Accuracy: 35.90%, Temperatures:(0.01, 70.52)\n",
      "Old & New Losses 1988.8103008270264 1996.1307048797607 Probab: tensor(0.9014, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 425, LR: 1.9531, Train Loss: 1.9972, Train Accuracy: 37.40%, Temperatures:(0.01, 69.81)\n",
      "Old & New Losses 1996.0150718688965 1994.7401285171509 Probab: tensor(1.0184, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 426, LR: 1.9531, Train Loss: 1.9930, Train Accuracy: 36.60%, Temperatures:(0.01, 69.11)\n",
      "Old & New Losses 1991.5263652801514 1987.859606742859 Probab: tensor(1.0545, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 427, LR: 1.9531, Train Loss: 1.9972, Train Accuracy: 36.50%, Temperatures:(0.01, 68.42)\n",
      "Old & New Losses 1993.6248064041138 1992.16890335083 Probab: tensor(1.0215, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 428, LR: 1.9531, Train Loss: 1.9904, Train Accuracy: 37.60%, Temperatures:(0.01, 67.74)\n",
      "Old & New Losses 1992.8935766220093 1993.275761604309 Probab: tensor(0.9944, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 429, LR: 1.9531, Train Loss: 1.9919, Train Accuracy: 37.30%, Temperatures:(0.01, 67.06)\n",
      "Old & New Losses 1991.5813207626343 1998.9244937896729 Probab: tensor(0.8963, device='cuda:0')\n",
      "Epoch 430, LR: 1.9531, Train Loss: 1.9956, Train Accuracy: 36.70%, Temperatures:(0.01, 66.39)\n",
      "Old & New Losses 1993.0768013000488 1999.767541885376 Probab: tensor(0.9041, device='cuda:0')\n",
      "Epoch 431, LR: 1.9531, Train Loss: 1.9926, Train Accuracy: 38.80%, Temperatures:(0.01, 65.73)\n",
      "Old & New Losses 1988.609790802002 1992.6544427871704 Probab: tensor(0.9403, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 432, LR: 1.9531, Train Loss: 1.9923, Train Accuracy: 37.50%, Temperatures:(0.01, 65.07)\n",
      "Old & New Losses 1996.0557222366333 1993.349552154541 Probab: tensor(1.0425, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 433, LR: 1.9531, Train Loss: 1.9948, Train Accuracy: 36.60%, Temperatures:(0.01, 64.42)\n",
      "Old & New Losses 1986.6706132888794 1994.205355644226 Probab: tensor(0.8896, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 434, LR: 1.9531, Train Loss: 1.9889, Train Accuracy: 38.00%, Temperatures:(0.01, 63.77)\n",
      "Old & New Losses 1990.044355392456 1993.4873580932617 Probab: tensor(0.9474, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 435, LR: 1.9531, Train Loss: 1.9931, Train Accuracy: 38.20%, Temperatures:(0.01, 63.14)\n",
      "Old & New Losses 1993.4662580490112 1994.2584037780762 Probab: tensor(0.9875, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 436, LR: 1.9531, Train Loss: 1.9910, Train Accuracy: 37.30%, Temperatures:(0.01, 62.51)\n",
      "Old & New Losses 1985.6741428375244 1997.3244667053223 Probab: tensor(0.8299, device='cuda:0')\n",
      "Epoch 437, LR: 1.9531, Train Loss: 1.9915, Train Accuracy: 36.80%, Temperatures:(0.01, 61.88)\n",
      "Old & New Losses 1986.6734743118286 1991.734266281128 Probab: tensor(0.9215, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 438, LR: 1.9531, Train Loss: 1.9881, Train Accuracy: 38.30%, Temperatures:(0.01, 61.26)\n",
      "Old & New Losses 1994.9028491973877 2000.917673110962 Probab: tensor(0.9065, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 439, LR: 1.9531, Train Loss: 1.9922, Train Accuracy: 36.50%, Temperatures:(0.01, 60.65)\n",
      "Old & New Losses 1993.8896894454956 2000.1788139343262 Probab: tensor(0.9015, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 440, LR: 1.9531, Train Loss: 1.9992, Train Accuracy: 36.60%, Temperatures:(0.01, 60.04)\n",
      "Old & New Losses 1995.3398704528809 1995.6897497177124 Probab: tensor(0.9942, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 441, LR: 1.9531, Train Loss: 1.9947, Train Accuracy: 37.80%, Temperatures:(0.01, 59.44)\n",
      "Old & New Losses 2001.7144680023193 2001.8482208251953 Probab: tensor(0.9978, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 442, LR: 1.9531, Train Loss: 1.9980, Train Accuracy: 35.80%, Temperatures:(0.01, 58.85)\n",
      "Old & New Losses 1995.6915378570557 1997.9069232940674 Probab: tensor(0.9631, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 443, LR: 1.9531, Train Loss: 1.9994, Train Accuracy: 34.70%, Temperatures:(0.01, 58.26)\n",
      "Old & New Losses 1996.1857795715332 1996.9147443771362 Probab: tensor(0.9876, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 444, LR: 1.9531, Train Loss: 2.0008, Train Accuracy: 31.00%, Temperatures:(0.01, 57.68)\n",
      "Old & New Losses 1997.098684310913 1998.2609748840332 Probab: tensor(0.9800, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 445, LR: 1.9531, Train Loss: 2.0021, Train Accuracy: 30.40%, Temperatures:(0.01, 57.10)\n",
      "Old & New Losses 1991.7304515838623 1996.9063997268677 Probab: tensor(0.9133, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 446, LR: 1.9531, Train Loss: 1.9972, Train Accuracy: 31.20%, Temperatures:(0.01, 56.53)\n",
      "Old & New Losses 1993.1561946868896 2004.0640830993652 Probab: tensor(0.8245, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 447, LR: 1.9531, Train Loss: 2.0007, Train Accuracy: 32.10%, Temperatures:(0.01, 55.96)\n",
      "Old & New Losses 1997.9674816131592 1995.9250688552856 Probab: tensor(1.0372, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 448, LR: 1.9531, Train Loss: 2.0014, Train Accuracy: 31.50%, Temperatures:(0.01, 55.40)\n",
      "Old & New Losses 1998.5640048980713 1995.6762790679932 Probab: tensor(1.0535, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 449, LR: 1.9531, Train Loss: 1.9923, Train Accuracy: 30.90%, Temperatures:(0.01, 54.85)\n",
      "Old & New Losses 1997.3535537719727 2004.3213367462158 Probab: tensor(0.8807, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 450, LR: 1.9531, Train Loss: 2.0002, Train Accuracy: 31.20%, Temperatures:(0.01, 54.30)\n",
      "Old & New Losses 1998.6374378204346 2006.531000137329 Probab: tensor(0.8647, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 451, LR: 1.9531, Train Loss: 2.0022, Train Accuracy: 30.10%, Temperatures:(0.01, 53.76)\n",
      "Old & New Losses 1998.514175415039 2019.4497108459473 Probab: tensor(0.6774, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 452, LR: 0.9766, Train Loss: 1.9977, Train Accuracy: 31.80%, Temperatures:(0.01, 53.22)\n",
      "Old & New Losses 2012.253999710083 1998.4371662139893 Probab: tensor(1.2964, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 453, LR: 0.9766, Train Loss: 2.0187, Train Accuracy: 30.90%, Temperatures:(0.01, 52.69)\n",
      "Old & New Losses 2002.2776126861572 1999.59397315979 Probab: tensor(1.0523, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 454, LR: 0.9766, Train Loss: 1.9967, Train Accuracy: 34.50%, Temperatures:(0.01, 52.16)\n",
      "Old & New Losses 1997.1117973327637 1993.7777519226074 Probab: tensor(1.0660, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 455, LR: 0.9766, Train Loss: 1.9955, Train Accuracy: 33.60%, Temperatures:(0.01, 51.64)\n",
      "Old & New Losses 1992.2513961791992 1996.9408512115479 Probab: tensor(0.9132, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 456, LR: 0.9766, Train Loss: 1.9975, Train Accuracy: 31.60%, Temperatures:(0.01, 51.12)\n",
      "Old & New Losses 1995.6111907958984 1999.001145362854 Probab: tensor(0.9358, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 457, LR: 0.9766, Train Loss: 2.0001, Train Accuracy: 32.70%, Temperatures:(0.01, 50.61)\n",
      "Old & New Losses 1999.0309476852417 2001.9619464874268 Probab: tensor(0.9437, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 458, LR: 0.9766, Train Loss: 1.9964, Train Accuracy: 34.20%, Temperatures:(0.01, 50.11)\n",
      "Old & New Losses 1997.6391792297363 1999.3698596954346 Probab: tensor(0.9660, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 459, LR: 0.9766, Train Loss: 2.0015, Train Accuracy: 33.60%, Temperatures:(0.01, 49.60)\n",
      "Old & New Losses 2003.0992031097412 1995.4229593276978 Probab: tensor(1.1674, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 460, LR: 0.9766, Train Loss: 1.9984, Train Accuracy: 34.10%, Temperatures:(0.01, 49.11)\n",
      "Old & New Losses 2001.9075870513916 1998.4995126724243 Probab: tensor(1.0719, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 461, LR: 0.9766, Train Loss: 1.9994, Train Accuracy: 33.40%, Temperatures:(0.01, 48.62)\n",
      "Old & New Losses 2001.2173652648926 2002.9292106628418 Probab: tensor(0.9654, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 462, LR: 0.9766, Train Loss: 2.0065, Train Accuracy: 32.70%, Temperatures:(0.01, 48.13)\n",
      "Old & New Losses 2002.253532409668 2001.2197494506836 Probab: tensor(1.0217, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 463, LR: 0.9766, Train Loss: 2.0018, Train Accuracy: 36.60%, Temperatures:(0.01, 47.65)\n",
      "Old & New Losses 2000.8490085601807 2006.2134265899658 Probab: tensor(0.8935, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 464, LR: 0.9766, Train Loss: 2.0049, Train Accuracy: 36.80%, Temperatures:(0.01, 47.17)\n",
      "Old & New Losses 2001.8117427825928 2004.225254058838 Probab: tensor(0.9501, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 465, LR: 0.9766, Train Loss: 2.0059, Train Accuracy: 37.30%, Temperatures:(0.01, 46.70)\n",
      "Old & New Losses 2015.1934623718262 2008.955478668213 Probab: tensor(1.1429, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 466, LR: 0.9766, Train Loss: 2.0110, Train Accuracy: 36.60%, Temperatures:(0.01, 46.23)\n",
      "Old & New Losses 2006.753921508789 2008.6863040924072 Probab: tensor(0.9591, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 467, LR: 0.9766, Train Loss: 2.0017, Train Accuracy: 36.50%, Temperatures:(0.01, 45.77)\n",
      "Old & New Losses 2006.4661502838135 2008.6264610290527 Probab: tensor(0.9539, device='cuda:0')\n",
      "Epoch 468, LR: 0.9766, Train Loss: 2.0103, Train Accuracy: 35.00%, Temperatures:(0.01, 45.31)\n",
      "Old & New Losses 2004.8470497131348 2010.166883468628 Probab: tensor(0.8892, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 469, LR: 0.9766, Train Loss: 2.0076, Train Accuracy: 35.50%, Temperatures:(0.01, 44.86)\n",
      "Old & New Losses 2008.3105564117432 2017.2176361083984 Probab: tensor(0.8199, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 470, LR: 0.9766, Train Loss: 2.0116, Train Accuracy: 35.60%, Temperatures:(0.01, 44.41)\n",
      "Old & New Losses 2014.678716659546 2029.6518802642822 Probab: tensor(0.7138, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 471, LR: 0.9766, Train Loss: 2.0124, Train Accuracy: 35.70%, Temperatures:(0.01, 43.97)\n",
      "Old & New Losses 2027.4972915649414 2024.9719619750977 Probab: tensor(1.0591, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 472, LR: 0.9766, Train Loss: 2.0315, Train Accuracy: 31.80%, Temperatures:(0.01, 43.53)\n",
      "Old & New Losses 2020.9310054779053 2010.4055404663086 Probab: tensor(1.2735, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 473, LR: 0.9766, Train Loss: 2.0233, Train Accuracy: 33.40%, Temperatures:(0.01, 43.09)\n",
      "Old & New Losses 2016.4237022399902 2016.526222229004 Probab: tensor(0.9976, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 474, LR: 0.9766, Train Loss: 2.0198, Train Accuracy: 34.60%, Temperatures:(0.01, 42.66)\n",
      "Old & New Losses 2017.918586730957 2016.4241790771484 Probab: tensor(1.0356, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 475, LR: 0.9766, Train Loss: 2.0130, Train Accuracy: 36.10%, Temperatures:(0.01, 42.24)\n",
      "Old & New Losses 2017.2829627990723 2017.4319744110107 Probab: tensor(0.9965, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 476, LR: 0.9766, Train Loss: 2.0166, Train Accuracy: 33.60%, Temperatures:(0.01, 41.81)\n",
      "Old & New Losses 2014.263391494751 2020.0648307800293 Probab: tensor(0.8705, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 477, LR: 0.9766, Train Loss: 2.0177, Train Accuracy: 34.40%, Temperatures:(0.01, 41.40)\n",
      "Old & New Losses 2019.3963050842285 2016.679286956787 Probab: tensor(1.0678, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 478, LR: 0.9766, Train Loss: 2.0169, Train Accuracy: 34.40%, Temperatures:(0.01, 40.98)\n",
      "Old & New Losses 2012.683391571045 2011.6851329803467 Probab: tensor(1.0247, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 479, LR: 0.9766, Train Loss: 2.0153, Train Accuracy: 33.70%, Temperatures:(0.01, 40.57)\n",
      "Old & New Losses 2009.901523590088 2012.1803283691406 Probab: tensor(0.9454, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 480, LR: 0.9766, Train Loss: 2.0072, Train Accuracy: 35.30%, Temperatures:(0.01, 40.17)\n",
      "Old & New Losses 2011.7404460906982 2013.941764831543 Probab: tensor(0.9467, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 481, LR: 0.9766, Train Loss: 2.0064, Train Accuracy: 34.60%, Temperatures:(0.01, 39.76)\n",
      "Old & New Losses 2006.0703754425049 2008.352279663086 Probab: tensor(0.9442, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 482, LR: 0.9766, Train Loss: 2.0139, Train Accuracy: 34.70%, Temperatures:(0.01, 39.37)\n",
      "Old & New Losses 2008.5723400115967 2012.679100036621 Probab: tensor(0.9009, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 483, LR: 0.9766, Train Loss: 2.0157, Train Accuracy: 34.20%, Temperatures:(0.01, 38.97)\n",
      "Old & New Losses 2010.0407600402832 2019.8016166687012 Probab: tensor(0.7785, device='cuda:0')\n",
      "Epoch 484, LR: 0.9766, Train Loss: 2.0144, Train Accuracy: 34.30%, Temperatures:(0.01, 38.58)\n",
      "Old & New Losses 2007.889986038208 2006.7169666290283 Probab: tensor(1.0309, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 485, LR: 0.9766, Train Loss: 2.0126, Train Accuracy: 38.00%, Temperatures:(0.01, 38.20)\n",
      "Old & New Losses 2006.9468021392822 2005.5046081542969 Probab: tensor(1.0385, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 486, LR: 0.9766, Train Loss: 2.0062, Train Accuracy: 35.10%, Temperatures:(0.01, 37.82)\n",
      "Old & New Losses 2005.838394165039 2005.3398609161377 Probab: tensor(1.0133, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 487, LR: 0.9766, Train Loss: 2.0018, Train Accuracy: 35.80%, Temperatures:(0.01, 37.44)\n",
      "Old & New Losses 2008.070707321167 2006.1461925506592 Probab: tensor(1.0527, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 488, LR: 0.9766, Train Loss: 2.0085, Train Accuracy: 37.50%, Temperatures:(0.01, 37.06)\n",
      "Old & New Losses 2006.3316822052002 2006.7675113677979 Probab: tensor(0.9883, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 489, LR: 0.9766, Train Loss: 2.0094, Train Accuracy: 37.30%, Temperatures:(0.01, 36.69)\n",
      "Old & New Losses 2004.4374465942383 2010.8706951141357 Probab: tensor(0.8392, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 490, LR: 0.9766, Train Loss: 2.0143, Train Accuracy: 37.20%, Temperatures:(0.01, 36.33)\n",
      "Old & New Losses 2006.0701370239258 2007.9450607299805 Probab: tensor(0.9497, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 491, LR: 0.9766, Train Loss: 2.0010, Train Accuracy: 37.10%, Temperatures:(0.01, 35.96)\n",
      "Old & New Losses 2003.953456878662 2003.9522647857666 Probab: tensor(1.0000, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 492, LR: 0.9766, Train Loss: 2.0052, Train Accuracy: 35.20%, Temperatures:(0.01, 35.60)\n",
      "Old & New Losses 2001.6274452209473 2008.4633827209473 Probab: tensor(0.8253, device='cuda:0')\n",
      "Epoch 493, LR: 0.9766, Train Loss: 2.0052, Train Accuracy: 38.80%, Temperatures:(0.01, 35.25)\n",
      "Old & New Losses 1996.7364072799683 2005.4876804351807 Probab: tensor(0.7801, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 494, LR: 0.9766, Train Loss: 2.0039, Train Accuracy: 37.10%, Temperatures:(0.01, 34.89)\n",
      "Old & New Losses 2006.9975852966309 2005.71870803833 Probab: tensor(1.0373, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 495, LR: 0.9766, Train Loss: 2.0025, Train Accuracy: 38.10%, Temperatures:(0.01, 34.55)\n",
      "Old & New Losses 2002.8502941131592 2004.1172504425049 Probab: tensor(0.9640, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 496, LR: 0.9766, Train Loss: 2.0025, Train Accuracy: 38.40%, Temperatures:(0.01, 34.20)\n",
      "Old & New Losses 1996.164083480835 2012.9778385162354 Probab: tensor(0.6116, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 497, LR: 0.9766, Train Loss: 2.0081, Train Accuracy: 38.30%, Temperatures:(0.01, 33.86)\n",
      "Old & New Losses 2010.0317001342773 2007.7786445617676 Probab: tensor(1.0688, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 498, LR: 0.9766, Train Loss: 2.0107, Train Accuracy: 35.80%, Temperatures:(0.01, 33.52)\n",
      "Old & New Losses 2004.7807693481445 2009.9031925201416 Probab: tensor(0.8583, device='cuda:0')\n",
      "Epoch 499, LR: 0.9766, Train Loss: 2.0054, Train Accuracy: 36.80%, Temperatures:(0.01, 33.18)\n",
      "Old & New Losses 2003.490924835205 2003.2281875610352 Probab: tensor(1.0079, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 500, LR: 0.9766, Train Loss: 2.0039, Train Accuracy: 36.90%, Temperatures:(0.01, 32.85)\n",
      "Old & New Losses 2004.1625499725342 2013.8452053070068 Probab: tensor(0.7447, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 501, LR: 0.9766, Train Loss: 2.0106, Train Accuracy: 35.20%, Temperatures:(0.01, 32.52)\n",
      "Old & New Losses 2009.74702835083 2012.6380920410156 Probab: tensor(0.9149, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 502, LR: 0.4883, Train Loss: 2.0139, Train Accuracy: 36.80%, Temperatures:(0.01, 32.20)\n",
      "Old & New Losses 2011.9822025299072 2008.2650184631348 Probab: tensor(1.1224, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 503, LR: 0.4883, Train Loss: 2.0067, Train Accuracy: 37.00%, Temperatures:(0.01, 31.88)\n",
      "Old & New Losses 2009.9470615386963 2014.5363807678223 Probab: tensor(0.8659, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 504, LR: 0.4883, Train Loss: 2.0145, Train Accuracy: 36.20%, Temperatures:(0.01, 31.56)\n",
      "Old & New Losses 2013.7336254119873 2010.5006694793701 Probab: tensor(1.1079, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 505, LR: 0.4883, Train Loss: 2.0123, Train Accuracy: 37.70%, Temperatures:(0.01, 31.24)\n",
      "Old & New Losses 2012.2406482696533 2013.1912231445312 Probab: tensor(0.9700, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 506, LR: 0.4883, Train Loss: 2.0097, Train Accuracy: 38.90%, Temperatures:(0.01, 30.93)\n",
      "Old & New Losses 2012.7856731414795 2012.1793746948242 Probab: tensor(1.0198, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 507, LR: 0.4883, Train Loss: 2.0124, Train Accuracy: 37.90%, Temperatures:(0.01, 30.62)\n",
      "Old & New Losses 2014.4970417022705 2013.692855834961 Probab: tensor(1.0266, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 508, LR: 0.4883, Train Loss: 2.0129, Train Accuracy: 38.00%, Temperatures:(0.01, 30.31)\n",
      "Old & New Losses 2012.9282474517822 2013.4613513946533 Probab: tensor(0.9826, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 509, LR: 0.4883, Train Loss: 2.0169, Train Accuracy: 37.70%, Temperatures:(0.01, 30.01)\n",
      "Old & New Losses 2013.228416442871 2016.3240432739258 Probab: tensor(0.9020, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 510, LR: 0.4883, Train Loss: 2.0110, Train Accuracy: 36.60%, Temperatures:(0.01, 29.71)\n",
      "Old & New Losses 2016.655445098877 2017.0247554779053 Probab: tensor(0.9876, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 511, LR: 0.4883, Train Loss: 2.0127, Train Accuracy: 36.20%, Temperatures:(0.01, 29.41)\n",
      "Old & New Losses 2016.3533687591553 2018.9309120178223 Probab: tensor(0.9161, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 512, LR: 0.4883, Train Loss: 2.0172, Train Accuracy: 37.60%, Temperatures:(0.01, 29.12)\n",
      "Old & New Losses 2014.4727230072021 2011.2838745117188 Probab: tensor(1.1157, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 513, LR: 0.4883, Train Loss: 2.0100, Train Accuracy: 37.70%, Temperatures:(0.01, 28.83)\n",
      "Old & New Losses 2018.6092853546143 2018.0509090423584 Probab: tensor(1.0196, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 514, LR: 0.4883, Train Loss: 2.0144, Train Accuracy: 35.40%, Temperatures:(0.01, 28.54)\n",
      "Old & New Losses 2017.3494815826416 2022.0048427581787 Probab: tensor(0.8495, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 515, LR: 0.4883, Train Loss: 2.0192, Train Accuracy: 36.50%, Temperatures:(0.01, 28.25)\n",
      "Old & New Losses 2021.7039585113525 2025.2268314361572 Probab: tensor(0.8828, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 516, LR: 0.4883, Train Loss: 2.0238, Train Accuracy: 34.70%, Temperatures:(0.01, 27.97)\n",
      "Old & New Losses 2022.0704078674316 2028.1167030334473 Probab: tensor(0.8056, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 517, LR: 0.4883, Train Loss: 2.0234, Train Accuracy: 36.70%, Temperatures:(0.01, 27.69)\n",
      "Old & New Losses 2028.1918048858643 2030.318021774292 Probab: tensor(0.9261, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 518, LR: 0.4883, Train Loss: 2.0317, Train Accuracy: 35.80%, Temperatures:(0.01, 27.42)\n",
      "Old & New Losses 2032.4053764343262 2038.952350616455 Probab: tensor(0.7876, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 519, LR: 0.4883, Train Loss: 2.0252, Train Accuracy: 36.90%, Temperatures:(0.01, 27.14)\n",
      "Old & New Losses 2037.4813079833984 2045.182228088379 Probab: tensor(0.7530, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 520, LR: 0.4883, Train Loss: 2.0419, Train Accuracy: 35.30%, Temperatures:(0.01, 26.87)\n",
      "Old & New Losses 2041.003942489624 2057.7924251556396 Probab: tensor(0.5354, device='cuda:0')\n",
      "Epoch 521, LR: 0.4883, Train Loss: 2.0485, Train Accuracy: 33.10%, Temperatures:(0.01, 26.60)\n",
      "Old & New Losses 2042.2630310058594 2045.6759929656982 Probab: tensor(0.8796, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 522, LR: 0.4883, Train Loss: 2.0470, Train Accuracy: 33.40%, Temperatures:(0.01, 26.34)\n",
      "Old & New Losses 2042.5093173980713 2043.015480041504 Probab: tensor(0.9810, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 523, LR: 0.4883, Train Loss: 2.0457, Train Accuracy: 35.40%, Temperatures:(0.01, 26.07)\n",
      "Old & New Losses 2036.825180053711 2048.8250255584717 Probab: tensor(0.6311, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 524, LR: 0.4883, Train Loss: 2.0407, Train Accuracy: 33.60%, Temperatures:(0.01, 25.81)\n",
      "Old & New Losses 2045.748233795166 2045.5725193023682 Probab: tensor(1.0068, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 525, LR: 0.4883, Train Loss: 2.0439, Train Accuracy: 34.10%, Temperatures:(0.01, 25.55)\n",
      "Old & New Losses 2045.2704429626465 2049.0989685058594 Probab: tensor(0.8609, device='cuda:0')\n",
      "Epoch 526, LR: 0.4883, Train Loss: 2.0453, Train Accuracy: 33.10%, Temperatures:(0.01, 25.30)\n",
      "Old & New Losses 2042.856216430664 2037.721872329712 Probab: tensor(1.2250, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 527, LR: 0.4883, Train Loss: 2.0489, Train Accuracy: 34.80%, Temperatures:(0.01, 25.04)\n",
      "Old & New Losses 2041.8860912322998 2044.1920757293701 Probab: tensor(0.9120, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 528, LR: 0.4883, Train Loss: 2.0418, Train Accuracy: 35.60%, Temperatures:(0.00, 24.79)\n",
      "Old & New Losses 2037.625789642334 2044.9514389038086 Probab: tensor(0.7442, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 529, LR: 0.4883, Train Loss: 2.0343, Train Accuracy: 36.20%, Temperatures:(0.00, 24.55)\n",
      "Old & New Losses 2041.621446609497 2040.7650470733643 Probab: tensor(1.0355, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 530, LR: 0.4883, Train Loss: 2.0464, Train Accuracy: 35.10%, Temperatures:(0.00, 24.30)\n",
      "Old & New Losses 2040.7960414886475 2042.9093837738037 Probab: tensor(0.9167, device='cuda:0')\n",
      "Epoch 531, LR: 0.4883, Train Loss: 2.0442, Train Accuracy: 35.50%, Temperatures:(0.00, 24.06)\n",
      "Old & New Losses 2039.902925491333 2032.999038696289 Probab: tensor(1.3324, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 532, LR: 0.4883, Train Loss: 2.0438, Train Accuracy: 36.20%, Temperatures:(0.00, 23.82)\n",
      "Old & New Losses 2037.6372337341309 2042.647123336792 Probab: tensor(0.8103, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 533, LR: 0.4883, Train Loss: 2.0389, Train Accuracy: 36.70%, Temperatures:(0.00, 23.58)\n",
      "Old & New Losses 2036.4091396331787 2036.3595485687256 Probab: tensor(1.0021, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 534, LR: 0.4883, Train Loss: 2.0403, Train Accuracy: 36.50%, Temperatures:(0.00, 23.34)\n",
      "Old & New Losses 2036.5166664123535 2045.5918312072754 Probab: tensor(0.6779, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 535, LR: 0.4883, Train Loss: 2.0412, Train Accuracy: 36.10%, Temperatures:(0.00, 23.11)\n",
      "Old & New Losses 2042.5467491149902 2042.4699783325195 Probab: tensor(1.0033, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 536, LR: 0.4883, Train Loss: 2.0430, Train Accuracy: 36.10%, Temperatures:(0.00, 22.88)\n",
      "Old & New Losses 2040.6992435455322 2061.927556991577 Probab: tensor(0.3954, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 537, LR: 0.4883, Train Loss: 2.0409, Train Accuracy: 36.40%, Temperatures:(0.00, 22.65)\n",
      "Old & New Losses 2062.912702560425 2063.8344287872314 Probab: tensor(0.9601, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 538, LR: 0.4883, Train Loss: 2.0657, Train Accuracy: 33.50%, Temperatures:(0.00, 22.42)\n",
      "Old & New Losses 2058.6166381835938 2056.6070079803467 Probab: tensor(1.0938, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 539, LR: 0.4883, Train Loss: 2.0615, Train Accuracy: 35.10%, Temperatures:(0.00, 22.20)\n",
      "Old & New Losses 2052.652597427368 2069.621801376343 Probab: tensor(0.4656, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 540, LR: 0.4883, Train Loss: 2.0625, Train Accuracy: 34.60%, Temperatures:(0.00, 21.98)\n",
      "Old & New Losses 2060.0688457489014 2059.9870681762695 Probab: tensor(1.0037, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 541, LR: 0.4883, Train Loss: 2.0669, Train Accuracy: 32.50%, Temperatures:(0.00, 21.76)\n",
      "Old & New Losses 2063.798427581787 2055.992841720581 Probab: tensor(1.4315, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 542, LR: 0.4883, Train Loss: 2.0677, Train Accuracy: 31.80%, Temperatures:(0.00, 21.54)\n",
      "Old & New Losses 2054.2354583740234 2053.6885261535645 Probab: tensor(1.0257, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 543, LR: 0.4883, Train Loss: 2.0522, Train Accuracy: 33.00%, Temperatures:(0.00, 21.32)\n",
      "Old & New Losses 2048.8014221191406 2053.6186695098877 Probab: tensor(0.7978, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 544, LR: 0.4883, Train Loss: 2.0540, Train Accuracy: 33.00%, Temperatures:(0.00, 21.11)\n",
      "Old & New Losses 2056.4634799957275 2044.950246810913 Probab: tensor(1.7252, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 545, LR: 0.4883, Train Loss: 2.0550, Train Accuracy: 34.00%, Temperatures:(0.00, 20.90)\n",
      "Old & New Losses 2047.1765995025635 2041.9087409973145 Probab: tensor(1.2867, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 546, LR: 0.4883, Train Loss: 2.0473, Train Accuracy: 35.10%, Temperatures:(0.00, 20.69)\n",
      "Old & New Losses 2045.6936359405518 2046.5829372406006 Probab: tensor(0.9579, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 547, LR: 0.4883, Train Loss: 2.0468, Train Accuracy: 34.20%, Temperatures:(0.00, 20.48)\n",
      "Old & New Losses 2050.2560138702393 2048.4843254089355 Probab: tensor(1.0903, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 548, LR: 0.4883, Train Loss: 2.0522, Train Accuracy: 34.80%, Temperatures:(0.00, 20.28)\n",
      "Old & New Losses 2047.680377960205 2058.584690093994 Probab: tensor(0.5841, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 549, LR: 0.4883, Train Loss: 2.0467, Train Accuracy: 32.60%, Temperatures:(0.00, 20.08)\n",
      "Old & New Losses 2060.1301193237305 2066.331148147583 Probab: tensor(0.7343, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 550, LR: 0.4883, Train Loss: 2.0626, Train Accuracy: 32.40%, Temperatures:(0.00, 19.88)\n",
      "Old & New Losses 2064.3913745880127 2068.913221359253 Probab: tensor(0.7965, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 551, LR: 0.4883, Train Loss: 2.0617, Train Accuracy: 32.30%, Temperatures:(0.00, 19.68)\n",
      "Old & New Losses 2062.5267028808594 2072.697639465332 Probab: tensor(0.5964, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 552, LR: 0.2441, Train Loss: 2.0698, Train Accuracy: 31.20%, Temperatures:(0.00, 19.48)\n",
      "Old & New Losses 2069.291591644287 2061.675786972046 Probab: tensor(1.4784, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 553, LR: 0.2441, Train Loss: 2.0650, Train Accuracy: 30.20%, Temperatures:(0.00, 19.29)\n",
      "Old & New Losses 2063.972234725952 2067.1451091766357 Probab: tensor(0.8483, device='cuda:0')\n",
      "Epoch 554, LR: 0.2441, Train Loss: 2.0631, Train Accuracy: 33.40%, Temperatures:(0.00, 19.09)\n",
      "Old & New Losses 2062.9780292510986 2066.6792392730713 Probab: tensor(0.8238, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 555, LR: 0.2441, Train Loss: 2.0692, Train Accuracy: 33.10%, Temperatures:(0.00, 18.90)\n",
      "Old & New Losses 2065.214157104492 2056.7190647125244 Probab: tensor(1.5674, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 556, LR: 0.2441, Train Loss: 2.0657, Train Accuracy: 31.90%, Temperatures:(0.00, 18.71)\n",
      "Old & New Losses 2061.137914657593 2065.0546550750732 Probab: tensor(0.8111, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 557, LR: 0.2441, Train Loss: 2.0653, Train Accuracy: 33.60%, Temperatures:(0.00, 18.53)\n",
      "Old & New Losses 2068.5317516326904 2061.910390853882 Probab: tensor(1.4296, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 558, LR: 0.2441, Train Loss: 2.0623, Train Accuracy: 33.70%, Temperatures:(0.00, 18.34)\n",
      "Old & New Losses 2068.4750080108643 2068.8085556030273 Probab: tensor(0.9820, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 559, LR: 0.2441, Train Loss: 2.0665, Train Accuracy: 31.80%, Temperatures:(0.00, 18.16)\n",
      "Old & New Losses 2067.3251152038574 2058.6888790130615 Probab: tensor(1.6090, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 560, LR: 0.2441, Train Loss: 2.0663, Train Accuracy: 33.30%, Temperatures:(0.00, 17.98)\n",
      "Old & New Losses 2059.8514080047607 2092.3914909362793 Probab: tensor(0.1636, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 561, LR: 0.2441, Train Loss: 2.0612, Train Accuracy: 33.20%, Temperatures:(0.00, 17.80)\n",
      "Old & New Losses 2087.1403217315674 2080.127239227295 Probab: tensor(1.4830, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 562, LR: 0.2441, Train Loss: 2.0868, Train Accuracy: 29.50%, Temperatures:(0.00, 17.62)\n",
      "Old & New Losses 2075.8016109466553 2074.4354724884033 Probab: tensor(1.0806, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 563, LR: 0.2441, Train Loss: 2.0812, Train Accuracy: 31.30%, Temperatures:(0.00, 17.44)\n",
      "Old & New Losses 2078.5534381866455 2091.9299125671387 Probab: tensor(0.4644, device='cuda:0')\n",
      "Epoch 564, LR: 0.2441, Train Loss: 2.0804, Train Accuracy: 32.10%, Temperatures:(0.00, 17.27)\n",
      "Old & New Losses 2072.9687213897705 2069.166660308838 Probab: tensor(1.2463, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 565, LR: 0.2441, Train Loss: 2.0732, Train Accuracy: 33.10%, Temperatures:(0.00, 17.09)\n",
      "Old & New Losses 2070.5621242523193 2066.3957595825195 Probab: tensor(1.2760, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 566, LR: 0.2441, Train Loss: 2.0765, Train Accuracy: 31.70%, Temperatures:(0.00, 16.92)\n",
      "Old & New Losses 2068.854570388794 2068.3279037475586 Probab: tensor(1.0316, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 567, LR: 0.2441, Train Loss: 2.0692, Train Accuracy: 32.40%, Temperatures:(0.00, 16.75)\n",
      "Old & New Losses 2064.7261142730713 2068.904161453247 Probab: tensor(0.7793, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 568, LR: 0.2441, Train Loss: 2.0667, Train Accuracy: 32.60%, Temperatures:(0.00, 16.59)\n",
      "Old & New Losses 2070.404529571533 2073.363780975342 Probab: tensor(0.8366, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 569, LR: 0.2441, Train Loss: 2.0645, Train Accuracy: 33.60%, Temperatures:(0.00, 16.42)\n",
      "Old & New Losses 2065.9148693084717 2071.1276531219482 Probab: tensor(0.7280, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 570, LR: 0.2441, Train Loss: 2.0687, Train Accuracy: 33.50%, Temperatures:(0.00, 16.26)\n",
      "Old & New Losses 2069.199800491333 2074.608087539673 Probab: tensor(0.7170, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 571, LR: 0.2441, Train Loss: 2.0679, Train Accuracy: 34.00%, Temperatures:(0.00, 16.09)\n",
      "Old & New Losses 2067.526340484619 2066.3554668426514 Probab: tensor(1.0755, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 572, LR: 0.2441, Train Loss: 2.0716, Train Accuracy: 32.50%, Temperatures:(0.00, 15.93)\n",
      "Old & New Losses 2070.553779602051 2071.8729496002197 Probab: tensor(0.9205, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 573, LR: 0.2441, Train Loss: 2.0736, Train Accuracy: 32.40%, Temperatures:(0.00, 15.77)\n",
      "Old & New Losses 2070.890188217163 2079.9899101257324 Probab: tensor(0.5616, device='cuda:0')\n",
      "Epoch 574, LR: 0.2441, Train Loss: 2.0771, Train Accuracy: 31.80%, Temperatures:(0.00, 15.62)\n",
      "Old & New Losses 2069.0548419952393 2075.873851776123 Probab: tensor(0.6462, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 575, LR: 0.2441, Train Loss: 2.0726, Train Accuracy: 32.90%, Temperatures:(0.00, 15.46)\n",
      "Old & New Losses 2072.30806350708 2073.026180267334 Probab: tensor(0.9546, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 576, LR: 0.2441, Train Loss: 2.0858, Train Accuracy: 31.10%, Temperatures:(0.00, 15.31)\n",
      "Old & New Losses 2075.222969055176 2079.510450363159 Probab: tensor(0.7557, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 577, LR: 0.2441, Train Loss: 2.0750, Train Accuracy: 33.90%, Temperatures:(0.00, 15.15)\n",
      "Old & New Losses 2070.971965789795 2080.0514221191406 Probab: tensor(0.5492, device='cuda:0')\n",
      "Epoch 578, LR: 0.2441, Train Loss: 2.0721, Train Accuracy: 34.10%, Temperatures:(0.00, 15.00)\n",
      "Old & New Losses 2077.2106647491455 2081.60138130188 Probab: tensor(0.7462, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 579, LR: 0.2441, Train Loss: 2.0753, Train Accuracy: 34.20%, Temperatures:(0.00, 14.85)\n",
      "Old & New Losses 2079.0023803710938 2078.169822692871 Probab: tensor(1.0577, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 580, LR: 0.2441, Train Loss: 2.0760, Train Accuracy: 32.80%, Temperatures:(0.00, 14.70)\n",
      "Old & New Losses 2075.084686279297 2073.180913925171 Probab: tensor(1.1382, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 581, LR: 0.2441, Train Loss: 2.0757, Train Accuracy: 33.50%, Temperatures:(0.00, 14.56)\n",
      "Old & New Losses 2075.727939605713 2070.842504501343 Probab: tensor(1.3988, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 582, LR: 0.2441, Train Loss: 2.0721, Train Accuracy: 33.90%, Temperatures:(0.00, 14.41)\n",
      "Old & New Losses 2075.782299041748 2117.8009510040283 Probab: tensor(0.0541, device='cuda:0')\n",
      "Epoch 583, LR: 0.2441, Train Loss: 2.0774, Train Accuracy: 34.60%, Temperatures:(0.00, 14.27)\n",
      "Old & New Losses 2067.8746700286865 2089.8358821868896 Probab: tensor(0.2145, device='cuda:0')\n",
      "Epoch 584, LR: 0.2441, Train Loss: 2.0720, Train Accuracy: 34.90%, Temperatures:(0.00, 14.12)\n",
      "Old & New Losses 2074.036121368408 2073.3745098114014 Probab: tensor(1.0480, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 585, LR: 0.2441, Train Loss: 2.0745, Train Accuracy: 34.70%, Temperatures:(0.00, 13.98)\n",
      "Old & New Losses 2071.885824203491 2070.4598426818848 Probab: tensor(1.1074, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 586, LR: 0.2441, Train Loss: 2.0706, Train Accuracy: 33.20%, Temperatures:(0.00, 13.84)\n",
      "Old & New Losses 2069.4565773010254 2072.7927684783936 Probab: tensor(0.7858, device='cuda:0')\n",
      "Epoch 587, LR: 0.2441, Train Loss: 2.0697, Train Accuracy: 33.90%, Temperatures:(0.00, 13.70)\n",
      "Old & New Losses 2075.1054286956787 2072.9422569274902 Probab: tensor(1.1710, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 588, LR: 0.2441, Train Loss: 2.0766, Train Accuracy: 32.70%, Temperatures:(0.00, 13.57)\n",
      "Old & New Losses 2069.6468353271484 2074.2709636688232 Probab: tensor(0.7112, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 589, LR: 0.2441, Train Loss: 2.0715, Train Accuracy: 34.20%, Temperatures:(0.00, 13.43)\n",
      "Old & New Losses 2073.615074157715 2076.2603282928467 Probab: tensor(0.8212, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 590, LR: 0.2441, Train Loss: 2.0772, Train Accuracy: 33.20%, Temperatures:(0.00, 13.30)\n",
      "Old & New Losses 2078.4807205200195 2075.9408473968506 Probab: tensor(1.2105, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 591, LR: 0.2441, Train Loss: 2.0826, Train Accuracy: 32.70%, Temperatures:(0.00, 13.16)\n",
      "Old & New Losses 2081.538677215576 2079.164743423462 Probab: tensor(1.1976, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 592, LR: 0.2441, Train Loss: 2.0815, Train Accuracy: 31.60%, Temperatures:(0.00, 13.03)\n",
      "Old & New Losses 2081.665515899658 2082.5419425964355 Probab: tensor(0.9350, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 593, LR: 0.2441, Train Loss: 2.0812, Train Accuracy: 32.80%, Temperatures:(0.00, 12.90)\n",
      "Old & New Losses 2078.491449356079 2083.0376148223877 Probab: tensor(0.7030, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 594, LR: 0.2441, Train Loss: 2.0824, Train Accuracy: 33.30%, Temperatures:(0.00, 12.77)\n",
      "Old & New Losses 2072.65043258667 2074.704885482788 Probab: tensor(0.8514, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 595, LR: 0.2441, Train Loss: 2.0814, Train Accuracy: 32.50%, Temperatures:(0.00, 12.64)\n",
      "Old & New Losses 2082.3814868927 2083.798408508301 Probab: tensor(0.8940, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 596, LR: 0.2441, Train Loss: 2.0825, Train Accuracy: 33.50%, Temperatures:(0.00, 12.52)\n",
      "Old & New Losses 2086.2927436828613 2083.81724357605 Probab: tensor(1.2187, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 597, LR: 0.2441, Train Loss: 2.0800, Train Accuracy: 33.60%, Temperatures:(0.00, 12.39)\n",
      "Old & New Losses 2087.2766971588135 2086.73095703125 Probab: tensor(1.0450, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 598, LR: 0.2441, Train Loss: 2.0847, Train Accuracy: 32.60%, Temperatures:(0.00, 12.27)\n",
      "Old & New Losses 2085.777759552002 2092.747688293457 Probab: tensor(0.5666, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 599, LR: 0.2441, Train Loss: 2.0902, Train Accuracy: 33.80%, Temperatures:(0.00, 12.15)\n",
      "Old & New Losses 2088.160753250122 2093.72878074646 Probab: tensor(0.6323, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 600, LR: 0.2441, Train Loss: 2.0856, Train Accuracy: 33.70%, Temperatures:(0.00, 12.03)\n",
      "Old & New Losses 2093.3361053466797 2123.694896697998 Probab: tensor(0.0801, device='cuda:0')\n",
      "Epoch 601, LR: 0.2441, Train Loss: 2.0936, Train Accuracy: 32.20%, Temperatures:(0.00, 11.90)\n",
      "Old & New Losses 2091.1338329315186 2094.0020084381104 Probab: tensor(0.7859, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 602, LR: 0.1221, Train Loss: 2.0873, Train Accuracy: 33.50%, Temperatures:(0.00, 11.79)\n",
      "Old & New Losses 2093.2834148406982 2094.3973064422607 Probab: tensor(0.9098, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 603, LR: 0.1221, Train Loss: 2.0928, Train Accuracy: 33.40%, Temperatures:(0.00, 11.67)\n",
      "Old & New Losses 2101.0403633117676 2087.855339050293 Probab: tensor(3.0957, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 604, LR: 0.1221, Train Loss: 2.0964, Train Accuracy: 32.10%, Temperatures:(0.00, 11.55)\n",
      "Old & New Losses 2087.3146057128906 2080.228328704834 Probab: tensor(1.8468, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 605, LR: 0.1221, Train Loss: 2.0859, Train Accuracy: 31.90%, Temperatures:(0.00, 11.44)\n",
      "Old & New Losses 2074.683427810669 2080.0745487213135 Probab: tensor(0.6241, device='cuda:0')\n",
      "Epoch 606, LR: 0.1221, Train Loss: 2.0779, Train Accuracy: 31.90%, Temperatures:(0.00, 11.32)\n",
      "Old & New Losses 2075.269937515259 2083.4248065948486 Probab: tensor(0.4866, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 607, LR: 0.1221, Train Loss: 2.0808, Train Accuracy: 32.40%, Temperatures:(0.00, 11.21)\n",
      "Old & New Losses 2079.413652420044 2082.123279571533 Probab: tensor(0.7852, device='cuda:0')\n",
      "Epoch 608, LR: 0.1221, Train Loss: 2.0780, Train Accuracy: 34.20%, Temperatures:(0.00, 11.10)\n",
      "Old & New Losses 2081.2013149261475 2078.984022140503 Probab: tensor(1.2212, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 609, LR: 0.1221, Train Loss: 2.0798, Train Accuracy: 34.00%, Temperatures:(0.00, 10.99)\n",
      "Old & New Losses 2082.1123123168945 2080.613851547241 Probab: tensor(1.1462, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 610, LR: 0.1221, Train Loss: 2.0813, Train Accuracy: 34.10%, Temperatures:(0.00, 10.88)\n",
      "Old & New Losses 2085.22367477417 2079.7879695892334 Probab: tensor(1.6484, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 611, LR: 0.1221, Train Loss: 2.0782, Train Accuracy: 32.80%, Temperatures:(0.00, 10.77)\n",
      "Old & New Losses 2081.5558433532715 2089.233875274658 Probab: tensor(0.4901, device='cuda:0')\n",
      "Epoch 612, LR: 0.1221, Train Loss: 2.0771, Train Accuracy: 32.50%, Temperatures:(0.00, 10.66)\n",
      "Old & New Losses 2077.815294265747 2078.2976150512695 Probab: tensor(0.9558, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 613, LR: 0.1221, Train Loss: 2.0803, Train Accuracy: 30.80%, Temperatures:(0.00, 10.55)\n",
      "Old & New Losses 2076.788902282715 2073.3273029327393 Probab: tensor(1.3883, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 614, LR: 0.1221, Train Loss: 2.0761, Train Accuracy: 31.60%, Temperatures:(0.00, 10.45)\n",
      "Old & New Losses 2078.5698890686035 2087.108850479126 Probab: tensor(0.4416, device='cuda:0')\n",
      "Epoch 615, LR: 0.1221, Train Loss: 2.0749, Train Accuracy: 34.70%, Temperatures:(0.00, 10.34)\n",
      "Old & New Losses 2075.0441551208496 2076.2996673583984 Probab: tensor(0.8857, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 616, LR: 0.1221, Train Loss: 2.0771, Train Accuracy: 34.80%, Temperatures:(0.00, 10.24)\n",
      "Old & New Losses 2078.890562057495 2077.092170715332 Probab: tensor(1.1920, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 617, LR: 0.1221, Train Loss: 2.0719, Train Accuracy: 32.70%, Temperatures:(0.00, 10.14)\n",
      "Old & New Losses 2072.9925632476807 2081.0790061950684 Probab: tensor(0.4503, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 618, LR: 0.1221, Train Loss: 2.0733, Train Accuracy: 33.10%, Temperatures:(0.00, 10.04)\n",
      "Old & New Losses 2081.5205574035645 2081.3467502593994 Probab: tensor(1.0175, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 619, LR: 0.1221, Train Loss: 2.0784, Train Accuracy: 32.10%, Temperatures:(0.00, 9.93)\n",
      "Old & New Losses 2079.9973011016846 2077.4595737457275 Probab: tensor(1.2910, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 620, LR: 0.1221, Train Loss: 2.0796, Train Accuracy: 29.30%, Temperatures:(0.00, 9.84)\n",
      "Old & New Losses 2075.7839679718018 2075.7129192352295 Probab: tensor(1.0072, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 621, LR: 0.1221, Train Loss: 2.0792, Train Accuracy: 32.50%, Temperatures:(0.00, 9.74)\n",
      "Old & New Losses 2081.5083980560303 2078.068494796753 Probab: tensor(1.4237, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 622, LR: 0.1221, Train Loss: 2.0801, Train Accuracy: 31.40%, Temperatures:(0.00, 9.64)\n",
      "Old & New Losses 2081.9132328033447 2123.176097869873 Probab: tensor(0.0138, device='cuda:0')\n",
      "Epoch 623, LR: 0.1221, Train Loss: 2.0748, Train Accuracy: 29.90%, Temperatures:(0.00, 9.54)\n",
      "Old & New Losses 2078.601837158203 2081.8097591400146 Probab: tensor(0.7145, device='cuda:0')\n",
      "Epoch 624, LR: 0.1221, Train Loss: 2.0780, Train Accuracy: 27.20%, Temperatures:(0.00, 9.45)\n",
      "Old & New Losses 2076.599597930908 2079.688310623169 Probab: tensor(0.7211, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 625, LR: 0.1221, Train Loss: 2.0836, Train Accuracy: 26.60%, Temperatures:(0.00, 9.35)\n",
      "Old & New Losses 2079.0510177612305 2087.733268737793 Probab: tensor(0.3952, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 626, LR: 0.1221, Train Loss: 2.0765, Train Accuracy: 28.00%, Temperatures:(0.00, 9.26)\n",
      "Old & New Losses 2090.216875076294 2088.5257720947266 Probab: tensor(1.2004, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 627, LR: 0.1221, Train Loss: 2.0930, Train Accuracy: 26.30%, Temperatures:(0.00, 9.17)\n",
      "Old & New Losses 2090.9364223480225 2088.0167484283447 Probab: tensor(1.3751, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 628, LR: 0.1221, Train Loss: 2.0884, Train Accuracy: 27.40%, Temperatures:(0.00, 9.08)\n",
      "Old & New Losses 2089.1921520233154 2091.8779373168945 Probab: tensor(0.7438, device='cuda:0')\n",
      "Epoch 629, LR: 0.1221, Train Loss: 2.0890, Train Accuracy: 26.40%, Temperatures:(0.00, 8.98)\n",
      "Old & New Losses 2094.1810607910156 2094.428062438965 Probab: tensor(0.9729, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 630, LR: 0.1221, Train Loss: 2.0920, Train Accuracy: 25.40%, Temperatures:(0.00, 8.89)\n",
      "Old & New Losses 2088.4718894958496 2093.3969020843506 Probab: tensor(0.5748, device='cuda:0')\n",
      "Epoch 631, LR: 0.1221, Train Loss: 2.0886, Train Accuracy: 27.60%, Temperatures:(0.00, 8.81)\n",
      "Old & New Losses 2091.447591781616 2093.461036682129 Probab: tensor(0.7956, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 632, LR: 0.1221, Train Loss: 2.0845, Train Accuracy: 25.90%, Temperatures:(0.00, 8.72)\n",
      "Old & New Losses 2091.3519859313965 2093.2812690734863 Probab: tensor(0.8015, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 633, LR: 0.1221, Train Loss: 2.0921, Train Accuracy: 27.40%, Temperatures:(0.00, 8.63)\n",
      "Old & New Losses 2092.3800468444824 2084.313154220581 Probab: tensor(2.5464, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 634, LR: 0.1221, Train Loss: 2.0926, Train Accuracy: 27.60%, Temperatures:(0.00, 8.54)\n",
      "Old & New Losses 2085.9782695770264 2095.7400798797607 Probab: tensor(0.3190, device='cuda:0')\n",
      "Epoch 635, LR: 0.1221, Train Loss: 2.0867, Train Accuracy: 28.90%, Temperatures:(0.00, 8.46)\n",
      "Old & New Losses 2089.3383026123047 2087.0039463043213 Probab: tensor(1.3178, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 636, LR: 0.1221, Train Loss: 2.0945, Train Accuracy: 27.10%, Temperatures:(0.00, 8.37)\n",
      "Old & New Losses 2088.19580078125 2090.9676551818848 Probab: tensor(0.7182, device='cuda:0')\n",
      "Epoch 637, LR: 0.1221, Train Loss: 2.0877, Train Accuracy: 29.60%, Temperatures:(0.00, 8.29)\n",
      "Old & New Losses 2080.9905529022217 2084.6846103668213 Probab: tensor(0.6405, device='cuda:0')\n",
      "Epoch 638, LR: 0.1221, Train Loss: 2.0828, Train Accuracy: 29.30%, Temperatures:(0.00, 8.21)\n",
      "Old & New Losses 2086.8685245513916 2084.702730178833 Probab: tensor(1.3020, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 639, LR: 0.1221, Train Loss: 2.0836, Train Accuracy: 29.70%, Temperatures:(0.00, 8.13)\n",
      "Old & New Losses 2089.6623134613037 2097.027063369751 Probab: tensor(0.4040, device='cuda:0')\n",
      "Epoch 640, LR: 0.1221, Train Loss: 2.0846, Train Accuracy: 28.40%, Temperatures:(0.00, 8.04)\n",
      "Old & New Losses 2092.057943344116 2082.8185081481934 Probab: tensor(3.1536, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 641, LR: 0.1221, Train Loss: 2.0865, Train Accuracy: 30.10%, Temperatures:(0.00, 7.96)\n",
      "Old & New Losses 2086.9526863098145 2088.5636806488037 Probab: tensor(0.8169, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 642, LR: 0.1221, Train Loss: 2.0859, Train Accuracy: 27.20%, Temperatures:(0.00, 7.88)\n",
      "Old & New Losses 2089.3337726593018 2083.90212059021 Probab: tensor(1.9916, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 643, LR: 0.1221, Train Loss: 2.0875, Train Accuracy: 29.00%, Temperatures:(0.00, 7.81)\n",
      "Old & New Losses 2078.016519546509 2083.0793380737305 Probab: tensor(0.5228, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 644, LR: 0.1221, Train Loss: 2.0825, Train Accuracy: 28.00%, Temperatures:(0.00, 7.73)\n",
      "Old & New Losses 2086.1313343048096 2084.2819213867188 Probab: tensor(1.2704, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 645, LR: 0.1221, Train Loss: 2.0868, Train Accuracy: 28.60%, Temperatures:(0.00, 7.65)\n",
      "Old & New Losses 2083.037853240967 2096.788167953491 Probab: tensor(0.1657, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 646, LR: 0.1221, Train Loss: 2.0797, Train Accuracy: 25.20%, Temperatures:(0.00, 7.57)\n",
      "Old & New Losses 2090.0070667266846 2088.712215423584 Probab: tensor(1.1865, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 647, LR: 0.1221, Train Loss: 2.1001, Train Accuracy: 25.10%, Temperatures:(0.00, 7.50)\n",
      "Old & New Losses 2093.1556224823 2098.5424518585205 Probab: tensor(0.4875, device='cuda:0')\n",
      "Epoch 648, LR: 0.1221, Train Loss: 2.0871, Train Accuracy: 26.10%, Temperatures:(0.00, 7.42)\n",
      "Old & New Losses 2089.0023708343506 2087.812900543213 Probab: tensor(1.1738, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 649, LR: 0.1221, Train Loss: 2.0863, Train Accuracy: 26.90%, Temperatures:(0.00, 7.35)\n",
      "Old & New Losses 2088.7324810028076 2084.759473800659 Probab: tensor(1.7171, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 650, LR: 0.1221, Train Loss: 2.0821, Train Accuracy: 28.00%, Temperatures:(0.00, 7.28)\n",
      "Old & New Losses 2080.9483528137207 2093.7507152557373 Probab: tensor(0.1721, device='cuda:0')\n",
      "Epoch 651, LR: 0.1221, Train Loss: 2.0874, Train Accuracy: 27.30%, Temperatures:(0.00, 7.20)\n",
      "Old & New Losses 2086.625337600708 2087.3351097106934 Probab: tensor(0.9062, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 652, LR: 0.0610, Train Loss: 2.0915, Train Accuracy: 27.40%, Temperatures:(0.00, 7.13)\n",
      "Old & New Losses 2088.564157485962 2089.9364948272705 Probab: tensor(0.8249, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 653, LR: 0.0610, Train Loss: 2.0914, Train Accuracy: 25.30%, Temperatures:(0.00, 7.06)\n",
      "Old & New Losses 2089.8663997650146 2085.994243621826 Probab: tensor(1.7307, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 654, LR: 0.0610, Train Loss: 2.0870, Train Accuracy: 25.70%, Temperatures:(0.00, 6.99)\n",
      "Old & New Losses 2083.8534832000732 2087.200880050659 Probab: tensor(0.6194, device='cuda:0')\n",
      "Epoch 655, LR: 0.0610, Train Loss: 2.0855, Train Accuracy: 24.00%, Temperatures:(0.00, 6.92)\n",
      "Old & New Losses 2091.1784172058105 2088.782548904419 Probab: tensor(1.4138, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 656, LR: 0.0610, Train Loss: 2.0915, Train Accuracy: 24.70%, Temperatures:(0.00, 6.85)\n",
      "Old & New Losses 2084.6879482269287 2091.9594764709473 Probab: tensor(0.3459, device='cuda:0')\n",
      "Epoch 657, LR: 0.0610, Train Loss: 2.0846, Train Accuracy: 24.30%, Temperatures:(0.00, 6.78)\n",
      "Old & New Losses 2087.780714035034 2084.522485733032 Probab: tensor(1.6169, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 658, LR: 0.0610, Train Loss: 2.0815, Train Accuracy: 26.00%, Temperatures:(0.00, 6.71)\n",
      "Old & New Losses 2086.0986709594727 2081.758737564087 Probab: tensor(1.9088, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 659, LR: 0.0610, Train Loss: 2.0889, Train Accuracy: 26.60%, Temperatures:(0.00, 6.65)\n",
      "Old & New Losses 2082.5672149658203 2088.9828205108643 Probab: tensor(0.3809, device='cuda:0')\n",
      "Epoch 660, LR: 0.0610, Train Loss: 2.0846, Train Accuracy: 26.00%, Temperatures:(0.00, 6.58)\n",
      "Old & New Losses 2083.083391189575 2080.9686183929443 Probab: tensor(1.3791, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 661, LR: 0.0610, Train Loss: 2.0806, Train Accuracy: 27.30%, Temperatures:(0.00, 6.51)\n",
      "Old & New Losses 2080.82914352417 2082.390308380127 Probab: tensor(0.7869, device='cuda:0')\n",
      "Epoch 662, LR: 0.0610, Train Loss: 2.0862, Train Accuracy: 24.40%, Temperatures:(0.00, 6.45)\n",
      "Old & New Losses 2085.679292678833 2093.4009552001953 Probab: tensor(0.3020, device='cuda:0')\n",
      "Epoch 663, LR: 0.0610, Train Loss: 2.0840, Train Accuracy: 27.60%, Temperatures:(0.00, 6.38)\n",
      "Old & New Losses 2086.219549179077 2082.885503768921 Probab: tensor(1.6858, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 664, LR: 0.0610, Train Loss: 2.0871, Train Accuracy: 25.40%, Temperatures:(0.00, 6.32)\n",
      "Old & New Losses 2087.3754024505615 2086.6894721984863 Probab: tensor(1.1146, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 665, LR: 0.0610, Train Loss: 2.0873, Train Accuracy: 25.40%, Temperatures:(0.00, 6.26)\n",
      "Old & New Losses 2091.4223194122314 2087.7552032470703 Probab: tensor(1.7969, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 666, LR: 0.0610, Train Loss: 2.0873, Train Accuracy: 26.40%, Temperatures:(0.00, 6.19)\n",
      "Old & New Losses 2088.5424613952637 2091.10426902771 Probab: tensor(0.6613, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 667, LR: 0.0610, Train Loss: 2.0955, Train Accuracy: 25.60%, Temperatures:(0.00, 6.13)\n",
      "Old & New Losses 2089.7867679595947 2093.2116508483887 Probab: tensor(0.5721, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 668, LR: 0.0610, Train Loss: 2.0906, Train Accuracy: 27.10%, Temperatures:(0.00, 6.07)\n",
      "Old & New Losses 2092.4453735351562 2092.606544494629 Probab: tensor(0.9738, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 669, LR: 0.0610, Train Loss: 2.0869, Train Accuracy: 27.20%, Temperatures:(0.00, 6.01)\n",
      "Old & New Losses 2091.7930603027344 2099.0254878997803 Probab: tensor(0.3002, device='cuda:0')\n",
      "Epoch 670, LR: 0.0610, Train Loss: 2.0942, Train Accuracy: 28.80%, Temperatures:(0.00, 5.95)\n",
      "Old & New Losses 2089.959144592285 2091.47047996521 Probab: tensor(0.7757, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 671, LR: 0.0610, Train Loss: 2.0925, Train Accuracy: 29.20%, Temperatures:(0.00, 5.89)\n",
      "Old & New Losses 2088.728666305542 2095.1614379882812 Probab: tensor(0.3356, device='cuda:0')\n",
      "Epoch 672, LR: 0.0610, Train Loss: 2.0904, Train Accuracy: 29.50%, Temperatures:(0.00, 5.83)\n",
      "Old & New Losses 2088.1683826446533 2090.7397270202637 Probab: tensor(0.6435, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 673, LR: 0.0610, Train Loss: 2.0872, Train Accuracy: 30.00%, Temperatures:(0.00, 5.77)\n",
      "Old & New Losses 2088.174343109131 2102.0667552948 Probab: tensor(0.0902, device='cuda:0')\n",
      "Epoch 674, LR: 0.0610, Train Loss: 2.0900, Train Accuracy: 28.50%, Temperatures:(0.00, 5.72)\n",
      "Old & New Losses 2091.231346130371 2098.7088680267334 Probab: tensor(0.2703, device='cuda:0')\n",
      "Epoch 675, LR: 0.0610, Train Loss: 2.0863, Train Accuracy: 29.50%, Temperatures:(0.00, 5.66)\n",
      "Old & New Losses 2084.688901901245 2095.8330631256104 Probab: tensor(0.1395, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 676, LR: 0.0610, Train Loss: 2.0905, Train Accuracy: 29.30%, Temperatures:(0.00, 5.60)\n",
      "Old & New Losses 2090.6002521514893 2100.788116455078 Probab: tensor(0.1623, device='cuda:0')\n",
      "Epoch 677, LR: 0.0610, Train Loss: 2.0940, Train Accuracy: 26.10%, Temperatures:(0.00, 5.55)\n",
      "Old & New Losses 2092.7045345306396 2091.031551361084 Probab: tensor(1.3521, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 678, LR: 0.0610, Train Loss: 2.0969, Train Accuracy: 27.40%, Temperatures:(0.00, 5.49)\n",
      "Old & New Losses 2091.665506362915 2092.348098754883 Probab: tensor(0.8831, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 679, LR: 0.0610, Train Loss: 2.0986, Train Accuracy: 24.50%, Temperatures:(0.00, 5.44)\n",
      "Old & New Losses 2094.266176223755 2102.818012237549 Probab: tensor(0.2074, device='cuda:0')\n",
      "Epoch 680, LR: 0.0610, Train Loss: 2.0956, Train Accuracy: 25.10%, Temperatures:(0.00, 5.38)\n",
      "Old & New Losses 2093.9700603485107 2095.1614379882812 Probab: tensor(0.8014, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 681, LR: 0.0610, Train Loss: 2.0967, Train Accuracy: 24.40%, Temperatures:(0.00, 5.33)\n",
      "Old & New Losses 2098.1712341308594 2094.9106216430664 Probab: tensor(1.8441, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 682, LR: 0.0610, Train Loss: 2.0929, Train Accuracy: 26.60%, Temperatures:(0.00, 5.27)\n",
      "Old & New Losses 2096.463680267334 2106.456756591797 Probab: tensor(0.1504, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 683, LR: 0.0610, Train Loss: 2.0965, Train Accuracy: 26.50%, Temperatures:(0.00, 5.22)\n",
      "Old & New Losses 2105.2920818328857 2110.3835105895996 Probab: tensor(0.3772, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 684, LR: 0.0610, Train Loss: 2.1087, Train Accuracy: 24.80%, Temperatures:(0.00, 5.17)\n",
      "Old & New Losses 2113.938808441162 2106.320858001709 Probab: tensor(4.3651, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 685, LR: 0.0610, Train Loss: 2.1110, Train Accuracy: 23.70%, Temperatures:(0.00, 5.12)\n",
      "Old & New Losses 2110.1434230804443 2118.9873218536377 Probab: tensor(0.1776, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 686, LR: 0.0610, Train Loss: 2.1107, Train Accuracy: 24.80%, Temperatures:(0.00, 5.07)\n",
      "Old & New Losses 2117.694854736328 2114.8951053619385 Probab: tensor(1.7378, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 687, LR: 0.0610, Train Loss: 2.1227, Train Accuracy: 22.30%, Temperatures:(0.00, 5.02)\n",
      "Old & New Losses 2119.469165802002 2123.3956813812256 Probab: tensor(0.4571, device='cuda:0')\n",
      "Epoch 688, LR: 0.0610, Train Loss: 2.1162, Train Accuracy: 23.80%, Temperatures:(0.00, 4.97)\n",
      "Old & New Losses 2116.971492767334 2113.13796043396 Probab: tensor(2.1641, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 689, LR: 0.0610, Train Loss: 2.1198, Train Accuracy: 23.00%, Temperatures:(0.00, 4.92)\n",
      "Old & New Losses 2114.5596504211426 2114.243507385254 Probab: tensor(1.0664, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 690, LR: 0.0610, Train Loss: 2.1104, Train Accuracy: 25.30%, Temperatures:(0.00, 4.87)\n",
      "Old & New Losses 2112.9918098449707 2117.8367137908936 Probab: tensor(0.3695, device='cuda:0')\n",
      "Epoch 691, LR: 0.0610, Train Loss: 2.1112, Train Accuracy: 25.90%, Temperatures:(0.00, 4.82)\n",
      "Old & New Losses 2107.773780822754 2108.004570007324 Probab: tensor(0.9532, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 692, LR: 0.0610, Train Loss: 2.1124, Train Accuracy: 24.10%, Temperatures:(0.00, 4.77)\n",
      "Old & New Losses 2113.828420639038 2116.164207458496 Probab: tensor(0.6128, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 693, LR: 0.0610, Train Loss: 2.1100, Train Accuracy: 24.00%, Temperatures:(0.00, 4.72)\n",
      "Old & New Losses 2122.3766803741455 2120.746612548828 Probab: tensor(1.4122, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 694, LR: 0.0610, Train Loss: 2.1221, Train Accuracy: 22.20%, Temperatures:(0.00, 4.68)\n",
      "Old & New Losses 2120.44358253479 2116.6226863861084 Probab: tensor(2.2643, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 695, LR: 0.0610, Train Loss: 2.1247, Train Accuracy: 23.50%, Temperatures:(0.00, 4.63)\n",
      "Old & New Losses 2124.1328716278076 2124.591827392578 Probab: tensor(0.9056, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 696, LR: 0.0610, Train Loss: 2.1212, Train Accuracy: 23.80%, Temperatures:(0.00, 4.58)\n",
      "Old & New Losses 2121.1209297180176 2132.22336769104 Probab: tensor(0.0887, device='cuda:0')\n",
      "Epoch 697, LR: 0.0610, Train Loss: 2.1231, Train Accuracy: 25.30%, Temperatures:(0.00, 4.54)\n",
      "Old & New Losses 2120.40376663208 2127.586603164673 Probab: tensor(0.2053, device='cuda:0')\n",
      "Epoch 698, LR: 0.0610, Train Loss: 2.1169, Train Accuracy: 23.70%, Temperatures:(0.00, 4.49)\n",
      "Old & New Losses 2118.9680099487305 2118.2479858398438 Probab: tensor(1.1739, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 699, LR: 0.0610, Train Loss: 2.1237, Train Accuracy: 22.80%, Temperatures:(0.00, 4.45)\n",
      "Old & New Losses 2120.4240322113037 2126.936197280884 Probab: tensor(0.2311, device='cuda:0')\n",
      "Epoch 700, LR: 0.0610, Train Loss: 2.1233, Train Accuracy: 23.70%, Temperatures:(0.00, 4.40)\n",
      "Old & New Losses 2121.8159198760986 2127.718925476074 Probab: tensor(0.2616, device='cuda:0')\n",
      "Epoch 701, LR: 0.0610, Train Loss: 2.1220, Train Accuracy: 23.40%, Temperatures:(0.00, 4.36)\n",
      "Old & New Losses 2115.927219390869 2118.5507774353027 Probab: tensor(0.5477, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 702, LR: 0.0305, Train Loss: 2.1234, Train Accuracy: 23.30%, Temperatures:(0.00, 4.31)\n",
      "Old & New Losses 2117.645740509033 2132.26580619812 Probab: tensor(0.0337, device='cuda:0')\n",
      "Epoch 703, LR: 0.0305, Train Loss: 2.1161, Train Accuracy: 23.80%, Temperatures:(0.00, 4.27)\n",
      "Old & New Losses 2122.143030166626 2115.159749984741 Probab: tensor(5.1300, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 704, LR: 0.0305, Train Loss: 2.1213, Train Accuracy: 24.00%, Temperatures:(0.00, 4.23)\n",
      "Old & New Losses 2118.224620819092 2113.607883453369 Probab: tensor(2.9800, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 705, LR: 0.0305, Train Loss: 2.1153, Train Accuracy: 22.40%, Temperatures:(0.00, 4.19)\n",
      "Old & New Losses 2116.9779300689697 2113.4212017059326 Probab: tensor(2.3390, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 706, LR: 0.0305, Train Loss: 2.1158, Train Accuracy: 21.80%, Temperatures:(0.00, 4.14)\n",
      "Old & New Losses 2113.274574279785 2114.246368408203 Probab: tensor(0.7910, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 707, LR: 0.0305, Train Loss: 2.1134, Train Accuracy: 26.60%, Temperatures:(0.00, 4.10)\n",
      "Old & New Losses 2109.4906330108643 2112.4472618103027 Probab: tensor(0.4864, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 708, LR: 0.0305, Train Loss: 2.1029, Train Accuracy: 26.10%, Temperatures:(0.00, 4.06)\n",
      "Old & New Losses 2112.1959686279297 2130.059242248535 Probab: tensor(0.0123, device='cuda:0')\n",
      "Epoch 709, LR: 0.0305, Train Loss: 2.1123, Train Accuracy: 25.90%, Temperatures:(0.00, 4.02)\n",
      "Old & New Losses 2111.4249229431152 2115.3564453125 Probab: tensor(0.3761, device='cuda:0')\n",
      "Epoch 710, LR: 0.0305, Train Loss: 2.1141, Train Accuracy: 24.40%, Temperatures:(0.00, 3.98)\n",
      "Old & New Losses 2110.130786895752 2114.940881729126 Probab: tensor(0.2987, device='cuda:0')\n",
      "Epoch 711, LR: 0.0305, Train Loss: 2.1148, Train Accuracy: 26.00%, Temperatures:(0.00, 3.94)\n",
      "Old & New Losses 2116.6210174560547 2115.2963638305664 Probab: tensor(1.3995, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 712, LR: 0.0305, Train Loss: 2.1097, Train Accuracy: 26.50%, Temperatures:(0.00, 3.90)\n",
      "Old & New Losses 2112.7748489379883 2116.9095039367676 Probab: tensor(0.3465, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 713, LR: 0.0305, Train Loss: 2.1234, Train Accuracy: 22.90%, Temperatures:(0.00, 3.86)\n",
      "Old & New Losses 2117.0263290405273 2119.0245151519775 Probab: tensor(0.5961, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 714, LR: 0.0305, Train Loss: 2.1142, Train Accuracy: 25.50%, Temperatures:(0.00, 3.82)\n",
      "Old & New Losses 2119.343042373657 2126.2457370758057 Probab: tensor(0.1644, device='cuda:0')\n",
      "Epoch 715, LR: 0.0305, Train Loss: 2.1194, Train Accuracy: 21.80%, Temperatures:(0.00, 3.79)\n",
      "Old & New Losses 2118.541717529297 2136.10577583313 Probab: tensor(0.0097, device='cuda:0')\n",
      "Epoch 716, LR: 0.0305, Train Loss: 2.1177, Train Accuracy: 22.10%, Temperatures:(0.00, 3.75)\n",
      "Old & New Losses 2121.5429306030273 2121.1957931518555 Probab: tensor(1.0971, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 717, LR: 0.0305, Train Loss: 2.1198, Train Accuracy: 21.30%, Temperatures:(0.00, 3.71)\n",
      "Old & New Losses 2119.7738647460938 2123.643159866333 Probab: tensor(0.3524, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 718, LR: 0.0305, Train Loss: 2.1232, Train Accuracy: 20.70%, Temperatures:(0.00, 3.67)\n",
      "Old & New Losses 2118.5855865478516 2121.0196018218994 Probab: tensor(0.5155, device='cuda:0')\n",
      "Epoch 719, LR: 0.0305, Train Loss: 2.1216, Train Accuracy: 23.00%, Temperatures:(0.00, 3.64)\n",
      "Old & New Losses 2120.889186859131 2127.014636993408 Probab: tensor(0.1855, device='cuda:0')\n",
      "Epoch 720, LR: 0.0305, Train Loss: 2.1202, Train Accuracy: 21.80%, Temperatures:(0.00, 3.60)\n",
      "Old & New Losses 2122.2593784332275 2122.703790664673 Probab: tensor(0.8839, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 721, LR: 0.0305, Train Loss: 2.1249, Train Accuracy: 23.40%, Temperatures:(0.00, 3.56)\n",
      "Old & New Losses 2121.84739112854 2121.845483779907 Probab: tensor(1.0005, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 722, LR: 0.0305, Train Loss: 2.1225, Train Accuracy: 22.20%, Temperatures:(0.00, 3.53)\n",
      "Old & New Losses 2123.9490509033203 2123.746633529663 Probab: tensor(1.0590, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 723, LR: 0.0305, Train Loss: 2.1165, Train Accuracy: 22.00%, Temperatures:(0.00, 3.49)\n",
      "Old & New Losses 2122.819423675537 2118.030071258545 Probab: tensor(3.9396, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 724, LR: 0.0305, Train Loss: 2.1222, Train Accuracy: 21.40%, Temperatures:(0.00, 3.46)\n",
      "Old & New Losses 2119.764566421509 2131.7875385284424 Probab: tensor(0.0309, device='cuda:0')\n",
      "Epoch 725, LR: 0.0305, Train Loss: 2.1217, Train Accuracy: 23.70%, Temperatures:(0.00, 3.42)\n",
      "Old & New Losses 2118.4048652648926 2129.7848224639893 Probab: tensor(0.0360, device='cuda:0')\n",
      "Epoch 726, LR: 0.0305, Train Loss: 2.1241, Train Accuracy: 23.60%, Temperatures:(0.00, 3.39)\n",
      "Old & New Losses 2122.917890548706 2116.1983013153076 Probab: tensor(7.2611, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 727, LR: 0.0305, Train Loss: 2.1222, Train Accuracy: 25.50%, Temperatures:(0.00, 3.36)\n",
      "Old & New Losses 2108.7143421173096 2116.9941425323486 Probab: tensor(0.0848, device='cuda:0')\n",
      "Epoch 728, LR: 0.0305, Train Loss: 2.1132, Train Accuracy: 23.10%, Temperatures:(0.00, 3.32)\n",
      "Old & New Losses 2109.2331409454346 2112.7822399139404 Probab: tensor(0.3436, device='cuda:0')\n",
      "Epoch 729, LR: 0.0305, Train Loss: 2.1092, Train Accuracy: 24.70%, Temperatures:(0.00, 3.29)\n",
      "Old & New Losses 2113.647937774658 2113.7166023254395 Probab: tensor(0.9793, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 730, LR: 0.0305, Train Loss: 2.1173, Train Accuracy: 24.60%, Temperatures:(0.00, 3.26)\n",
      "Old & New Losses 2112.489938735962 2117.5718307495117 Probab: tensor(0.2100, device='cuda:0')\n",
      "Epoch 731, LR: 0.0305, Train Loss: 2.1083, Train Accuracy: 25.00%, Temperatures:(0.00, 3.22)\n",
      "Old & New Losses 2111.89341545105 2132.277488708496 Probab: tensor(0.0018, device='cuda:0')\n",
      "Epoch 732, LR: 0.0305, Train Loss: 2.1107, Train Accuracy: 25.60%, Temperatures:(0.00, 3.19)\n",
      "Old & New Losses 2111.4208698272705 2111.1083030700684 Probab: tensor(1.1029, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 733, LR: 0.0305, Train Loss: 2.1135, Train Accuracy: 24.70%, Temperatures:(0.00, 3.16)\n",
      "Old & New Losses 2114.1715049743652 2112.440586090088 Probab: tensor(1.7296, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 734, LR: 0.0305, Train Loss: 2.1112, Train Accuracy: 23.90%, Temperatures:(0.00, 3.13)\n",
      "Old & New Losses 2118.1163787841797 2113.807439804077 Probab: tensor(3.9659, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 735, LR: 0.0305, Train Loss: 2.1132, Train Accuracy: 22.90%, Temperatures:(0.00, 3.10)\n",
      "Old & New Losses 2114.327907562256 2124.2189407348633 Probab: tensor(0.0410, device='cuda:0')\n",
      "Epoch 736, LR: 0.0305, Train Loss: 2.1160, Train Accuracy: 21.90%, Temperatures:(0.00, 3.07)\n",
      "Old & New Losses 2116.4956092834473 2122.776508331299 Probab: tensor(0.1289, device='cuda:0')\n",
      "Epoch 737, LR: 0.0305, Train Loss: 2.1162, Train Accuracy: 22.70%, Temperatures:(0.00, 3.03)\n",
      "Old & New Losses 2116.746187210083 2128.8461685180664 Probab: tensor(0.0186, device='cuda:0')\n",
      "Epoch 738, LR: 0.0305, Train Loss: 2.1160, Train Accuracy: 21.90%, Temperatures:(0.00, 3.00)\n",
      "Old & New Losses 2116.1231994628906 2117.8553104400635 Probab: tensor(0.5618, device='cuda:0')\n",
      "Epoch 739, LR: 0.0305, Train Loss: 2.1090, Train Accuracy: 22.90%, Temperatures:(0.00, 2.97)\n",
      "Old & New Losses 2115.5974864959717 2118.964433670044 Probab: tensor(0.3224, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 740, LR: 0.0305, Train Loss: 2.1144, Train Accuracy: 21.70%, Temperatures:(0.00, 2.94)\n",
      "Old & New Losses 2120.758295059204 2122.8723526000977 Probab: tensor(0.4877, device='cuda:0')\n",
      "Epoch 741, LR: 0.0305, Train Loss: 2.1158, Train Accuracy: 22.30%, Temperatures:(0.00, 2.92)\n",
      "Old & New Losses 2117.7780628204346 2118.986129760742 Probab: tensor(0.6607, device='cuda:0')\n",
      "Epoch 742, LR: 0.0305, Train Loss: 2.1234, Train Accuracy: 21.50%, Temperatures:(0.00, 2.89)\n",
      "Old & New Losses 2117.736577987671 2124.187231063843 Probab: tensor(0.1070, device='cuda:0')\n",
      "Epoch 743, LR: 0.0305, Train Loss: 2.1211, Train Accuracy: 22.40%, Temperatures:(0.00, 2.86)\n",
      "Old & New Losses 2126.1372566223145 2125.7810592651367 Probab: tensor(1.1328, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 744, LR: 0.0305, Train Loss: 2.1231, Train Accuracy: 21.40%, Temperatures:(0.00, 2.83)\n",
      "Old & New Losses 2127.6791095733643 2127.498149871826 Probab: tensor(1.0661, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 745, LR: 0.0305, Train Loss: 2.1295, Train Accuracy: 21.60%, Temperatures:(0.00, 2.80)\n",
      "Old & New Losses 2126.6658306121826 2129.7826766967773 Probab: tensor(0.3285, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 746, LR: 0.0305, Train Loss: 2.1243, Train Accuracy: 20.70%, Temperatures:(0.00, 2.77)\n",
      "Old & New Losses 2126.0812282562256 2132.1017742156982 Probab: tensor(0.1140, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 747, LR: 0.0305, Train Loss: 2.1300, Train Accuracy: 18.80%, Temperatures:(0.00, 2.74)\n",
      "Old & New Losses 2132.1396827697754 2140.09690284729 Probab: tensor(0.0551, device='cuda:0')\n",
      "Epoch 748, LR: 0.0305, Train Loss: 2.1365, Train Accuracy: 19.30%, Temperatures:(0.00, 2.72)\n",
      "Old & New Losses 2128.2589435577393 2141.2858963012695 Probab: tensor(0.0083, device='cuda:0')\n",
      "Epoch 749, LR: 0.0305, Train Loss: 2.1348, Train Accuracy: 18.80%, Temperatures:(0.00, 2.69)\n",
      "Old & New Losses 2133.281707763672 2138.7109756469727 Probab: tensor(0.1329, device='cuda:0')\n",
      "Epoch 750, LR: 0.0305, Train Loss: 2.1329, Train Accuracy: 18.90%, Temperatures:(0.00, 2.66)\n",
      "Old & New Losses 2134.6216201782227 2143.348217010498 Probab: tensor(0.0377, device='cuda:0')\n",
      "Epoch 751, LR: 0.0305, Train Loss: 2.1328, Train Accuracy: 18.40%, Temperatures:(0.00, 2.64)\n",
      "Old & New Losses 2132.5554847717285 2127.030372619629 Probab: tensor(8.1316, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 752, LR: 0.0153, Train Loss: 2.1343, Train Accuracy: 18.90%, Temperatures:(0.00, 2.61)\n",
      "Old & New Losses 2131.021499633789 2132.8952312469482 Probab: tensor(0.4878, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 753, LR: 0.0153, Train Loss: 2.1335, Train Accuracy: 19.70%, Temperatures:(0.00, 2.58)\n",
      "Old & New Losses 2127.603530883789 2124.5265007019043 Probab: tensor(3.2899, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 754, LR: 0.0153, Train Loss: 2.1323, Train Accuracy: 19.00%, Temperatures:(0.00, 2.56)\n",
      "Old & New Losses 2129.2452812194824 2125.922441482544 Probab: tensor(3.6656, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 755, LR: 0.0153, Train Loss: 2.1251, Train Accuracy: 19.90%, Temperatures:(0.00, 2.53)\n",
      "Old & New Losses 2129.282236099243 2126.354694366455 Probab: tensor(3.1772, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 756, LR: 0.0153, Train Loss: 2.1273, Train Accuracy: 21.90%, Temperatures:(0.00, 2.51)\n",
      "Old & New Losses 2130.321979522705 2123.272657394409 Probab: tensor(16.6383, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 757, LR: 0.0153, Train Loss: 2.1260, Train Accuracy: 22.00%, Temperatures:(0.00, 2.48)\n",
      "Old & New Losses 2128.9258003234863 2131.6733360290527 Probab: tensor(0.3306, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 758, LR: 0.0153, Train Loss: 2.1288, Train Accuracy: 21.40%, Temperatures:(0.00, 2.46)\n",
      "Old & New Losses 2133.2123279571533 2148.13494682312 Probab: tensor(0.0023, device='cuda:0')\n",
      "Epoch 759, LR: 0.0153, Train Loss: 2.1242, Train Accuracy: 21.90%, Temperatures:(0.00, 2.43)\n",
      "Old & New Losses 2126.650333404541 2129.5294761657715 Probab: tensor(0.3062, device='cuda:0')\n",
      "Epoch 760, LR: 0.0153, Train Loss: 2.1317, Train Accuracy: 21.30%, Temperatures:(0.00, 2.41)\n",
      "Old & New Losses 2129.3554306030273 2129.1916370391846 Probab: tensor(1.0704, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 761, LR: 0.0153, Train Loss: 2.1301, Train Accuracy: 21.00%, Temperatures:(0.00, 2.38)\n",
      "Old & New Losses 2131.6003799438477 2135.2579593658447 Probab: tensor(0.2157, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 762, LR: 0.0153, Train Loss: 2.1334, Train Accuracy: 21.40%, Temperatures:(0.00, 2.36)\n",
      "Old & New Losses 2134.063959121704 2138.9341354370117 Probab: tensor(0.1270, device='cuda:0')\n",
      "Epoch 763, LR: 0.0153, Train Loss: 2.1348, Train Accuracy: 19.10%, Temperatures:(0.00, 2.34)\n",
      "Old & New Losses 2136.3894939422607 2152.494192123413 Probab: tensor(0.0010, device='cuda:0')\n",
      "Epoch 764, LR: 0.0153, Train Loss: 2.1377, Train Accuracy: 19.40%, Temperatures:(0.00, 2.31)\n",
      "Old & New Losses 2133.1560611724854 2147.883653640747 Probab: tensor(0.0017, device='cuda:0')\n",
      "Epoch 765, LR: 0.0153, Train Loss: 2.1297, Train Accuracy: 20.50%, Temperatures:(0.00, 2.29)\n",
      "Old & New Losses 2133.345127105713 2134.528160095215 Probab: tensor(0.5966, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 766, LR: 0.0153, Train Loss: 2.1357, Train Accuracy: 20.60%, Temperatures:(0.00, 2.27)\n",
      "Old & New Losses 2136.3441944122314 2135.3909969329834 Probab: tensor(1.5226, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 767, LR: 0.0153, Train Loss: 2.1351, Train Accuracy: 20.90%, Temperatures:(0.00, 2.24)\n",
      "Old & New Losses 2133.8281631469727 2141.6406631469727 Probab: tensor(0.0308, device='cuda:0')\n",
      "Epoch 768, LR: 0.0153, Train Loss: 2.1362, Train Accuracy: 20.30%, Temperatures:(0.00, 2.22)\n",
      "Old & New Losses 2137.190341949463 2136.0738277435303 Probab: tensor(1.6527, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 769, LR: 0.0153, Train Loss: 2.1344, Train Accuracy: 20.60%, Temperatures:(0.00, 2.20)\n",
      "Old & New Losses 2135.8885765075684 2140.8119201660156 Probab: tensor(0.1067, device='cuda:0')\n",
      "Epoch 770, LR: 0.0153, Train Loss: 2.1329, Train Accuracy: 19.90%, Temperatures:(0.00, 2.18)\n",
      "Old & New Losses 2135.2031230926514 2138.296127319336 Probab: tensor(0.2417, device='cuda:0')\n",
      "Epoch 771, LR: 0.0153, Train Loss: 2.1372, Train Accuracy: 20.80%, Temperatures:(0.00, 2.16)\n",
      "Old & New Losses 2140.688896179199 2135.7789039611816 Probab: tensor(9.7480, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 772, LR: 0.0153, Train Loss: 2.1364, Train Accuracy: 21.10%, Temperatures:(0.00, 2.13)\n",
      "Old & New Losses 2136.9550228118896 2137.3233795166016 Probab: tensor(0.8415, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 773, LR: 0.0153, Train Loss: 2.1315, Train Accuracy: 22.90%, Temperatures:(0.00, 2.11)\n",
      "Old & New Losses 2135.2767944335938 2134.5582008361816 Probab: tensor(1.4050, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 774, LR: 0.0153, Train Loss: 2.1379, Train Accuracy: 21.50%, Temperatures:(0.00, 2.09)\n",
      "Old & New Losses 2140.7954692840576 2135.7479095458984 Probab: tensor(11.1620, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 775, LR: 0.0153, Train Loss: 2.1397, Train Accuracy: 20.00%, Temperatures:(0.00, 2.07)\n",
      "Old & New Losses 2139.73331451416 2135.202407836914 Probab: tensor(8.9125, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 776, LR: 0.0153, Train Loss: 2.1333, Train Accuracy: 20.70%, Temperatures:(0.00, 2.05)\n",
      "Old & New Losses 2129.8699378967285 2135.424852371216 Probab: tensor(0.0666, device='cuda:0')\n",
      "Epoch 777, LR: 0.0153, Train Loss: 2.1371, Train Accuracy: 23.80%, Temperatures:(0.00, 2.03)\n",
      "Old & New Losses 2136.0762119293213 2142.512798309326 Probab: tensor(0.0420, device='cuda:0')\n",
      "Epoch 778, LR: 0.0153, Train Loss: 2.1328, Train Accuracy: 23.10%, Temperatures:(0.00, 2.01)\n",
      "Old & New Losses 2136.7573738098145 2140.7430171966553 Probab: tensor(0.1376, device='cuda:0')\n",
      "Epoch 779, LR: 0.0153, Train Loss: 2.1370, Train Accuracy: 24.40%, Temperatures:(0.00, 1.99)\n",
      "Old & New Losses 2137.0973587036133 2146.0232734680176 Probab: tensor(0.0113, device='cuda:0')\n",
      "Epoch 780, LR: 0.0153, Train Loss: 2.1400, Train Accuracy: 23.30%, Temperatures:(0.00, 1.97)\n",
      "Old & New Losses 2140.153408050537 2139.4810676574707 Probab: tensor(1.4068, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 781, LR: 0.0153, Train Loss: 2.1349, Train Accuracy: 24.00%, Temperatures:(0.00, 1.95)\n",
      "Old & New Losses 2130.5322647094727 2138.411283493042 Probab: tensor(0.0176, device='cuda:0')\n",
      "Epoch 782, LR: 0.0153, Train Loss: 2.1349, Train Accuracy: 23.90%, Temperatures:(0.00, 1.93)\n",
      "Old & New Losses 2140.9530639648438 2140.564203262329 Probab: tensor(1.2231, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 783, LR: 0.0153, Train Loss: 2.1353, Train Accuracy: 25.00%, Temperatures:(0.00, 1.91)\n",
      "Old & New Losses 2131.12211227417 2153.197765350342 Probab: tensor(9.6350e-06, device='cuda:0')\n",
      "Epoch 784, LR: 0.0153, Train Loss: 2.1377, Train Accuracy: 26.90%, Temperatures:(0.00, 1.89)\n",
      "Old & New Losses 2140.352487564087 2150.0518321990967 Probab: tensor(0.0059, device='cuda:0')\n",
      "Epoch 785, LR: 0.0153, Train Loss: 2.1336, Train Accuracy: 24.60%, Temperatures:(0.00, 1.87)\n",
      "Old & New Losses 2140.991687774658 2138.2148265838623 Probab: tensor(4.4034, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 786, LR: 0.0153, Train Loss: 2.1360, Train Accuracy: 26.40%, Temperatures:(0.00, 1.85)\n",
      "Old & New Losses 2138.7336254119873 2141.2718296051025 Probab: tensor(0.2544, device='cuda:0')\n",
      "Epoch 787, LR: 0.0153, Train Loss: 2.1388, Train Accuracy: 25.80%, Temperatures:(0.00, 1.84)\n",
      "Old & New Losses 2140.7649517059326 2141.676902770996 Probab: tensor(0.6085, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 788, LR: 0.0153, Train Loss: 2.1394, Train Accuracy: 25.30%, Temperatures:(0.00, 1.82)\n",
      "Old & New Losses 2143.162250518799 2141.6454315185547 Probab: tensor(2.3037, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 789, LR: 0.0153, Train Loss: 2.1422, Train Accuracy: 25.20%, Temperatures:(0.00, 1.80)\n",
      "Old & New Losses 2146.576404571533 2145.221948623657 Probab: tensor(2.1227, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 790, LR: 0.0153, Train Loss: 2.1418, Train Accuracy: 26.10%, Temperatures:(0.00, 1.78)\n",
      "Old & New Losses 2140.3210163116455 2146.305561065674 Probab: tensor(0.0348, device='cuda:0')\n",
      "Epoch 791, LR: 0.0153, Train Loss: 2.1450, Train Accuracy: 24.20%, Temperatures:(0.00, 1.76)\n",
      "Old & New Losses 2147.733211517334 2154.017448425293 Probab: tensor(0.0283, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 792, LR: 0.0153, Train Loss: 2.1480, Train Accuracy: 24.90%, Temperatures:(0.00, 1.75)\n",
      "Old & New Losses 2152.977705001831 2157.2155952453613 Probab: tensor(0.0883, device='cuda:0')\n",
      "Epoch 793, LR: 0.0153, Train Loss: 2.1548, Train Accuracy: 23.30%, Temperatures:(0.00, 1.73)\n",
      "Old & New Losses 2153.829336166382 2158.2515239715576 Probab: tensor(0.0774, device='cuda:0')\n",
      "Epoch 794, LR: 0.0153, Train Loss: 2.1523, Train Accuracy: 23.90%, Temperatures:(0.00, 1.71)\n",
      "Old & New Losses 2153.0418395996094 2151.977777481079 Probab: tensor(1.8623, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 795, LR: 0.0153, Train Loss: 2.1578, Train Accuracy: 22.10%, Temperatures:(0.00, 1.69)\n",
      "Old & New Losses 2151.071548461914 2157.490015029907 Probab: tensor(0.0226, device='cuda:0')\n",
      "Epoch 796, LR: 0.0153, Train Loss: 2.1557, Train Accuracy: 22.70%, Temperatures:(0.00, 1.68)\n",
      "Old & New Losses 2154.9227237701416 2165.846347808838 Probab: tensor(0.0015, device='cuda:0')\n",
      "Epoch 797, LR: 0.0153, Train Loss: 2.1584, Train Accuracy: 22.80%, Temperatures:(0.00, 1.66)\n",
      "Old & New Losses 2155.1811695098877 2156.731605529785 Probab: tensor(0.3931, device='cuda:0')\n",
      "Epoch 798, LR: 0.0153, Train Loss: 2.1599, Train Accuracy: 21.20%, Temperatures:(0.00, 1.64)\n",
      "Old & New Losses 2154.2086601257324 2147.3472118377686 Probab: tensor(64.9799, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 799, LR: 0.0153, Train Loss: 2.1533, Train Accuracy: 23.10%, Temperatures:(0.00, 1.63)\n",
      "Old & New Losses 2153.4297466278076 2146.4977264404297 Probab: tensor(70.7820, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 800, LR: 0.0153, Train Loss: 2.1532, Train Accuracy: 22.80%, Temperatures:(0.00, 1.61)\n",
      "Old & New Losses 2146.2996006011963 2145.5225944519043 Probab: tensor(1.6198, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 801, LR: 0.0153, Train Loss: 2.1503, Train Accuracy: 24.00%, Temperatures:(0.00, 1.60)\n",
      "Old & New Losses 2148.245334625244 2149.2786407470703 Probab: tensor(0.5232, device='cuda:0')\n",
      "Epoch 802, LR: 0.0076, Train Loss: 2.1450, Train Accuracy: 25.20%, Temperatures:(0.00, 1.58)\n",
      "Old & New Losses 2145.2951431274414 2149.1236686706543 Probab: tensor(0.0885, device='cuda:0')\n",
      "Epoch 803, LR: 0.0076, Train Loss: 2.1453, Train Accuracy: 22.70%, Temperatures:(0.00, 1.56)\n",
      "Old & New Losses 2144.850969314575 2150.338888168335 Probab: tensor(0.0299, device='cuda:0')\n",
      "Epoch 804, LR: 0.0076, Train Loss: 2.1462, Train Accuracy: 23.70%, Temperatures:(0.00, 1.55)\n",
      "Old & New Losses 2144.965887069702 2150.0697135925293 Probab: tensor(0.0370, device='cuda:0')\n",
      "Epoch 805, LR: 0.0076, Train Loss: 2.1527, Train Accuracy: 22.80%, Temperatures:(0.00, 1.53)\n",
      "Old & New Losses 2151.042699813843 2143.2478427886963 Probab: tensor(161.9884, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 806, LR: 0.0076, Train Loss: 2.1444, Train Accuracy: 24.20%, Temperatures:(0.00, 1.52)\n",
      "Old & New Losses 2144.449234008789 2144.0913677215576 Probab: tensor(1.2661, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 807, LR: 0.0076, Train Loss: 2.1454, Train Accuracy: 22.90%, Temperatures:(0.00, 1.50)\n",
      "Old & New Losses 2141.984701156616 2146.97003364563 Probab: tensor(0.0362, device='cuda:0')\n",
      "Epoch 808, LR: 0.0076, Train Loss: 2.1431, Train Accuracy: 23.20%, Temperatures:(0.00, 1.49)\n",
      "Old & New Losses 2150.4452228546143 2144.545793533325 Probab: tensor(52.8937, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 809, LR: 0.0076, Train Loss: 2.1420, Train Accuracy: 25.30%, Temperatures:(0.00, 1.47)\n",
      "Old & New Losses 2142.7249908447266 2142.5912380218506 Probab: tensor(1.0951, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 810, LR: 0.0076, Train Loss: 2.1461, Train Accuracy: 24.50%, Temperatures:(0.00, 1.46)\n",
      "Old & New Losses 2145.5585956573486 2149.0442752838135 Probab: tensor(0.0914, device='cuda:0')\n",
      "Epoch 811, LR: 0.0076, Train Loss: 2.1482, Train Accuracy: 22.30%, Temperatures:(0.00, 1.44)\n",
      "Old & New Losses 2146.9368934631348 2159.5206260681152 Probab: tensor(0.0002, device='cuda:0')\n",
      "Epoch 812, LR: 0.0076, Train Loss: 2.1396, Train Accuracy: 22.90%, Temperatures:(0.00, 1.43)\n",
      "Old & New Losses 2149.2202281951904 2141.8957710266113 Probab: tensor(168.8377, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 813, LR: 0.0076, Train Loss: 2.1474, Train Accuracy: 22.10%, Temperatures:(0.00, 1.41)\n",
      "Old & New Losses 2142.106056213379 2137.031316757202 Probab: tensor(36.2149, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 814, LR: 0.0076, Train Loss: 2.1439, Train Accuracy: 24.50%, Temperatures:(0.00, 1.40)\n",
      "Old & New Losses 2145.683526992798 2146.5513706207275 Probab: tensor(0.5379, device='cuda:0')\n",
      "Epoch 815, LR: 0.0076, Train Loss: 2.1454, Train Accuracy: 22.10%, Temperatures:(0.00, 1.39)\n",
      "Old & New Losses 2144.4594860076904 2146.650552749634 Probab: tensor(0.2057, device='cuda:0')\n",
      "Epoch 816, LR: 0.0076, Train Loss: 2.1434, Train Accuracy: 22.30%, Temperatures:(0.00, 1.37)\n",
      "Old & New Losses 2146.9814777374268 2145.4436779022217 Probab: tensor(3.0680, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 817, LR: 0.0076, Train Loss: 2.1427, Train Accuracy: 24.00%, Temperatures:(0.00, 1.36)\n",
      "Old & New Losses 2147.113800048828 2152.15802192688 Probab: tensor(0.0244, device='cuda:0')\n",
      "Epoch 818, LR: 0.0076, Train Loss: 2.1505, Train Accuracy: 24.40%, Temperatures:(0.00, 1.34)\n",
      "Old & New Losses 2146.462917327881 2156.1076641082764 Probab: tensor(0.0008, device='cuda:0')\n",
      "Epoch 819, LR: 0.0076, Train Loss: 2.1432, Train Accuracy: 25.80%, Temperatures:(0.00, 1.33)\n",
      "Old & New Losses 2147.956132888794 2142.5936222076416 Probab: tensor(56.1924, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 820, LR: 0.0076, Train Loss: 2.1437, Train Accuracy: 22.50%, Temperatures:(0.00, 1.32)\n",
      "Old & New Losses 2141.2110328674316 2153.872013092041 Probab: tensor(6.7182e-05, device='cuda:0')\n",
      "Epoch 821, LR: 0.0076, Train Loss: 2.1432, Train Accuracy: 24.60%, Temperatures:(0.00, 1.30)\n",
      "Old & New Losses 2142.6286697387695 2141.890287399292 Probab: tensor(1.7612, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 822, LR: 0.0076, Train Loss: 2.1417, Train Accuracy: 22.80%, Temperatures:(0.00, 1.29)\n",
      "Old & New Losses 2143.765926361084 2146.7859745025635 Probab: tensor(0.0965, device='cuda:0')\n",
      "Epoch 823, LR: 0.0076, Train Loss: 2.1446, Train Accuracy: 23.10%, Temperatures:(0.00, 1.28)\n",
      "Old & New Losses 2145.4174518585205 2145.052194595337 Probab: tensor(1.3307, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 824, LR: 0.0076, Train Loss: 2.1384, Train Accuracy: 25.30%, Temperatures:(0.00, 1.27)\n",
      "Old & New Losses 2148.8962173461914 2148.881196975708 Probab: tensor(1.0119, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 825, LR: 0.0076, Train Loss: 2.1480, Train Accuracy: 22.60%, Temperatures:(0.00, 1.25)\n",
      "Old & New Losses 2145.9991931915283 2157.5207710266113 Probab: tensor(0.0001, device='cuda:0')\n",
      "Epoch 826, LR: 0.0076, Train Loss: 2.1460, Train Accuracy: 24.70%, Temperatures:(0.00, 1.24)\n",
      "Old & New Losses 2142.9357528686523 2151.8023014068604 Probab: tensor(0.0008, device='cuda:0')\n",
      "Epoch 827, LR: 0.0076, Train Loss: 2.1495, Train Accuracy: 23.30%, Temperatures:(0.00, 1.23)\n",
      "Old & New Losses 2146.099090576172 2151.398181915283 Probab: tensor(0.0134, device='cuda:0')\n",
      "Epoch 828, LR: 0.0076, Train Loss: 2.1524, Train Accuracy: 22.20%, Temperatures:(0.00, 1.22)\n",
      "Old & New Losses 2152.9741287231445 2151.399612426758 Probab: tensor(3.6506, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 829, LR: 0.0076, Train Loss: 2.1530, Train Accuracy: 22.30%, Temperatures:(0.00, 1.20)\n",
      "Old & New Losses 2155.0333499908447 2156.2321186065674 Probab: tensor(0.3694, device='cuda:0')\n",
      "Epoch 830, LR: 0.0076, Train Loss: 2.1552, Train Accuracy: 21.90%, Temperatures:(0.00, 1.19)\n",
      "Old & New Losses 2151.352643966675 2152.600049972534 Probab: tensor(0.3511, device='cuda:0')\n",
      "Epoch 831, LR: 0.0076, Train Loss: 2.1511, Train Accuracy: 22.80%, Temperatures:(0.00, 1.18)\n",
      "Old & New Losses 2147.5348472595215 2157.4008464813232 Probab: tensor(0.0002, device='cuda:0')\n",
      "Epoch 832, LR: 0.0076, Train Loss: 2.1545, Train Accuracy: 21.50%, Temperatures:(0.00, 1.17)\n",
      "Old & New Losses 2154.245138168335 2151.4642238616943 Probab: tensor(10.8143, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 833, LR: 0.0076, Train Loss: 2.1473, Train Accuracy: 21.10%, Temperatures:(0.00, 1.16)\n",
      "Old & New Losses 2151.7398357391357 2155.3430557250977 Probab: tensor(0.0443, device='cuda:0')\n",
      "Epoch 834, LR: 0.0076, Train Loss: 2.1530, Train Accuracy: 23.70%, Temperatures:(0.00, 1.14)\n",
      "Old & New Losses 2150.4807472229004 2156.245470046997 Probab: tensor(0.0065, device='cuda:0')\n",
      "Epoch 835, LR: 0.0076, Train Loss: 2.1534, Train Accuracy: 22.10%, Temperatures:(0.00, 1.13)\n",
      "Old & New Losses 2155.7323932647705 2152.9603004455566 Probab: tensor(11.5417, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 836, LR: 0.0076, Train Loss: 2.1541, Train Accuracy: 22.40%, Temperatures:(0.00, 1.12)\n",
      "Old & New Losses 2154.090642929077 2153.949737548828 Probab: tensor(1.1338, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 837, LR: 0.0076, Train Loss: 2.1574, Train Accuracy: 22.00%, Temperatures:(0.00, 1.11)\n",
      "Old & New Losses 2149.9133110046387 2154.667377471924 Probab: tensor(0.0138, device='cuda:0')\n",
      "Epoch 838, LR: 0.0076, Train Loss: 2.1541, Train Accuracy: 22.90%, Temperatures:(0.00, 1.10)\n",
      "Old & New Losses 2149.183988571167 2149.8427391052246 Probab: tensor(0.5493, device='cuda:0')\n",
      "Epoch 839, LR: 0.0076, Train Loss: 2.1558, Train Accuracy: 22.00%, Temperatures:(0.00, 1.09)\n",
      "Old & New Losses 2153.1999111175537 2153.578758239746 Probab: tensor(0.7061, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 840, LR: 0.0076, Train Loss: 2.1514, Train Accuracy: 23.20%, Temperatures:(0.00, 1.08)\n",
      "Old & New Losses 2160.2885723114014 2153.7599563598633 Probab: tensor(427.2716, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 841, LR: 0.0076, Train Loss: 2.1588, Train Accuracy: 21.70%, Temperatures:(0.00, 1.07)\n",
      "Old & New Losses 2156.625509262085 2152.5089740753174 Probab: tensor(47.3709, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 842, LR: 0.0076, Train Loss: 2.1545, Train Accuracy: 20.60%, Temperatures:(0.00, 1.06)\n",
      "Old & New Losses 2159.999132156372 2154.8357009887695 Probab: tensor(132.6929, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 843, LR: 0.0076, Train Loss: 2.1598, Train Accuracy: 20.30%, Temperatures:(0.00, 1.05)\n",
      "Old & New Losses 2152.2817611694336 2155.4880142211914 Probab: tensor(0.0466, device='cuda:0')\n",
      "Epoch 844, LR: 0.0076, Train Loss: 2.1557, Train Accuracy: 22.90%, Temperatures:(0.00, 1.04)\n",
      "Old & New Losses 2153.785467147827 2157.999277114868 Probab: tensor(0.0171, device='cuda:0')\n",
      "Epoch 845, LR: 0.0076, Train Loss: 2.1549, Train Accuracy: 21.70%, Temperatures:(0.00, 1.02)\n",
      "Old & New Losses 2151.669502258301 2153.8188457489014 Probab: tensor(0.1228, device='cuda:0')\n",
      "Epoch 846, LR: 0.0076, Train Loss: 2.1556, Train Accuracy: 21.90%, Temperatures:(0.00, 1.01)\n",
      "Old & New Losses 2153.503179550171 2185.229539871216 Probab: tensor(2.6378e-14, device='cuda:0')\n",
      "Epoch 847, LR: 0.0076, Train Loss: 2.1530, Train Accuracy: 23.10%, Temperatures:(0.00, 1.00)\n",
      "Old & New Losses 2152.5213718414307 2153.852939605713 Probab: tensor(0.2657, device='cuda:0')\n",
      "Epoch 848, LR: 0.0076, Train Loss: 2.1564, Train Accuracy: 22.40%, Temperatures:(0.00, 0.99)\n",
      "Old & New Losses 2154.816150665283 2157.7768325805664 Probab: tensor(0.0509, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 849, LR: 0.0076, Train Loss: 2.1558, Train Accuracy: 20.50%, Temperatures:(0.00, 0.98)\n",
      "Old & New Losses 2153.6641120910645 2153.320074081421 Probab: tensor(1.4183, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 850, LR: 0.0076, Train Loss: 2.1515, Train Accuracy: 22.90%, Temperatures:(0.00, 0.97)\n",
      "Old & New Losses 2156.8639278411865 2148.8046646118164 Probab: tensor(3897.8481, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 851, LR: 0.0076, Train Loss: 2.1552, Train Accuracy: 22.50%, Temperatures:(0.00, 0.96)\n",
      "Old & New Losses 2152.0185470581055 2157.259702682495 Probab: tensor(0.0044, device='cuda:0')\n",
      "Epoch 852, LR: 0.0038, Train Loss: 2.1547, Train Accuracy: 21.70%, Temperatures:(0.00, 0.96)\n",
      "Old & New Losses 2155.640125274658 2151.0093212127686 Probab: tensor(127.3969, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 853, LR: 0.0038, Train Loss: 2.1591, Train Accuracy: 19.00%, Temperatures:(0.00, 0.95)\n",
      "Old & New Losses 2146.8842029571533 2149.113655090332 Probab: tensor(0.0947, device='cuda:0')\n",
      "Epoch 854, LR: 0.0038, Train Loss: 2.1486, Train Accuracy: 21.40%, Temperatures:(0.00, 0.94)\n",
      "Old & New Losses 2152.9457569122314 2150.8584022521973 Probab: tensor(9.2934, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 855, LR: 0.0038, Train Loss: 2.1489, Train Accuracy: 21.30%, Temperatures:(0.00, 0.93)\n",
      "Old & New Losses 2148.487091064453 2156.7704677581787 Probab: tensor(0.0001, device='cuda:0')\n",
      "Epoch 856, LR: 0.0038, Train Loss: 2.1486, Train Accuracy: 21.10%, Temperatures:(0.00, 0.92)\n",
      "Old & New Losses 2151.132822036743 2144.1776752471924 Probab: tensor(1956.5967, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 857, LR: 0.0038, Train Loss: 2.1528, Train Accuracy: 21.00%, Temperatures:(0.00, 0.91)\n",
      "Old & New Losses 2154.96563911438 2152.146339416504 Probab: tensor(22.2690, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 858, LR: 0.0038, Train Loss: 2.1524, Train Accuracy: 21.60%, Temperatures:(0.00, 0.90)\n",
      "Old & New Losses 2154.98685836792 2163.8970375061035 Probab: tensor(4.9850e-05, device='cuda:0')\n",
      "Epoch 859, LR: 0.0038, Train Loss: 2.1489, Train Accuracy: 20.10%, Temperatures:(0.00, 0.89)\n",
      "Old & New Losses 2149.8916149139404 2150.2845287323 Probab: tensor(0.6432, device='cuda:0')\n",
      "Epoch 860, LR: 0.0038, Train Loss: 2.1504, Train Accuracy: 21.90%, Temperatures:(0.00, 0.88)\n",
      "Old & New Losses 2154.4485092163086 2157.5560569763184 Probab: tensor(0.0294, device='cuda:0')\n",
      "Epoch 861, LR: 0.0038, Train Loss: 2.1544, Train Accuracy: 19.50%, Temperatures:(0.00, 0.87)\n",
      "Old & New Losses 2155.961751937866 2171.126127243042 Probab: tensor(2.8422e-08, device='cuda:0')\n",
      "Epoch 862, LR: 0.0038, Train Loss: 2.1500, Train Accuracy: 20.70%, Temperatures:(0.00, 0.86)\n",
      "Old & New Losses 2152.024269104004 2149.2159366607666 Probab: tensor(25.8014, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 863, LR: 0.0038, Train Loss: 2.1536, Train Accuracy: 21.70%, Temperatures:(0.00, 0.86)\n",
      "Old & New Losses 2152.095317840576 2152.6293754577637 Probab: tensor(0.5356, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 864, LR: 0.0038, Train Loss: 2.1511, Train Accuracy: 21.80%, Temperatures:(0.00, 0.85)\n",
      "Old & New Losses 2149.672746658325 2156.681776046753 Probab: tensor(0.0003, device='cuda:0')\n",
      "Epoch 865, LR: 0.0038, Train Loss: 2.1542, Train Accuracy: 23.60%, Temperatures:(0.00, 0.84)\n",
      "Old & New Losses 2152.44722366333 2149.984121322632 Probab: tensor(18.8803, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 866, LR: 0.0038, Train Loss: 2.1512, Train Accuracy: 21.20%, Temperatures:(0.00, 0.83)\n",
      "Old & New Losses 2152.2834300994873 2148.547410964966 Probab: tensor(90.1554, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 867, LR: 0.0038, Train Loss: 2.1529, Train Accuracy: 22.50%, Temperatures:(0.00, 0.82)\n",
      "Old & New Losses 2152.60910987854 2142.839193344116 Probab: tensor(145902.1875, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 868, LR: 0.0038, Train Loss: 2.1519, Train Accuracy: 20.80%, Temperatures:(0.00, 0.81)\n",
      "Old & New Losses 2148.8027572631836 2144.402503967285 Probab: tensor(223.5243, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 869, LR: 0.0038, Train Loss: 2.1530, Train Accuracy: 22.70%, Temperatures:(0.00, 0.81)\n",
      "Old & New Losses 2146.437406539917 2151.4554023742676 Probab: tensor(0.0020, device='cuda:0')\n",
      "Epoch 870, LR: 0.0038, Train Loss: 2.1462, Train Accuracy: 21.30%, Temperatures:(0.00, 0.80)\n",
      "Old & New Losses 2142.137289047241 2148.3781337738037 Probab: tensor(0.0004, device='cuda:0')\n",
      "Epoch 871, LR: 0.0038, Train Loss: 2.1492, Train Accuracy: 21.60%, Temperatures:(0.00, 0.79)\n",
      "Old & New Losses 2147.9592323303223 2150.6454944610596 Probab: tensor(0.0333, device='cuda:0')\n",
      "Epoch 872, LR: 0.0038, Train Loss: 2.1438, Train Accuracy: 22.50%, Temperatures:(0.00, 0.78)\n",
      "Old & New Losses 2148.228406906128 2154.080390930176 Probab: tensor(0.0006, device='cuda:0')\n",
      "Epoch 873, LR: 0.0038, Train Loss: 2.1506, Train Accuracy: 22.50%, Temperatures:(0.00, 0.77)\n",
      "Old & New Losses 2143.505573272705 2149.2390632629395 Probab: tensor(0.0006, device='cuda:0')\n",
      "Epoch 874, LR: 0.0038, Train Loss: 2.1494, Train Accuracy: 22.90%, Temperatures:(0.00, 0.77)\n",
      "Old & New Losses 2140.166759490967 2149.7890949249268 Probab: tensor(3.4933e-06, device='cuda:0')\n",
      "Epoch 875, LR: 0.0038, Train Loss: 2.1422, Train Accuracy: 22.70%, Temperatures:(0.00, 0.76)\n",
      "Old & New Losses 2145.40958404541 2148.482084274292 Probab: tensor(0.0174, device='cuda:0')\n",
      "Epoch 876, LR: 0.0038, Train Loss: 2.1456, Train Accuracy: 22.30%, Temperatures:(0.00, 0.75)\n",
      "Old & New Losses 2142.39239692688 2141.4217948913574 Probab: tensor(3.6442, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 877, LR: 0.0038, Train Loss: 2.1424, Train Accuracy: 22.50%, Temperatures:(0.00, 0.74)\n",
      "Old & New Losses 2149.6336460113525 2146.2764739990234 Probab: tensor(91.6442, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 878, LR: 0.0038, Train Loss: 2.1455, Train Accuracy: 24.80%, Temperatures:(0.00, 0.74)\n",
      "Old & New Losses 2142.772436141968 2139.6970748901367 Probab: tensor(65.3967, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 879, LR: 0.0038, Train Loss: 2.1438, Train Accuracy: 20.90%, Temperatures:(0.00, 0.73)\n",
      "Old & New Losses 2144.7887420654297 2149.266004562378 Probab: tensor(0.0021, device='cuda:0')\n",
      "Epoch 880, LR: 0.0038, Train Loss: 2.1393, Train Accuracy: 23.80%, Temperatures:(0.00, 0.72)\n",
      "Old & New Losses 2136.902332305908 2133.9473724365234 Probab: tensor(60.2415, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 881, LR: 0.0038, Train Loss: 2.1412, Train Accuracy: 24.00%, Temperatures:(0.00, 0.71)\n",
      "Old & New Losses 2143.7113285064697 2144.442319869995 Probab: tensor(0.3591, device='cuda:0')\n",
      "Epoch 882, LR: 0.0038, Train Loss: 2.1351, Train Accuracy: 24.80%, Temperatures:(0.00, 0.71)\n",
      "Old & New Losses 2141.7016983032227 2145.2410221099854 Probab: tensor(0.0067, device='cuda:0')\n",
      "Epoch 883, LR: 0.0038, Train Loss: 2.1383, Train Accuracy: 24.70%, Temperatures:(0.00, 0.70)\n",
      "Old & New Losses 2138.4949684143066 2143.8863277435303 Probab: tensor(0.0004, device='cuda:0')\n",
      "Epoch 884, LR: 0.0038, Train Loss: 2.1401, Train Accuracy: 24.00%, Temperatures:(0.00, 0.69)\n",
      "Old & New Losses 2139.8231983184814 2142.336130142212 Probab: tensor(0.0266, device='cuda:0')\n",
      "Epoch 885, LR: 0.0038, Train Loss: 2.1420, Train Accuracy: 23.80%, Temperatures:(0.00, 0.69)\n",
      "Old & New Losses 2139.7130489349365 2136.4941596984863 Probab: tensor(109.3435, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 886, LR: 0.0038, Train Loss: 2.1404, Train Accuracy: 24.40%, Temperatures:(0.00, 0.68)\n",
      "Old & New Losses 2139.713764190674 2142.00758934021 Probab: tensor(0.0341, device='cuda:0')\n",
      "Epoch 887, LR: 0.0038, Train Loss: 2.1388, Train Accuracy: 24.10%, Temperatures:(0.00, 0.67)\n",
      "Old & New Losses 2140.8944129943848 2138.370990753174 Probab: tensor(42.7314, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 888, LR: 0.0038, Train Loss: 2.1376, Train Accuracy: 25.20%, Temperatures:(0.00, 0.67)\n",
      "Old & New Losses 2135.5693340301514 2141.7384147644043 Probab: tensor(9.3972e-05, device='cuda:0')\n",
      "Epoch 889, LR: 0.0038, Train Loss: 2.1382, Train Accuracy: 23.20%, Temperatures:(0.00, 0.66)\n",
      "Old & New Losses 2141.3235664367676 2142.212152481079 Probab: tensor(0.2595, device='cuda:0')\n",
      "Epoch 890, LR: 0.0038, Train Loss: 2.1388, Train Accuracy: 26.00%, Temperatures:(0.00, 0.65)\n",
      "Old & New Losses 2137.962818145752 2140.11812210083 Probab: tensor(0.0367, device='cuda:0')\n",
      "Epoch 891, LR: 0.0038, Train Loss: 2.1414, Train Accuracy: 24.00%, Temperatures:(0.00, 0.65)\n",
      "Old & New Losses 2138.4730339050293 2137.5279426574707 Probab: tensor(4.3233, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 892, LR: 0.0038, Train Loss: 2.1431, Train Accuracy: 24.40%, Temperatures:(0.00, 0.64)\n",
      "Old & New Losses 2143.3701515197754 2141.4263248443604 Probab: tensor(20.9375, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 893, LR: 0.0038, Train Loss: 2.1420, Train Accuracy: 25.60%, Temperatures:(0.00, 0.63)\n",
      "Old & New Losses 2146.9624042510986 2149.4662761688232 Probab: tensor(0.0191, device='cuda:0')\n",
      "Epoch 894, LR: 0.0038, Train Loss: 2.1487, Train Accuracy: 23.50%, Temperatures:(0.00, 0.63)\n",
      "Old & New Losses 2145.965576171875 2147.904396057129 Probab: tensor(0.0453, device='cuda:0')\n",
      "Epoch 895, LR: 0.0038, Train Loss: 2.1461, Train Accuracy: 24.50%, Temperatures:(0.00, 0.62)\n",
      "Old & New Losses 2145.326614379883 2151.4477729797363 Probab: tensor(5.1647e-05, device='cuda:0')\n",
      "Epoch 896, LR: 0.0038, Train Loss: 2.1421, Train Accuracy: 24.30%, Temperatures:(0.00, 0.61)\n",
      "Old & New Losses 2141.0233974456787 2151.353120803833 Probab: tensor(4.9261e-08, device='cuda:0')\n",
      "Epoch 897, LR: 0.0038, Train Loss: 2.1472, Train Accuracy: 24.00%, Temperatures:(0.00, 0.61)\n",
      "Old & New Losses 2151.4251232147217 2141.7665481567383 Probab: tensor(7974944., device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 898, LR: 0.0038, Train Loss: 2.1491, Train Accuracy: 23.70%, Temperatures:(0.00, 0.60)\n",
      "Old & New Losses 2146.7292308807373 2148.723840713501 Probab: tensor(0.0363, device='cuda:0')\n",
      "Epoch 899, LR: 0.0038, Train Loss: 2.1504, Train Accuracy: 23.40%, Temperatures:(0.00, 0.60)\n",
      "Old & New Losses 2151.010513305664 2149.6100425720215 Probab: tensor(10.4967, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 900, LR: 0.0038, Train Loss: 2.1479, Train Accuracy: 23.70%, Temperatures:(0.00, 0.59)\n",
      "Old & New Losses 2153.7885665893555 2153.6641120910645 Probab: tensor(1.2350, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 901, LR: 0.0038, Train Loss: 2.1512, Train Accuracy: 25.10%, Temperatures:(0.00, 0.58)\n",
      "Old & New Losses 2152.2562503814697 2150.970458984375 Probab: tensor(9.0464, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 902, LR: 0.0019, Train Loss: 2.1539, Train Accuracy: 25.60%, Temperatures:(0.00, 0.58)\n",
      "Old & New Losses 2148.6990451812744 2148.2605934143066 Probab: tensor(2.1353, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 903, LR: 0.0019, Train Loss: 2.1556, Train Accuracy: 26.40%, Temperatures:(0.00, 0.57)\n",
      "Old & New Losses 2155.569314956665 2156.130313873291 Probab: tensor(0.3752, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 904, LR: 0.0019, Train Loss: 2.1530, Train Accuracy: 25.90%, Temperatures:(0.00, 0.57)\n",
      "Old & New Losses 2153.6524295806885 2146.925926208496 Probab: tensor(143512.1562, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 905, LR: 0.0019, Train Loss: 2.1572, Train Accuracy: 25.80%, Temperatures:(0.00, 0.56)\n",
      "Old & New Losses 2152.6660919189453 2157.525062561035 Probab: tensor(0.0002, device='cuda:0')\n",
      "Epoch 906, LR: 0.0019, Train Loss: 2.1488, Train Accuracy: 26.00%, Temperatures:(0.00, 0.56)\n",
      "Old & New Losses 2154.2067527770996 2155.282497406006 Probab: tensor(0.1441, device='cuda:0')\n",
      "Epoch 907, LR: 0.0019, Train Loss: 2.1553, Train Accuracy: 27.10%, Temperatures:(0.00, 0.55)\n",
      "Old & New Losses 2151.081323623657 2152.8594493865967 Probab: tensor(0.0394, device='cuda:0')\n",
      "Epoch 908, LR: 0.0019, Train Loss: 2.1510, Train Accuracy: 27.40%, Temperatures:(0.00, 0.54)\n",
      "Old & New Losses 2152.18448638916 2164.6077632904053 Probab: tensor(1.2161e-10, device='cuda:0')\n",
      "Epoch 909, LR: 0.0019, Train Loss: 2.1522, Train Accuracy: 27.30%, Temperatures:(0.00, 0.54)\n",
      "Old & New Losses 2148.892641067505 2156.905174255371 Probab: tensor(3.4722e-07, device='cuda:0')\n",
      "Epoch 910, LR: 0.0019, Train Loss: 2.1532, Train Accuracy: 25.90%, Temperatures:(0.00, 0.53)\n",
      "Old & New Losses 2152.435302734375 2150.4170894622803 Probab: tensor(43.9989, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 911, LR: 0.0019, Train Loss: 2.1535, Train Accuracy: 26.40%, Temperatures:(0.00, 0.53)\n",
      "Old & New Losses 2154.1616916656494 2154.747486114502 Probab: tensor(0.3297, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 912, LR: 0.0019, Train Loss: 2.1524, Train Accuracy: 25.50%, Temperatures:(0.00, 0.52)\n",
      "Old & New Losses 2152.1918773651123 2157.8927040100098 Probab: tensor(1.8346e-05, device='cuda:0')\n",
      "Epoch 913, LR: 0.0019, Train Loss: 2.1542, Train Accuracy: 26.60%, Temperatures:(0.00, 0.52)\n",
      "Old & New Losses 2154.8938751220703 2164.689064025879 Probab: tensor(6.0197e-09, device='cuda:0')\n",
      "Epoch 914, LR: 0.0019, Train Loss: 2.1508, Train Accuracy: 26.70%, Temperatures:(0.00, 0.51)\n",
      "Old & New Losses 2150.6118774414062 2153.721570968628 Probab: tensor(0.0023, device='cuda:0')\n",
      "Epoch 915, LR: 0.0019, Train Loss: 2.1519, Train Accuracy: 25.60%, Temperatures:(0.00, 0.51)\n",
      "Old & New Losses 2148.86474609375 2156.308174133301 Probab: tensor(4.2307e-07, device='cuda:0')\n",
      "Epoch 916, LR: 0.0019, Train Loss: 2.1513, Train Accuracy: 27.60%, Temperatures:(0.00, 0.50)\n",
      "Old & New Losses 2153.2065868377686 2158.51092338562 Probab: tensor(2.5833e-05, device='cuda:0')\n",
      "Epoch 917, LR: 0.0019, Train Loss: 2.1527, Train Accuracy: 26.40%, Temperatures:(0.00, 0.50)\n",
      "Old & New Losses 2150.811195373535 2161.9796752929688 Probab: tensor(1.7482e-10, device='cuda:0')\n",
      "Epoch 918, LR: 0.0019, Train Loss: 2.1570, Train Accuracy: 24.70%, Temperatures:(0.00, 0.49)\n",
      "Old & New Losses 2157.381772994995 2154.0260314941406 Probab: tensor(914.9140, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 919, LR: 0.0019, Train Loss: 2.1567, Train Accuracy: 25.30%, Temperatures:(0.00, 0.49)\n",
      "Old & New Losses 2149.7414112091064 2160.489320755005 Probab: tensor(2.6264e-10, device='cuda:0')\n",
      "Epoch 920, LR: 0.0019, Train Loss: 2.1559, Train Accuracy: 26.70%, Temperatures:(0.00, 0.48)\n",
      "Old & New Losses 2155.0464630126953 2147.6361751556396 Probab: tensor(4701323.5000, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 921, LR: 0.0019, Train Loss: 2.1572, Train Accuracy: 28.00%, Temperatures:(0.00, 0.48)\n",
      "Old & New Losses 2147.676944732666 2154.844284057617 Probab: tensor(3.0293e-07, device='cuda:0')\n",
      "Epoch 922, LR: 0.0019, Train Loss: 2.1522, Train Accuracy: 26.60%, Temperatures:(0.00, 0.47)\n",
      "Old & New Losses 2153.993844985962 2150.6471633911133 Probab: tensor(1187.2272, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 923, LR: 0.0019, Train Loss: 2.1494, Train Accuracy: 27.60%, Temperatures:(0.00, 0.47)\n",
      "Old & New Losses 2152.899742126465 2154.982566833496 Probab: tensor(0.0117, device='cuda:0')\n",
      "Epoch 924, LR: 0.0019, Train Loss: 2.1498, Train Accuracy: 26.10%, Temperatures:(0.00, 0.46)\n",
      "Old & New Losses 2154.5300483703613 2148.8733291625977 Probab: tensor(200556.6094, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 925, LR: 0.0019, Train Loss: 2.1510, Train Accuracy: 26.10%, Temperatures:(0.00, 0.46)\n",
      "Old & New Losses 2149.4836807250977 2150.5796909332275 Probab: tensor(0.0917, device='cuda:0')\n",
      "Epoch 926, LR: 0.0019, Train Loss: 2.1414, Train Accuracy: 26.00%, Temperatures:(0.00, 0.45)\n",
      "Old & New Losses 2150.9010791778564 2152.0166397094727 Probab: tensor(0.0857, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 927, LR: 0.0019, Train Loss: 2.1500, Train Accuracy: 26.40%, Temperatures:(0.00, 0.45)\n",
      "Old & New Losses 2152.9107093811035 2152.1172523498535 Probab: tensor(5.8412, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 928, LR: 0.0019, Train Loss: 2.1555, Train Accuracy: 24.50%, Temperatures:(0.00, 0.45)\n",
      "Old & New Losses 2154.8736095428467 2150.4170894622803 Probab: tensor(22315.2207, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 929, LR: 0.0019, Train Loss: 2.1531, Train Accuracy: 26.00%, Temperatures:(0.00, 0.44)\n",
      "Old & New Losses 2157.623052597046 2160.694360733032 Probab: tensor(0.0009, device='cuda:0')\n",
      "Epoch 930, LR: 0.0019, Train Loss: 2.1557, Train Accuracy: 25.30%, Temperatures:(0.00, 0.44)\n",
      "Old & New Losses 2157.2577953338623 2174.126625061035 Probab: tensor(1.6049e-17, device='cuda:0')\n",
      "Epoch 931, LR: 0.0019, Train Loss: 2.1515, Train Accuracy: 25.40%, Temperatures:(0.00, 0.43)\n",
      "Old & New Losses 2153.6848545074463 2159.336805343628 Probab: tensor(2.0706e-06, device='cuda:0')\n",
      "Epoch 932, LR: 0.0019, Train Loss: 2.1574, Train Accuracy: 25.70%, Temperatures:(0.00, 0.43)\n",
      "Old & New Losses 2157.0165157318115 2163.4016036987305 Probab: tensor(3.2656e-07, device='cuda:0')\n",
      "Epoch 933, LR: 0.0019, Train Loss: 2.1570, Train Accuracy: 26.40%, Temperatures:(0.00, 0.42)\n",
      "Old & New Losses 2158.9131355285645 2161.4019870758057 Probab: tensor(0.0028, device='cuda:0')\n",
      "Epoch 934, LR: 0.0019, Train Loss: 2.1540, Train Accuracy: 26.70%, Temperatures:(0.00, 0.42)\n",
      "Old & New Losses 2153.5582542419434 2157.2413444519043 Probab: tensor(0.0002, device='cuda:0')\n",
      "Epoch 935, LR: 0.0019, Train Loss: 2.1528, Train Accuracy: 26.50%, Temperatures:(0.00, 0.41)\n",
      "Old & New Losses 2153.250217437744 2157.6485633850098 Probab: tensor(2.4851e-05, device='cuda:0')\n",
      "Epoch 936, LR: 0.0019, Train Loss: 2.1561, Train Accuracy: 25.60%, Temperatures:(0.00, 0.41)\n",
      "Old & New Losses 2159.621477127075 2175.21071434021 Probab: tensor(3.2711e-17, device='cuda:0')\n",
      "Epoch 937, LR: 0.0019, Train Loss: 2.1553, Train Accuracy: 26.30%, Temperatures:(0.00, 0.41)\n",
      "Old & New Losses 2154.6874046325684 2152.7159214019775 Probab: tensor(127.6035, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 938, LR: 0.0019, Train Loss: 2.1582, Train Accuracy: 26.00%, Temperatures:(0.00, 0.40)\n",
      "Old & New Losses 2152.027368545532 2156.775712966919 Probab: tensor(7.5295e-06, device='cuda:0')\n",
      "Epoch 939, LR: 0.0019, Train Loss: 2.1502, Train Accuracy: 25.90%, Temperatures:(0.00, 0.40)\n",
      "Old & New Losses 2155.028820037842 2160.825490951538 Probab: tensor(4.8139e-07, device='cuda:0')\n",
      "Epoch 940, LR: 0.0019, Train Loss: 2.1501, Train Accuracy: 25.10%, Temperatures:(0.00, 0.39)\n",
      "Old & New Losses 2153.376579284668 2153.632164001465 Probab: tensor(0.5232, device='cuda:0')\n",
      "Epoch 941, LR: 0.0019, Train Loss: 2.1565, Train Accuracy: 24.90%, Temperatures:(0.00, 0.39)\n",
      "Old & New Losses 2156.2578678131104 2160.4228019714355 Probab: tensor(2.3371e-05, device='cuda:0')\n",
      "Epoch 942, LR: 0.0019, Train Loss: 2.1546, Train Accuracy: 25.60%, Temperatures:(0.00, 0.39)\n",
      "Old & New Losses 2156.329393386841 2160.5303287506104 Probab: tensor(1.9119e-05, device='cuda:0')\n",
      "Epoch 943, LR: 0.0019, Train Loss: 2.1575, Train Accuracy: 24.50%, Temperatures:(0.00, 0.38)\n",
      "Old & New Losses 2157.6571464538574 2156.881809234619 Probab: tensor(7.5799, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 944, LR: 0.0019, Train Loss: 2.1605, Train Accuracy: 24.60%, Temperatures:(0.00, 0.38)\n",
      "Old & New Losses 2156.254291534424 2153.470516204834 Probab: tensor(1549.7102, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 945, LR: 0.0019, Train Loss: 2.1572, Train Accuracy: 26.40%, Temperatures:(0.00, 0.38)\n",
      "Old & New Losses 2153.5117626190186 2157.4738025665283 Probab: tensor(2.5916e-05, device='cuda:0')\n",
      "Epoch 946, LR: 0.0019, Train Loss: 2.1533, Train Accuracy: 24.70%, Temperatures:(0.00, 0.37)\n",
      "Old & New Losses 2153.3405780792236 2156.2881469726562 Probab: tensor(0.0004, device='cuda:0')\n",
      "Epoch 947, LR: 0.0019, Train Loss: 2.1560, Train Accuracy: 25.20%, Temperatures:(0.00, 0.37)\n",
      "Old & New Losses 2157.038927078247 2153.3851623535156 Probab: tensor(20674.9863, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 948, LR: 0.0019, Train Loss: 2.1565, Train Accuracy: 25.90%, Temperatures:(0.00, 0.36)\n",
      "Old & New Losses 2154.8192501068115 2150.0892639160156 Probab: tensor(439536.8438, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 949, LR: 0.0019, Train Loss: 2.1556, Train Accuracy: 24.30%, Temperatures:(0.00, 0.36)\n",
      "Old & New Losses 2157.569646835327 2156.229257583618 Probab: tensor(41.2355, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 950, LR: 0.0019, Train Loss: 2.1548, Train Accuracy: 25.10%, Temperatures:(0.00, 0.36)\n",
      "Old & New Losses 2161.771297454834 2152.7011394500732 Probab: tensor(1.0981e+11, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 951, LR: 0.0019, Train Loss: 2.1511, Train Accuracy: 26.10%, Temperatures:(0.00, 0.35)\n",
      "Old & New Losses 2155.118227005005 2175.6715774536133 Probab: tensor(5.3548e-26, device='cuda:0')\n",
      "Epoch 952, LR: 0.0010, Train Loss: 2.1548, Train Accuracy: 23.70%, Temperatures:(0.00, 0.35)\n",
      "Old & New Losses 2155.759811401367 2157.092809677124 Probab: tensor(0.0221, device='cuda:0')\n",
      "Epoch 953, LR: 0.0010, Train Loss: 2.1530, Train Accuracy: 23.80%, Temperatures:(0.00, 0.35)\n",
      "Old & New Losses 2152.2140502929688 2155.5871963500977 Probab: tensor(5.8662e-05, device='cuda:0')\n",
      "Epoch 954, LR: 0.0010, Train Loss: 2.1501, Train Accuracy: 24.90%, Temperatures:(0.00, 0.34)\n",
      "Old & New Losses 2153.574228286743 2157.308340072632 Probab: tensor(1.8544e-05, device='cuda:0')\n",
      "Epoch 955, LR: 0.0010, Train Loss: 2.1524, Train Accuracy: 24.40%, Temperatures:(0.00, 0.34)\n",
      "Old & New Losses 2152.0588397979736 2158.721923828125 Probab: tensor(2.9606e-09, device='cuda:0')\n",
      "Epoch 956, LR: 0.0010, Train Loss: 2.1547, Train Accuracy: 25.40%, Temperatures:(0.00, 0.34)\n",
      "Old & New Losses 2150.480270385742 2158.2248210906982 Probab: tensor(9.7050e-11, device='cuda:0')\n",
      "Epoch 957, LR: 0.0010, Train Loss: 2.1591, Train Accuracy: 23.50%, Temperatures:(0.00, 0.33)\n",
      "Old & New Losses 2155.320882797241 2157.076120376587 Probab: tensor(0.0051, device='cuda:0')\n",
      "Epoch 958, LR: 0.0010, Train Loss: 2.1569, Train Accuracy: 23.80%, Temperatures:(0.00, 0.33)\n",
      "Old & New Losses 2157.0067405700684 2158.2767963409424 Probab: tensor(0.0211, device='cuda:0')\n",
      "Epoch 959, LR: 0.0010, Train Loss: 2.1569, Train Accuracy: 26.30%, Temperatures:(0.00, 0.33)\n",
      "Old & New Losses 2160.0236892700195 2159.1532230377197 Probab: tensor(14.4505, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 960, LR: 0.0010, Train Loss: 2.1589, Train Accuracy: 23.50%, Temperatures:(0.00, 0.32)\n",
      "Old & New Losses 2154.360055923462 2167.9506301879883 Probab: tensor(5.1030e-19, device='cuda:0')\n",
      "Epoch 961, LR: 0.0010, Train Loss: 2.1603, Train Accuracy: 22.20%, Temperatures:(0.00, 0.32)\n",
      "Old & New Losses 2156.7842960357666 2157.660722732544 Probab: tensor(0.0643, device='cuda:0')\n",
      "Epoch 962, LR: 0.0010, Train Loss: 2.1587, Train Accuracy: 25.20%, Temperatures:(0.00, 0.32)\n",
      "Old & New Losses 2159.5723628997803 2159.0352058410645 Probab: tensor(5.4659, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 963, LR: 0.0010, Train Loss: 2.1542, Train Accuracy: 24.20%, Temperatures:(0.00, 0.31)\n",
      "Old & New Losses 2154.6096801757812 2158.4293842315674 Probab: tensor(5.0294e-06, device='cuda:0')\n",
      "Epoch 964, LR: 0.0010, Train Loss: 2.1582, Train Accuracy: 23.30%, Temperatures:(0.00, 0.31)\n",
      "Old & New Losses 2155.797004699707 2163.407325744629 Probab: tensor(2.1714e-11, device='cuda:0')\n",
      "Epoch 965, LR: 0.0010, Train Loss: 2.1531, Train Accuracy: 24.40%, Temperatures:(0.00, 0.31)\n",
      "Old & New Losses 2159.2748165130615 2162.1334552764893 Probab: tensor(8.9979e-05, device='cuda:0')\n",
      "Epoch 966, LR: 0.0010, Train Loss: 2.1598, Train Accuracy: 24.80%, Temperatures:(0.00, 0.30)\n",
      "Old & New Losses 2155.792236328125 2160.447359085083 Probab: tensor(2.2131e-07, device='cuda:0')\n",
      "Epoch 967, LR: 0.0010, Train Loss: 2.1567, Train Accuracy: 23.40%, Temperatures:(0.00, 0.30)\n",
      "Old & New Losses 2162.403345108032 2161.997079849243 Probab: tensor(3.8607, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 968, LR: 0.0010, Train Loss: 2.1598, Train Accuracy: 23.90%, Temperatures:(0.00, 0.30)\n",
      "Old & New Losses 2157.4277877807617 2158.8973999023438 Probab: tensor(0.0072, device='cuda:0')\n",
      "Epoch 969, LR: 0.0010, Train Loss: 2.1556, Train Accuracy: 23.40%, Temperatures:(0.00, 0.29)\n",
      "Old & New Losses 2160.9559059143066 2160.9787940979004 Probab: tensor(0.9253, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 970, LR: 0.0010, Train Loss: 2.1570, Train Accuracy: 22.20%, Temperatures:(0.00, 0.29)\n",
      "Old & New Losses 2163.135766983032 2162.4860763549805 Probab: tensor(9.2662, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 971, LR: 0.0010, Train Loss: 2.1655, Train Accuracy: 20.60%, Temperatures:(0.00, 0.29)\n",
      "Old & New Losses 2161.497116088867 2160.5584621429443 Probab: tensor(25.7668, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 972, LR: 0.0010, Train Loss: 2.1648, Train Accuracy: 22.10%, Temperatures:(0.00, 0.29)\n",
      "Old & New Losses 2161.470651626587 2159.492015838623 Probab: tensor(1010.3964, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 973, LR: 0.0010, Train Loss: 2.1656, Train Accuracy: 22.70%, Temperatures:(0.00, 0.28)\n",
      "Old & New Losses 2163.5499000549316 2165.2300357818604 Probab: tensor(0.0026, device='cuda:0')\n",
      "Epoch 974, LR: 0.0010, Train Loss: 2.1581, Train Accuracy: 22.70%, Temperatures:(0.00, 0.28)\n",
      "Old & New Losses 2162.9838943481445 2166.6436195373535 Probab: tensor(2.1380e-06, device='cuda:0')\n",
      "Epoch 975, LR: 0.0010, Train Loss: 2.1649, Train Accuracy: 22.10%, Temperatures:(0.00, 0.28)\n",
      "Old & New Losses 2157.1595668792725 2164.551258087158 Probab: tensor(2.7064e-12, device='cuda:0')\n",
      "Epoch 976, LR: 0.0010, Train Loss: 2.1656, Train Accuracy: 21.40%, Temperatures:(0.00, 0.27)\n",
      "Old & New Losses 2158.534526824951 2162.231683731079 Probab: tensor(1.4312e-06, device='cuda:0')\n",
      "Epoch 977, LR: 0.0010, Train Loss: 2.1589, Train Accuracy: 22.40%, Temperatures:(0.00, 0.27)\n",
      "Old & New Losses 2157.761573791504 2161.2119674682617 Probab: tensor(3.0952e-06, device='cuda:0')\n",
      "Epoch 978, LR: 0.0010, Train Loss: 2.1655, Train Accuracy: 21.90%, Temperatures:(0.00, 0.27)\n",
      "Old & New Losses 2158.6921215057373 2157.698392868042 Probab: tensor(40.0622, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 979, LR: 0.0010, Train Loss: 2.1621, Train Accuracy: 23.00%, Temperatures:(0.00, 0.27)\n",
      "Old & New Losses 2160.3002548217773 2157.1643352508545 Probab: tensor(128486.2891, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 980, LR: 0.0010, Train Loss: 2.1635, Train Accuracy: 22.20%, Temperatures:(0.00, 0.26)\n",
      "Old & New Losses 2160.316228866577 2166.795253753662 Probab: tensor(2.1784e-11, device='cuda:0')\n",
      "Epoch 981, LR: 0.0010, Train Loss: 2.1664, Train Accuracy: 21.60%, Temperatures:(0.00, 0.26)\n",
      "Old & New Losses 2160.0377559661865 2165.555477142334 Probab: tensor(6.7350e-10, device='cuda:0')\n",
      "Epoch 982, LR: 0.0010, Train Loss: 2.1637, Train Accuracy: 19.90%, Temperatures:(0.00, 0.26)\n",
      "Old & New Losses 2161.450147628784 2167.1972274780273 Probab: tensor(2.2418e-10, device='cuda:0')\n",
      "Epoch 983, LR: 0.0010, Train Loss: 2.1655, Train Accuracy: 21.50%, Temperatures:(0.00, 0.26)\n",
      "Old & New Losses 2161.3004207611084 2158.520460128784 Probab: tensor(51847.5508, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 984, LR: 0.0010, Train Loss: 2.1617, Train Accuracy: 21.30%, Temperatures:(0.00, 0.25)\n",
      "Old & New Losses 2162.7118587493896 2169.7564125061035 Probab: tensor(8.5501e-13, device='cuda:0')\n",
      "Epoch 985, LR: 0.0010, Train Loss: 2.1648, Train Accuracy: 21.50%, Temperatures:(0.00, 0.25)\n",
      "Old & New Losses 2165.8365726470947 2185.572862625122 Probab: tensor(7.0507e-35, device='cuda:0')\n",
      "Epoch 986, LR: 0.0010, Train Loss: 2.1614, Train Accuracy: 21.60%, Temperatures:(0.00, 0.25)\n",
      "Old & New Losses 2162.1522903442383 2167.875051498413 Probab: tensor(9.9376e-11, device='cuda:0')\n",
      "Epoch 987, LR: 0.0010, Train Loss: 2.1615, Train Accuracy: 22.80%, Temperatures:(0.00, 0.25)\n",
      "Old & New Losses 2161.9768142700195 2169.757843017578 Probab: tensor(1.8293e-14, device='cuda:0')\n",
      "Epoch 988, LR: 0.0010, Train Loss: 2.1598, Train Accuracy: 21.00%, Temperatures:(0.00, 0.24)\n",
      "Old & New Losses 2156.2860012054443 2166.2356853485107 Probab: tensor(1.8031e-18, device='cuda:0')\n",
      "Epoch 989, LR: 0.0010, Train Loss: 2.1625, Train Accuracy: 22.00%, Temperatures:(0.00, 0.24)\n",
      "Old & New Losses 2169.1815853118896 2164.501190185547 Probab: tensor(2.6990e+08, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 990, LR: 0.0010, Train Loss: 2.1617, Train Accuracy: 21.20%, Temperatures:(0.00, 0.24)\n",
      "Old & New Losses 2160.8192920684814 2155.616044998169 Probab: tensor(2.9359e+09, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 991, LR: 0.0010, Train Loss: 2.1646, Train Accuracy: 22.10%, Temperatures:(0.00, 0.24)\n",
      "Old & New Losses 2164.011240005493 2162.976026535034 Probab: tensor(79.9250, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 992, LR: 0.0010, Train Loss: 2.1588, Train Accuracy: 23.50%, Temperatures:(0.00, 0.23)\n",
      "Old & New Losses 2160.8269214630127 2166.4645671844482 Probab: tensor(3.4163e-11, device='cuda:0')\n",
      "Epoch 993, LR: 0.0010, Train Loss: 2.1684, Train Accuracy: 19.50%, Temperatures:(0.00, 0.23)\n",
      "Old & New Losses 2165.131092071533 2165.381669998169 Probab: tensor(0.3389, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 994, LR: 0.0010, Train Loss: 2.1635, Train Accuracy: 20.50%, Temperatures:(0.00, 0.23)\n",
      "Old & New Losses 2170.936346054077 2164.473295211792 Probab: tensor(1.7476e+12, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 995, LR: 0.0010, Train Loss: 2.1659, Train Accuracy: 19.90%, Temperatures:(0.00, 0.23)\n",
      "Old & New Losses 2168.3309078216553 2161.7863178253174 Probab: tensor(3.3276e+12, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 996, LR: 0.0010, Train Loss: 2.1704, Train Accuracy: 20.70%, Temperatures:(0.00, 0.22)\n",
      "Old & New Losses 2168.6015129089355 2167.7258014678955 Probab: tensor(49.2575, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 997, LR: 0.0010, Train Loss: 2.1624, Train Accuracy: 19.80%, Temperatures:(0.00, 0.22)\n",
      "Old & New Losses 2166.829824447632 2171.415090560913 Probab: tensor(1.1185e-09, device='cuda:0')\n",
      "Epoch 998, LR: 0.0010, Train Loss: 2.1713, Train Accuracy: 19.90%, Temperatures:(0.00, 0.22)\n",
      "Old & New Losses 2168.917655944824 2198.181390762329 Probab: tensor(0., device='cuda:0')\n",
      "Epoch 999, LR: 0.0010, Train Loss: 2.1679, Train Accuracy: 20.70%, Temperatures:(0.00, 0.22)\n",
      "Old & New Losses 2170.285940170288 2166.307210922241 Probab: tensor(84139592., device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 1000, LR: 0.0010, Train Loss: 2.1714, Train Accuracy: 20.30%, Temperatures:(0.00, 0.22)\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "lr = params_RRAM[\"ext_lr\"] / 25  # Initial learning rate\n",
    "num_epochs = params_RRAM[\"epochs\"]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch == 1:\n",
    "        lr *= 5\n",
    "    elif epoch == 2:\n",
    "        lr *= 5\n",
    "\n",
    "    model_RRAM.train()\n",
    "    outputs = model_RRAM(train_in)\n",
    "    loss = criterion(outputs, train_lab)\n",
    "    loss.backward()\n",
    "    model_RRAM.backprop(lr)\n",
    "    model_RRAM.anneal(train_in, train_lab,0.99, 0.99)\n",
    "\n",
    "    _, train_preds = torch.max(outputs, dim=1)\n",
    "    train_accuracy = (train_preds == train_lab).float().mean().item() * 100\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, LR: {lr:.4f}, Train Loss: {loss.item():.4f}, \"\n",
    "          f\"Train Accuracy: {train_accuracy:.2f}%, Temperatures:({model_RRAM.temperature_1:.2f}, {model_RRAM.temperature_2:.2f})\")\n",
    "\n",
    "    if epoch % 50 == 0 and epoch != 0:\n",
    "        lr /= 2\n",
    "\n",
    "model_RRAM = SoftBinaryRecurrentForwardNetwork(**model_params).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282eeb54-9acf-4ce6-b5f0-13e5c60f8614",
   "metadata": {
    "id": "282eeb54-9acf-4ce6-b5f0-13e5c60f8614"
   },
   "source": [
    "### Loading Past Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0de620a-e0e4-49cf-863d-721e2680dd58",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0de620a-e0e4-49cf-863d-721e2680dd58",
    "outputId": "3fc15a9d-cc8e-4351-ff9c-d40906fc055f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Loss: 1.528846\n",
      "Parameters for Best Loss Model: {'scaling': 2, 'G_ON': 6e-05, 'G_OFF': 2.88e-06, 'V_INV': 1.65, 'R_INV': 3000.0, 'V_1': 0.1, 'V_0': -0.1, 'zeta': 10.0, 'initial_factor': 0.01, 'crossbar': (64, 64), 'input_size': 676, 'encoding_size': 4, 'output_size': 10, 'data_in': 52, 'bin_active': True, 'monitor_volts': False, 'monitor_grads': False, 'monitor_latents': False, 'dropout': 0.1, 'int_lr': 0.01, 'int_norm': True, 'ext_lr': 1000, 'epochs': 40}\n",
      "Best Validation Accuracy: 70.53\n",
      "Parameters for Best Accuracy Model: {'scaling': 5, 'G_ON': 6e-05, 'G_OFF': 2.88e-06, 'V_INV': 0.6, 'R_INV': 1000.0, 'V_1': 0.1, 'V_0': -0.1, 'zeta': 10.0, 'initial_factor': 0.01, 'crossbar': (64, 64), 'input_size': 676, 'encoding_size': 4, 'output_size': 10, 'data_in': 52, 'bin_active': True, 'monitor_volts': False, 'monitor_grads': False, 'monitor_latents': False, 'dropout': 0.1, 'int_lr': 0.01, 'int_norm': True, 'ext_lr': 500, 'epochs': 1000, 'temperature_1': 1, 'temperature_2': 5000, 'monitor_annealing': True}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load best validation loss model\n",
    "    with open(\"Best_Val_Loss.txt\", 'r') as f:\n",
    "        global_best_val_loss = float(f.read())\n",
    "    with open(\"Best_Params_Loss.txt\", 'r') as f:\n",
    "        params_best_loss = ast.literal_eval(f.read())\n",
    "\n",
    "    model_best_loss = SoftBinaryRecurrentForwardNetwork(**model_params).to(device)\n",
    "\n",
    "    print(\"\\n Best Validation Loss:\", global_best_val_loss)\n",
    "    print(\"\\n Parameters for Best Loss Model:\", params_best_loss)\n",
    "\n",
    "    checkpoint_loss = torch.load(\"Best_model_loss.pth\")\n",
    "    model_best_loss.load_state_dict(checkpoint_loss)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error loading best loss model:\", e)\n",
    "    global_best_val_loss = float('inf')\n",
    "    print(\"No Saved Model for Best Loss\")\n",
    "\n",
    "try:\n",
    "    # Load best validation accuracy model\n",
    "    with open(\"Best_Val_Acc.txt\", 'r') as f:\n",
    "        global_best_val_acc = float(f.read())\n",
    "    with open(\"Best_Params_Acc.txt\", 'r') as f:\n",
    "        params_best_acc = ast.literal_eval(f.read())\n",
    "\n",
    "    model_best_acc = SoftBinaryRecurrentForwardNetwork(**model_params).to(device)\n",
    "\n",
    "    print(\"\\n Best Validation Accuracy:\", global_best_val_acc)\n",
    "    print(\"\\n Parameters for Best Accuracy Model:\", params_best_acc)\n",
    "\n",
    "    checkpoint_acc = torch.load(\"Best_model_acc.pth\")\n",
    "    model_best_acc.load_state_dict(checkpoint_acc)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error loading best accuracy model:\", e)\n",
    "    global_best_val_acc = 0.0\n",
    "    print(\"No Saved Model for Best Accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "786757d6-3ef8-4327-900a-94924d5a1b57",
   "metadata": {
    "id": "786757d6-3ef8-4327-900a-94924d5a1b57"
   },
   "outputs": [],
   "source": [
    "history_RRAM = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_accuracy\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_accuracy\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc53685-ceb0-4b1c-a03f-04ca24416013",
   "metadata": {
    "id": "8dc53685-ceb0-4b1c-a03f-04ca24416013"
   },
   "source": [
    "### Complete Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ed1842-f618-4562-8e82-c5355ca39f90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9ed1842-f618-4562-8e82-c5355ca39f90",
    "outputId": "aba2264c-4110-4c7e-b283-98b16068904d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, LR: 20.0000, Train Loss: 2.0710, Train Acc: 31.11%, Val Loss: 1.9036, Val Acc: 44.44%, Temperatures: (1.00, 5000.00)\n",
      "Old & New Losses 19036.20719909668 23028.228282928467 Probab: tensor(0.4500, device='cuda:0')\n",
      "Annealed weights accepted\n"
     ]
    }
   ],
   "source": [
    "lr = params_RRAM[\"ext_lr\"] / 25\n",
    "num_epochs = params_RRAM[\"epochs\"]\n",
    "patience, wait, wait_2, wait_3 = 500, 0, 75, 15\n",
    "wait_lr = 0\n",
    "cur_best_val_loss, cur_best_val_acc = float('inf'), 0\n",
    "temp_boosted = False\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch == 0:\n",
    "        lr = lr\n",
    "    elif epoch <= 2:\n",
    "        lr *= 5\n",
    "\n",
    "    model_RRAM.train().to(device)\n",
    "    train_loss, train_correct, total_samples = 0, 0, 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model_RRAM(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        model_RRAM.backprop(lr)\n",
    "\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        train_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "        total_samples += inputs.size(0)\n",
    "\n",
    "        # model_RRAM.anneal(inputs, labels, 0.99, 0.999)\n",
    "\n",
    "    train_loss /= total_samples\n",
    "    train_accuracy = 100 * train_correct / total_samples\n",
    "\n",
    "    model_RRAM.eval()\n",
    "    val_loss, val_correct, total_test_samples = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model_RRAM(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            total_test_samples += inputs.size(0)\n",
    "\n",
    "    val_loss /= total_test_samples\n",
    "    val_accuracy = 100 * val_correct / total_test_samples\n",
    "\n",
    "    history_RRAM[\"train_loss\"].append(train_loss)\n",
    "    history_RRAM[\"train_accuracy\"].append(train_accuracy)\n",
    "    history_RRAM[\"val_loss\"].append(val_loss)\n",
    "    history_RRAM[\"val_accuracy\"].append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, LR: {lr:.4f}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%, Temperatures: ({model_RRAM.temperature_1:.2f}, {model_RRAM.temperature_2:.2f})\")\n",
    "\n",
    "    if val_loss < global_best_val_loss:\n",
    "        global_best_val_loss = val_loss\n",
    "        torch.save(model_RRAM.state_dict(), \"Best_model_loss.pth\")\n",
    "        with open(\"Best_Val_Loss.txt\", \"w\") as f:\n",
    "            f.write(f\"{val_loss:.6f}\")\n",
    "        with open(\"Best_Params_Loss.txt\", \"w\") as f:\n",
    "            f.write(f\"{params_RRAM}\")\n",
    "        print(f\"Model saved with best validation loss: {val_loss:.6f}\")\n",
    "\n",
    "    if val_accuracy > global_best_val_acc:\n",
    "        global_best_val_acc = val_accuracy\n",
    "        torch.save(model_RRAM.state_dict(), \"Best_model_acc.pth\")\n",
    "        with open(\"Best_Val_Acc.txt\", \"w\") as f:\n",
    "            f.write(f\"{val_accuracy:.6f}\")\n",
    "        with open(\"Best_Params_Acc.txt\", \"w\") as f:\n",
    "            f.write(f\"{params_RRAM}\")\n",
    "        print(f\"Model saved with best validation accuracy: {val_accuracy:.6f}\")\n",
    "\n",
    "    # Anneal Using Validation Set, Once Per Epoch\n",
    "    model_RRAM.anneal(inputs, labels, 0.9, 0.9)\n",
    "\n",
    "    if wait_lr >= wait_2 and epoch > 3:\n",
    "        lr /= 5\n",
    "        wait_lr = 0\n",
    "        print(f\"No improvement for {wait_2} epochs. Reducing LR to {lr:.4f}\")\n",
    "\n",
    "    if wait_lr >= wait_3 and not temp_boosted:\n",
    "        model_RRAM.temperature_2 *= 10\n",
    "        temp_boosted = True\n",
    "        print(f\"Training plateau detected. Temporarily increasing temperature_2 to {model_RRAM.temperature_2:.2f}\")\n",
    "\n",
    "    if val_loss < cur_best_val_loss - 0.001 or val_accuracy > cur_best_val_acc + 0.1:\n",
    "        if temp_boosted:\n",
    "            model_RRAM.temperature_2 /= 10\n",
    "            temp_boosted = False\n",
    "            print(f\"Training improved. Restoring temperature_2 to {model_RRAM.temperature_2:.2f}\")\n",
    "\n",
    "        cur_best_val_loss = min(cur_best_val_loss, val_loss)\n",
    "        cur_best_val_acc = max(cur_best_val_acc, val_accuracy)\n",
    "        wait = 0\n",
    "        wait_lr = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "        wait_lr += 1\n",
    "\n",
    "    if wait >= patience and epoch > 6:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070a2ef8-94c5-49ca-bd35-3ed75ae2327b",
   "metadata": {
    "id": "070a2ef8-94c5-49ca-bd35-3ed75ae2327b"
   },
   "outputs": [],
   "source": [
    "plot_history(history_RRAM, num_epochs, \"RRAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93e55bc-1bae-4483-a3cf-01cde188dc69",
   "metadata": {
    "id": "c93e55bc-1bae-4483-a3cf-01cde188dc69"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af52e53e-15ed-441f-8412-d94c5d6a0bdb",
   "metadata": {
    "id": "af52e53e-15ed-441f-8412-d94c5d6a0bdb"
   },
   "source": [
    "### Current Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51271c7c-25b1-4702-9db1-9ae4b805843c",
   "metadata": {
    "id": "51271c7c-25b1-4702-9db1-9ae4b805843c"
   },
   "outputs": [],
   "source": [
    "print(0 + (model_RRAM.w > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d477c9e-5678-4d07-a5ac-48f01480c916",
   "metadata": {
    "id": "9d477c9e-5678-4d07-a5ac-48f01480c916"
   },
   "outputs": [],
   "source": [
    "cm = test(model_RRAM, val_inputs, val_labels, class_names = [\"A\", \"T\", \"V\", \"X\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7941df-4678-4955-8a0f-f38b43f6b197",
   "metadata": {
    "id": "ed7941df-4678-4955-8a0f-f38b43f6b197"
   },
   "source": [
    "## Best Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867574d7-3627-49ff-a430-1d658fa4d655",
   "metadata": {
    "id": "867574d7-3627-49ff-a430-1d658fa4d655"
   },
   "outputs": [],
   "source": [
    "0+1*(model_best.w>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ca56c6-7023-416e-9675-93f1a3c8cc78",
   "metadata": {
    "id": "32ca56c6-7023-416e-9675-93f1a3c8cc78"
   },
   "outputs": [],
   "source": [
    "cm = test(model_best, val_inputs, val_labels, class_names = [\"A\", \"T\", \"V\", \"X\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad288f0b-0ec9-415d-96cc-a4ecec7aa5ce",
   "metadata": {
    "id": "ad288f0b-0ec9-415d-96cc-a4ecec7aa5ce"
   },
   "source": [
    "## PWL Generation\n",
    "\n",
    "Let's assume that we will program the two crossbars with seperate PWLs. That is, during programming, we will cut the Inverting Amplifier stages with a pass transistor and connect the programming lines with a pass transistor. First array has 16 Top PWLs and 8 Bottom PWLs. Second array has 8 Top PWLs and 4 Bottom PWLs. And then once the programming switch is toggled to inference mode, only the 16 Top PWLs are to be changed. Let's also generate a PWL for that too.\n",
    "\n",
    "In the code below, we will first maintain tuples for each PWL that holds what the voltage should be. And then we will write a function that will take there and space pulses of the given voltage that are 100us apart from other and have an ON duration of 100us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d5cebb-fc99-4a00-87a1-e183838c9a64",
   "metadata": {
    "id": "58d5cebb-fc99-4a00-87a1-e183838c9a64"
   },
   "outputs": [],
   "source": [
    "WL_FC1 = [list() for i in range(16)]\n",
    "BL_FC1 = [list() for i in range(8)]\n",
    "WL_FC2 = [list() for i in range(8)]\n",
    "BL_FC2 = [list() for i in range(4)]\n",
    "Mode = []\n",
    "Mode_B = []\n",
    "\n",
    "V_WRITE = 1.5\n",
    "V_READ = 0.1\n",
    "V_mode = 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c647afda-e1a7-47a7-b634-212ec5527be2",
   "metadata": {
    "id": "c647afda-e1a7-47a7-b634-212ec5527be2"
   },
   "source": [
    "#### Fully Connected Weights 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03334172-89f6-4d73-9461-6ac922bdb6ea",
   "metadata": {
    "id": "03334172-89f6-4d73-9461-6ac922bdb6ea"
   },
   "outputs": [],
   "source": [
    "target = (model_RRAM_best.w1>0).int()\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49287afe-8437-40aa-8825-93f2cffe3a66",
   "metadata": {
    "id": "49287afe-8437-40aa-8825-93f2cffe3a66"
   },
   "outputs": [],
   "source": [
    "for ind_i, i in enumerate(target):\n",
    "    for ind_j, j in enumerate(i):\n",
    "        if j==1: WL_FC1[ind_j].append(V_WRITE)\n",
    "        else: WL_FC1[ind_j].append(V_WRITE/3)\n",
    "    for ind_k in range(len(target)):\n",
    "        if ind_k==ind_i: BL_FC1[ind_i].append(0)\n",
    "        else: BL_FC1[ind_k].append(2*V_WRITE/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be77131c-005d-4f06-8a70-f40d267eb67e",
   "metadata": {
    "id": "be77131c-005d-4f06-8a70-f40d267eb67e"
   },
   "source": [
    "#### Fully Connected Weights 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adc2b24-ffb0-4470-8eae-3830221cb33d",
   "metadata": {
    "id": "9adc2b24-ffb0-4470-8eae-3830221cb33d"
   },
   "outputs": [],
   "source": [
    "target = (model_RRAM_best.w2>0).int()\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7ca037-0e2f-49a3-98b3-b2e0b93f8b79",
   "metadata": {
    "id": "3f7ca037-0e2f-49a3-98b3-b2e0b93f8b79"
   },
   "outputs": [],
   "source": [
    "for ind_i, i in enumerate(target):\n",
    "    for ind_j, j in enumerate(i):\n",
    "        if j==1: WL_FC2[ind_j].append(V_WRITE)\n",
    "        else: WL_FC2[ind_j].append(V_WRITE/3)\n",
    "    for ind_k in range(len(target)):\n",
    "        if ind_k==ind_i: BL_FC2[ind_i].append(0)\n",
    "        else: BL_FC2[ind_k].append(2*V_WRITE/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a602368f-07a2-4beb-90e3-d782ca717fea",
   "metadata": {
    "id": "a602368f-07a2-4beb-90e3-d782ca717fea"
   },
   "source": [
    "#### Filling Out Programming Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aa24d2-0d95-4ff4-8dfd-5c0800d9de11",
   "metadata": {
    "id": "c4aa24d2-0d95-4ff4-8dfd-5c0800d9de11"
   },
   "outputs": [],
   "source": [
    "WL_FC1 = [i + [0,0] for i in WL_FC1]\n",
    "BL_FC1 = [i + [0,0] for i in BL_FC1]\n",
    "while(len(WL_FC2[0]) < len(WL_FC1[0])):\n",
    "    WL_FC2 = [i + [0,] for i in WL_FC2]\n",
    "    BL_FC2 = [i + [0,] for i in BL_FC2]\n",
    "Mode.extend([V_mode]*(len(WL_FC1[0])-1) + [-V_mode])\n",
    "Mode_B.extend([-V_mode]*(len(WL_FC1[0])-1) + [V_mode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f953198a-a107-428c-9624-3c712086e922",
   "metadata": {
    "id": "f953198a-a107-428c-9624-3c712086e922"
   },
   "outputs": [],
   "source": [
    "print(WL_FC1[0])\n",
    "print(BL_FC1[0])\n",
    "print(WL_FC2[0])\n",
    "print(BL_FC2[0])\n",
    "print(Mode)\n",
    "print(Mode_B)\n",
    "print(len(WL_FC1[0]), len(BL_FC1[0]), len(WL_FC2[0]), len(BL_FC2[0]), len(Mode), len(Mode_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92339f84-e8d6-4280-92af-e5215aa129a0",
   "metadata": {
    "id": "92339f84-e8d6-4280-92af-e5215aa129a0"
   },
   "source": [
    "### Inference: Loading the Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97dacd2-42bb-49f0-8ca9-e0f02ca4cc58",
   "metadata": {
    "id": "b97dacd2-42bb-49f0-8ca9-e0f02ca4cc58"
   },
   "outputs": [],
   "source": [
    "val_inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2336c3ab-2181-4ee7-b1c9-0a97a3700248",
   "metadata": {
    "id": "2336c3ab-2181-4ee7-b1c9-0a97a3700248"
   },
   "outputs": [],
   "source": [
    "V_1 = 0.1\n",
    "V_0 = -0.1\n",
    "include_testing = True\n",
    "include_every = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1cfba2-3243-4062-9ef6-4cd309622c73",
   "metadata": {
    "id": "6a1cfba2-3243-4062-9ef6-4cd309622c73"
   },
   "outputs": [],
   "source": [
    "if include_testing:\n",
    "    for i in val_inputs[::include_every]:\n",
    "        i = i.flatten()\n",
    "        for ind, j in enumerate(i):\n",
    "            WL_FC1[ind].append(V_1 if j==1 else V_0)\n",
    "        BL_FC1 = [i + [0,] for i in BL_FC1]\n",
    "        WL_FC2 = [i + [0,] for i in WL_FC2]\n",
    "        BL_FC2 = [i + [0,] for i in BL_FC2]\n",
    "        Mode = Mode + [-V_mode,]\n",
    "        Mode_B = Mode_B + [V_mode,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dd97b8-fb1b-4634-9ae4-3a6fe05d0a2c",
   "metadata": {
    "id": "b9dd97b8-fb1b-4634-9ae4-3a6fe05d0a2c"
   },
   "source": [
    "### PWL Convertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98af5ff-8f34-4472-89fc-e308d98d4072",
   "metadata": {
    "id": "c98af5ff-8f34-4472-89fc-e308d98d4072"
   },
   "outputs": [],
   "source": [
    "def pwl(l):\n",
    "    t = 0\n",
    "    res = \"pwl(time, 0us, 0V\"\n",
    "    for i in l:\n",
    "        res += f\", {t+5}us, {i:.2f}V, {t+100}us, {i:.2f}V, {t+105}us, 0V, {t+200}us, 0V\"\n",
    "        t+=200\n",
    "    res += \")\"\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54850dbc-1317-44a2-9093-c5a4e048094c",
   "metadata": {
    "id": "54850dbc-1317-44a2-9093-c5a4e048094c"
   },
   "outputs": [],
   "source": [
    "pwl_data = []\n",
    "\n",
    "for ind, i in enumerate(WL_FC1):\n",
    "    pwl_data.append({\"Signal\": f\"WL_FC1_{ind}\", \"Index\": ind, \"PWL\": pwl(i)})\n",
    "for ind, i in enumerate(BL_FC1):\n",
    "    pwl_data.append({\"Signal\": f\"BL_FC1_{ind}\", \"Index\": ind, \"PWL\": pwl(i)})\n",
    "for ind, i in enumerate(WL_FC2):\n",
    "    pwl_data.append({\"Signal\": f\"WL_FC2_{ind}\", \"Index\": ind, \"PWL\": pwl(i)})\n",
    "for ind, i in enumerate(BL_FC2):\n",
    "    pwl_data.append({\"Signal\": f\"BL_FC2_{ind}\", \"Index\": ind, \"PWL\": pwl(i)})\n",
    "pwl_data.append({\"Signal\": \"Mode\", \"Index\": \"\", \"PWL\": pwl(Mode)})\n",
    "pwl_data.append({\"Signal\": \"Mode_b\", \"Index\": \"\", \"PWL\": pwl(Mode_B)})\n",
    "\n",
    "pwl_data = pd.DataFrame(pwl_data)\n",
    "pwl_data.to_csv(\"pwl_data.csv\", index=False)\n",
    "pwl_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e13866-8cd8-4b8e-a8d0-a5c69e951174",
   "metadata": {
    "id": "03e13866-8cd8-4b8e-a8d0-a5c69e951174"
   },
   "source": [
    "#### Testing Accuracy on 160 Images\n",
    "ADS isn't allowing PWLs longer than 160 Images, so let's check software accuracy for the same too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4e4e97-5ecd-4c41-9035-4a51015d684e",
   "metadata": {
    "id": "fd4e4e97-5ecd-4c41-9035-4a51015d684e"
   },
   "outputs": [],
   "source": [
    "test(model_RRAM_best, val_inputs[::4], val_labels[::4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf44b69-14ac-4fdd-a2b5-b9c2cc361e7f",
   "metadata": {
    "id": "7cf44b69-14ac-4fdd-a2b5-b9c2cc361e7f"
   },
   "outputs": [],
   "source": [
    "test(model_RRAM_best, train_inputs, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1079074f-8781-43af-b52e-fb3581ef8931",
   "metadata": {
    "id": "1079074f-8781-43af-b52e-fb3581ef8931"
   },
   "source": [
    "## Simulation Data from ADS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d523a2-f99f-46bb-93e0-fead2768da75",
   "metadata": {
    "id": "e0d523a2-f99f-46bb-93e0-fead2768da75"
   },
   "outputs": [],
   "source": [
    "simu = pd.read_csv(\"Testing_160_Images.csv\")\n",
    "simu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd96e845-0c19-470b-8910-6b45df78022c",
   "metadata": {
    "id": "fd96e845-0c19-470b-8910-6b45df78022c"
   },
   "outputs": [],
   "source": [
    "def remove_units(value):\n",
    "    return float(value.replace('E', 'e').split('V')[0].replace('sec', ''))\n",
    "\n",
    "simu['time'] = simu['time'].apply(remove_units)\n",
    "for col in ['A', 'X', 'V', 'T']:\n",
    "    simu[col] = simu[col].apply(remove_units)\n",
    "simu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223576d6-266c-49bd-aaca-ff3444617f09",
   "metadata": {
    "id": "223576d6-266c-49bd-aaca-ff3444617f09"
   },
   "source": [
    "We just need one sample every 0.1ms samples of these starting from 2.050ms to 33.850ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13d6e19-daa6-481f-8ad2-11a357dc20d3",
   "metadata": {
    "id": "a13d6e19-daa6-481f-8ad2-11a357dc20d3"
   },
   "outputs": [],
   "source": [
    "t_stamps = np.arange(2.05e-3, 33.9e-3, 0.2e-3)\n",
    "t_stamps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5548dd9-e376-4bd1-b9d2-484f4358f700",
   "metadata": {
    "id": "f5548dd9-e376-4bd1-b9d2-484f4358f700"
   },
   "outputs": [],
   "source": [
    "sampled = []\n",
    "window = 0.02e-3\n",
    "\n",
    "for t in t_stamps:\n",
    "    filtered = simu[(simu['time'] >= t - window) & (simu['time'] <= t + window)]\n",
    "\n",
    "    avg_A = filtered['A'].mean()\n",
    "    avg_X = filtered['X'].mean()\n",
    "    avg_V = filtered['V'].mean()\n",
    "    avg_T = filtered['T'].mean()\n",
    "\n",
    "    sampled.append({\n",
    "        'Image Index': t,\n",
    "        'A': avg_A,\n",
    "        'X': avg_X,\n",
    "        'V': avg_V,\n",
    "        'T': avg_T\n",
    "    })\n",
    "\n",
    "sampled = pd.DataFrame(sampled)\n",
    "sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096828ea-b6be-4839-8b7d-4e0f5a515a7e",
   "metadata": {
    "id": "096828ea-b6be-4839-8b7d-4e0f5a515a7e"
   },
   "outputs": [],
   "source": [
    "def get_max_column(row):\n",
    "    return row[['A', 'X', 'V', 'T']].idxmax()\n",
    "sampled['Predicted Class'] = sampled.apply(get_max_column, axis=1)\n",
    "sampled.to_csv(\"Sampled_Results.csv\", index=False)\n",
    "sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1143c06-5631-4c79-9965-0cb937929706",
   "metadata": {
    "id": "d1143c06-5631-4c79-9965-0cb937929706"
   },
   "outputs": [],
   "source": [
    "ground_truth = ['A']*40 + ['X']*40 + ['V']*40 + ['T']*40\n",
    "correct_predictions = sampled['Predicted Class'] == ground_truth\n",
    "accuracy = correct_predictions.sum() / len(ground_truth)\n",
    "print(accuracy*100,end=\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918b4552-b0d3-4768-bfe7-9b874959d42c",
   "metadata": {
    "id": "918b4552-b0d3-4768-bfe7-9b874959d42c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 3.5))\n",
    "\n",
    "plt.scatter(sampled.index, sampled['A'], color='red', label='A_pred', s=30, marker='o')  # Red dots for A\n",
    "plt.scatter(sampled.index, sampled['X'], color='blue', label='X_pred', s=30, marker='o')  # Blue dots for X\n",
    "plt.scatter(sampled.index, sampled['T'], color='green', label='T_pred', s=30, marker='o')  # Green dots for T\n",
    "plt.scatter(sampled.index, sampled['V'], color='orange', label='V_pred', s=30, marker='o')  # Orange dots for V\n",
    "\n",
    "plt.xlabel('Image Index')\n",
    "plt.ylabel('Predicted Voltages (V)')\n",
    "plt.legend()\n",
    "\n",
    "plt.axvline(x=40, color='gray', linestyle='--', linewidth=2)\n",
    "plt.axvline(x=80, color='gray', linestyle='--', linewidth=2)\n",
    "plt.axvline(x=120, color='gray', linestyle='--', linewidth=2)\n",
    "\n",
    "plt.text(20, plt.ylim()[1]*(-0.8), 'A', fontsize=15, color='black', ha='center')\n",
    "plt.text(60, plt.ylim()[1]*0.8, 'X', fontsize=15, color='black', ha='center')\n",
    "plt.text(100, plt.ylim()[1]*0.8, 'V', fontsize=15, color='black', ha='center')\n",
    "plt.text(140, plt.ylim()[1]*(-0.8), 'T', fontsize=15, color='black', ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f31ef-ad69-472f-9217-bdde6f8ab1e8",
   "metadata": {
    "id": "e86f31ef-ad69-472f-9217-bdde6f8ab1e8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
