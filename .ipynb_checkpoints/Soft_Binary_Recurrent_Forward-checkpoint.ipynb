{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "141370fc-54a4-4a6b-afcf-a7677dc6dc87",
   "metadata": {
    "id": "141370fc-54a4-4a6b-afcf-a7677dc6dc87"
   },
   "source": [
    "# Soft Binary Neural Network with Recurrent Crossbar Recycling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508058d8-e23a-4c29-aad7-c2b233d621c9",
   "metadata": {
    "id": "508058d8-e23a-4c29-aad7-c2b233d621c9"
   },
   "source": [
    "## Imports and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a70e539-1dc9-4e36-9c9f-18fbdaeede1f",
   "metadata": {
    "id": "9a70e539-1dc9-4e36-9c9f-18fbdaeede1f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d406d54c-db27-4536-a8c1-f46437f6fb71",
   "metadata": {
    "id": "d406d54c-db27-4536-a8c1-f46437f6fb71"
   },
   "outputs": [],
   "source": [
    "def plot_history(history, num_epochs, element):\n",
    "    epochs = range(len(history[list(history.keys())[0]]))\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    ax1.plot(epochs, history[\"train_loss\"], label=\"Train Loss\", color=\"blue\")\n",
    "    ax1.plot(epochs, history[\"val_loss\"], label=\"Validation Loss\", color=\"red\")\n",
    "    ax1.set_xlabel(\"Epochs\", fontsize=14)\n",
    "    ax1.set_ylabel(\"Loss\", fontsize=14, color=\"blue\")\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=\"blue\")\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(epochs, history[\"train_accuracy\"], label=\"Train Accuracy\", color=\"green\")\n",
    "    ax2.plot(epochs, history[\"val_accuracy\"], label=\"Validation Accuracy\", color=\"orange\")\n",
    "    ax2.set_ylabel(\"Accuracy (%)\", fontsize=14, color=\"green\")\n",
    "    ax2.tick_params(axis=\"y\", labelcolor=\"green\")\n",
    "    ax2.legend(loc=\"upper right\")\n",
    "\n",
    "    plt.title(f\"Training and Validation Metrics for {element}\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c658023-75df-4754-a617-a8ba6d08d068",
   "metadata": {
    "id": "7c658023-75df-4754-a617-a8ba6d08d068"
   },
   "outputs": [],
   "source": [
    "def test(model, test_loader, class_names=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = (total_correct / total_samples) * 100\n",
    "\n",
    "    print(f\"Validation Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5e8ced-6bb5-445a-9b04-bc268eec917e",
   "metadata": {
    "id": "be5e8ced-6bb5-445a-9b04-bc268eec917e"
   },
   "source": [
    "### MNIST Handwritten Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3bbad17-7067-4f06-8755-012646ca9567",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3bbad17-7067-4f06-8755-012646ca9567",
    "outputId": "f1798d9d-41cf-4c7c-810b-01a97006c6d8"
   },
   "outputs": [],
   "source": [
    "class BinarizeAndAddNoiseTransform:\n",
    "    def __init__(self, threshold, noise_std):\n",
    "        self.threshold = threshold\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = transforms.ToTensor()(img).to(device)\n",
    "        img = (img > self.threshold).float()\n",
    "        img = img[:,1:-1, 1:-1]\n",
    "        noise = torch.randn(img.size(), device=device) * self.noise_std\n",
    "        noisy_img = img + noise\n",
    "        return noisy_img\n",
    "\n",
    "binary_noise_transform = transforms.Compose([\n",
    "    BinarizeAndAddNoiseTransform(threshold=0.48, noise_std=0.05)\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=binary_noise_transform)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1000, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=binary_noise_transform)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=10000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc40653e-ebac-4013-9ecb-27dfe1370edd",
   "metadata": {
    "id": "bc40653e-ebac-4013-9ecb-27dfe1370edd"
   },
   "outputs": [],
   "source": [
    "# Get a subset of the dataset\n",
    "train_in, train_lab = next(iter(train_loader))\n",
    "val_in, val_lab = next(iter(test_loader))\n",
    "\n",
    "# Move data to the appropriate device\n",
    "train_in, train_lab = train_in.to(device), train_lab.to(device)\n",
    "val_in, val_lab = val_in.to(device), val_lab.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d76599e3-3d87-4d1d-bd5f-41ca0adab18f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "d76599e3-3d87-4d1d-bd5f-41ca0adab18f",
    "outputId": "1b72a9c5-d410-4fd7-d135-0495307455db"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1, 26, 26])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=10, figsize=(15, 4))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(train_in[i].cpu().squeeze(), cmap='gray')\n",
    "    ax.set_title(f\"Label: {train_lab[i].item()}\", fontsize=10)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "train_in.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b44def-df85-406b-87e8-fbc5b4f7fe7a",
   "metadata": {
    "id": "d1b44def-df85-406b-87e8-fbc5b4f7fe7a"
   },
   "source": [
    "## Custom Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "391dfb3d-1a2b-42d7-9ff7-e3f0e831d50a",
   "metadata": {
    "id": "391dfb3d-1a2b-42d7-9ff7-e3f0e831d50a"
   },
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def tensor_stats(tensor, name=\"Tensor\"):\n",
    "    tensor = tensor.to(device)\n",
    "    mean_magnitude = tensor.abs().mean().item()\n",
    "    print(f\"{name} - Mean Magnitude: {mean_magnitude:.2e}, Max: {tensor.max().item():.2e}, Min: {tensor.min().item():.2e}\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SoftBinaryRecurrentForwardNetwork(nn.Module):\n",
    "    def __init__(self, scaling, G_ON, G_OFF, V_INV, R_INV, V_1, V_0, zeta, initial_factor, crossbar=(64,64),\n",
    "                 input_size=676, encoding_size=4, output_size=10, data_in=52, bin_active=True,\n",
    "                 monitor_volts=False, monitor_grads=True, monitor_latents=False, dropout=0.01,\n",
    "                 int_lr=0.01, int_norm=True, temperature_1 = 500, temperature_2 = 10000,monitor_annealing=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.w = nn.Parameter(initial_factor * torch.empty(crossbar, device=device))\n",
    "        nn.init.xavier_uniform_(self.w)\n",
    "        self.w.data = (initial_factor*(self.w.data))\n",
    "\n",
    "        self.G_ON, self.G_OFF = torch.tensor(G_ON, device=device)*scaling, torch.tensor(G_OFF, device=device)*scaling\n",
    "        self.V_INV, self.R_INV = torch.tensor(V_INV, device=device), torch.tensor(R_INV, device=device)\n",
    "        self.V_1, self.V_0 = torch.tensor(V_1, device=device), torch.tensor(V_0, device=device)\n",
    "\n",
    "        self.crossbar_in, self.crossbar_out = crossbar\n",
    "        self.encoding, self.data_in, self.output_size = encoding_size, data_in, output_size\n",
    "        self.r_passes = input_size // data_in\n",
    "\n",
    "        self.first_bias = (crossbar[0] - data_in) % encoding_size\n",
    "        self.extra_final = crossbar[1] - self.encoding * self.r_passes - output_size\n",
    "        self.final_bias = (crossbar[0] - self.encoding * self.r_passes - self.extra_final) % (self.extra_final + self.encoding)\n",
    "\n",
    "        self.feed_repeats = (crossbar[0] - data_in)//encoding_size\n",
    "        self.final_repeats = (crossbar[0] - self.encoding * self.r_passes - self.extra_final)//(self.extra_final + self.encoding)\n",
    "\n",
    "        self.zeta, self.int_lr = torch.tensor(zeta, device=device), torch.tensor(int_lr, device=device)\n",
    "        self.bin_active, self.int_norm = bin_active, int_norm\n",
    "        self.monitor_volts, self.monitor_grads, self.monitor_latents = monitor_volts, monitor_grads, monitor_latents\n",
    "        self.monitor_annealing = monitor_annealing\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.temperature_1 = temperature_1\n",
    "        self.temperature_2 = temperature_2\n",
    "        self.device = device\n",
    "\n",
    "    def INV_AMP(self, x, R_INV):\n",
    "        return -self.V_INV * torch.tanh(R_INV * x / self.V_INV)\n",
    "\n",
    "    def SOFT_BIN(self, x):\n",
    "        if self.bin_active: return ((self.G_ON - self.G_OFF) * torch.sigmoid(x * self.zeta) + self.G_OFF)\n",
    "        else: return self.G_ON * x * self.zeta * 0.4\n",
    "\n",
    "    def PREPROCESS(self, img):\n",
    "        return (self.V_1 - self.V_0) * img.to(device) + self.V_0\n",
    "\n",
    "    def ANNEALER(self):\n",
    "        prob = torch.exp(torch.tensor(-1.0, device=self.device) / self.temperature_1)\n",
    "        prob = torch.clamp(prob, min=1e-3, max=1)\n",
    "\n",
    "        rand_vals = torch.rand((64, 64), device=self.device)\n",
    "        annealed_mask = torch.where(rand_vals < prob, -1, torch.where(rand_vals < 2 * prob, 0, 1))\n",
    "\n",
    "        return annealed_mask\n",
    "\n",
    "    def forward(self, img):\n",
    "        # Preprocessing: Two States of input (V_ON and V_OFF)\n",
    "        img = self.PREPROCESS(img.view(img.size(0), -1))\n",
    "        bias = self.PREPROCESS(((-1) ** torch.arange(self.first_bias, device=device)).repeat(img.shape[0], 1))\n",
    "        bias2 = self.PREPROCESS(((-1) ** torch.arange(self.final_bias, device=device)).repeat(img.shape[0], 1))\n",
    "\n",
    "        # RRAM Soft Binarization\n",
    "        g = self.SOFT_BIN(self.w)\n",
    "        if self.monitor_latents: tensor_stats(self.w, \"Latent Weights:\")\n",
    "\n",
    "        # Recurrent Encoding Layer\n",
    "        out1size = self.crossbar_out - self.output_size\n",
    "        feedback = torch.zeros((img.shape[0], self.encoding*self.feed_repeats), device=device)\n",
    "        out1 = torch.zeros((img.shape[0], out1size), device = device)\n",
    "\n",
    "        for r_pass in range(self.r_passes - 1):\n",
    "            ind_s, ind_f = self.crossbar_out - (r_pass+1)*self.encoding, self.crossbar_out - (r_pass)*self.encoding\n",
    "            ind_a, ind_b = out1size - (r_pass+1)*self.encoding, out1size - (r_pass)*self.encoding\n",
    "\n",
    "            x = torch.cat((feedback, bias, img[:, r_pass * self.data_in:(r_pass + 1) * self.data_in]), dim=1)\n",
    "            x = F.linear(x, g[ind_s:ind_f, : ], bias=None)\n",
    "\n",
    "            out1[:, ind_a:ind_b] = self.INV_AMP(x, self.R_INV)\n",
    "            if self.monitor_volts: tensor_stats(feedback, f\"Voltages in Recurrent Stage after pass {r_pass}\")\n",
    "\n",
    "            feedback = out1[:, ind_a:ind_b].repeat(1,self.feed_repeats)\n",
    "\n",
    "        else:\n",
    "            r_pass += 1\n",
    "            ind_s, ind_f = self.crossbar_out - (r_pass+1)*self.encoding - self.extra_final, self.crossbar_out - (r_pass)*self.encoding\n",
    "            ind_a, ind_b = out1size - (r_pass+1)*self.encoding - self.extra_final, out1size - (r_pass)*self.encoding\n",
    "\n",
    "            x = torch.cat((feedback, bias, img[:, r_pass * self.data_in:(r_pass + 1) * self.data_in]), dim=1)\n",
    "            x = F.linear(x, g[-(r_pass+1)*self.encoding - self.extra_final:-r_pass*self.encoding, : ], bias=None)\n",
    "\n",
    "            out1[:, ind_a:ind_b] = self.INV_AMP(x, self.R_INV)\n",
    "            if self.monitor_volts: tensor_stats(out1, f\"All Voltages in Recurrent Stage\")\n",
    "\n",
    "            feedback = out1[:, ind_a:ind_b].repeat(1,self.final_repeats)\n",
    "\n",
    "        x = torch.cat((feedback, bias2, out1), dim = 1)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Feature Extraction Layer\n",
    "        x = F.linear(x, g[:self.output_size, : ], bias=None)\n",
    "        x = self.INV_AMP(x, self.R_INV)\n",
    "        if self.monitor_volts: tensor_stats(x, f\"Voltages after h_layer {h_pass}\")\n",
    "\n",
    "        return x\n",
    "\n",
    "    def backprop(self, ext_lr):\n",
    "        with torch.no_grad():\n",
    "            if self.w.grad is not None:\n",
    "                grad = self.w.grad.to(device)\n",
    "                for i in range(grad.shape[0]):\n",
    "                    if self.int_norm:\n",
    "                        grad[i] = self.int_lr * grad[i] / (torch.norm(grad[i]) + 1e-20)\n",
    "                    grad[i] = ext_lr * grad[i]\n",
    "                if self.monitor_grads: tensor_stats(grad, \"Gradients\")\n",
    "                self.w -= grad\n",
    "                self.w.grad.zero_()\n",
    "\n",
    "    def anneal(self, inputs, labels, decay1, decay2):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.forward(inputs)\n",
    "            old_loss = criterion(outputs, labels).item() * inputs.size(0)\n",
    "            old_w = self.w.data.clone()\n",
    "\n",
    "            self.w.data = self.w.data * self.ANNEALER()\n",
    "            outputs = self.forward(inputs)\n",
    "            new_loss = criterion(outputs, labels).item() * inputs.size(0)\n",
    "\n",
    "            acceptance_prob = torch.exp(torch.tensor(-(new_loss - old_loss) / self.temperature_2, device=self.device))\n",
    "            if self.monitor_annealing: print(\"Old & New Losses\", old_loss, new_loss,\"Probab:\", acceptance_prob)\n",
    "            if new_loss < old_loss or torch.rand(1, device=self.device) < acceptance_prob:\n",
    "                if self.monitor_annealing: print(\"Annealed weights accepted\")\n",
    "            else:\n",
    "                self.w.data = old_w\n",
    "\n",
    "            self.temperature_1 *= decay1\n",
    "            self.temperature_2 *= decay2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845d51da-8368-4c97-89a2-9fc1374f408b",
   "metadata": {
    "id": "845d51da-8368-4c97-89a2-9fc1374f408b"
   },
   "source": [
    "## Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d332b5ba-a9e0-4e8a-bff9-3176267bef00",
   "metadata": {
    "id": "d332b5ba-a9e0-4e8a-bff9-3176267bef00"
   },
   "outputs": [],
   "source": [
    "params_RRAM = {\n",
    "    \"scaling\": 5,\n",
    "    \"G_ON\": 6e-5,\n",
    "    \"G_OFF\": 2.88e-6,\n",
    "    \"V_INV\": 0.6,\n",
    "    \"R_INV\": 1000.0,\n",
    "    \"V_1\": 0.1,\n",
    "    \"V_0\": -0.1,\n",
    "    \"zeta\": 10.0,\n",
    "    \"initial_factor\": 0.01,\n",
    "    \"crossbar\": (64, 64),\n",
    "    \"input_size\": 676,\n",
    "    \"encoding_size\": 4,\n",
    "    \"output_size\": 10,\n",
    "    \"data_in\": 52,\n",
    "    \"bin_active\": True,\n",
    "    \"monitor_volts\": False,\n",
    "    \"monitor_grads\": False,\n",
    "    \"monitor_latents\": False,\n",
    "    \"dropout\": 0.1,\n",
    "    \"int_lr\": 0.01,\n",
    "    \"int_norm\": True,\n",
    "    \"ext_lr\": 500,\n",
    "    \"epochs\": 1000,\n",
    "    \"temperature_1\": 0.2,\n",
    "    \"temperature_2\": 1000,\n",
    "    \"monitor_annealing\": True\n",
    "}\n",
    "\n",
    "\n",
    "model_params = {k: v for k, v in params_RRAM.items() if k not in [\"noise_std\", \"batch_size\", \"lr\", \"epochs\",\"ext_lr\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83a6271e-f7f2-4be1-8a71-b162ad2055e9",
   "metadata": {
    "id": "83a6271e-f7f2-4be1-8a71-b162ad2055e9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_RRAM = SoftBinaryRecurrentForwardNetwork(**model_params).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d1155-be9e-4a7a-9a74-a1d608188350",
   "metadata": {
    "id": "241d1155-be9e-4a7a-9a74-a1d608188350"
   },
   "source": [
    "## Training:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c0b7d2-7c19-4b3e-8856-76d2e9f32792",
   "metadata": {
    "id": "25c0b7d2-7c19-4b3e-8856-76d2e9f32792"
   },
   "source": [
    "### Training to a subset of Dataset First\n",
    "\n",
    "This is just to see if the model is backpropagating before putting in into the full training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efd13def-9751-4e0e-89bf-666a98444d02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "efd13def-9751-4e0e-89bf-666a98444d02",
    "outputId": "76bd894d-87a5-429b-bb31-d646e8921856"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old & New Losses 2301.3384342193604 2300.8852005004883 Probab: tensor(1.0005, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 1, LR: 20.0000, Train Loss: 2.3026, Train Accuracy: 8.10%, Temperatures:(0.20, 990.00)\n",
      "Old & New Losses 2525.710105895996 2526.263952255249 Probab: tensor(0.9994, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 2, LR: 100.0000, Train Loss: 2.3013, Train Accuracy: 10.70%, Temperatures:(0.20, 980.10)\n",
      "Old & New Losses 2408.0512523651123 2408.050537109375 Probab: tensor(1.0000, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 3, LR: 500.0000, Train Loss: 2.5247, Train Accuracy: 8.40%, Temperatures:(0.19, 970.30)\n",
      "Old & New Losses 2395.172357559204 2396.6596126556396 Probab: tensor(0.9985, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 4, LR: 500.0000, Train Loss: 2.4072, Train Accuracy: 12.00%, Temperatures:(0.19, 960.60)\n",
      "Old & New Losses 2353.4202575683594 2354.785203933716 Probab: tensor(0.9986, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 5, LR: 500.0000, Train Loss: 2.3976, Train Accuracy: 16.90%, Temperatures:(0.19, 950.99)\n",
      "Old & New Losses 2291.0513877868652 2300.9610176086426 Probab: tensor(0.9896, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 6, LR: 500.0000, Train Loss: 2.3531, Train Accuracy: 19.20%, Temperatures:(0.19, 941.48)\n",
      "Old & New Losses 2245.1603412628174 2241.0106658935547 Probab: tensor(1.0044, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 7, LR: 500.0000, Train Loss: 2.3035, Train Accuracy: 18.00%, Temperatures:(0.19, 932.07)\n",
      "Old & New Losses 2201.343059539795 2208.515167236328 Probab: tensor(0.9923, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 8, LR: 500.0000, Train Loss: 2.2431, Train Accuracy: 19.40%, Temperatures:(0.18, 922.74)\n",
      "Old & New Losses 2160.2139472961426 2182.718515396118 Probab: tensor(0.9759, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 9, LR: 500.0000, Train Loss: 2.2049, Train Accuracy: 22.90%, Temperatures:(0.18, 913.52)\n",
      "Old & New Losses 2135.212182998657 2135.4031562805176 Probab: tensor(0.9998, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 10, LR: 500.0000, Train Loss: 2.1798, Train Accuracy: 22.40%, Temperatures:(0.18, 904.38)\n",
      "Old & New Losses 2068.4990882873535 2096.3985919952393 Probab: tensor(0.9696, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 11, LR: 500.0000, Train Loss: 2.1398, Train Accuracy: 20.90%, Temperatures:(0.18, 895.34)\n",
      "Old & New Losses 2041.3424968719482 2072.791337966919 Probab: tensor(0.9655, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 12, LR: 500.0000, Train Loss: 2.0925, Train Accuracy: 29.30%, Temperatures:(0.18, 886.38)\n",
      "Old & New Losses 2046.2312698364258 2041.344404220581 Probab: tensor(1.0055, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 13, LR: 500.0000, Train Loss: 2.0766, Train Accuracy: 30.80%, Temperatures:(0.18, 877.52)\n",
      "Old & New Losses 2042.4883365631104 2043.4579849243164 Probab: tensor(0.9989, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 14, LR: 500.0000, Train Loss: 2.0464, Train Accuracy: 34.70%, Temperatures:(0.17, 868.75)\n",
      "Old & New Losses 2009.8652839660645 2015.0728225708008 Probab: tensor(0.9940, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 15, LR: 500.0000, Train Loss: 2.0436, Train Accuracy: 34.60%, Temperatures:(0.17, 860.06)\n",
      "Old & New Losses 1987.0655536651611 1998.052716255188 Probab: tensor(0.9873, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 16, LR: 500.0000, Train Loss: 2.0158, Train Accuracy: 42.70%, Temperatures:(0.17, 851.46)\n",
      "Old & New Losses 1985.8638048171997 1976.6428470611572 Probab: tensor(1.0109, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 17, LR: 500.0000, Train Loss: 2.0023, Train Accuracy: 40.30%, Temperatures:(0.17, 842.94)\n",
      "Old & New Losses 1976.9946336746216 1986.6935014724731 Probab: tensor(0.9886, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 18, LR: 500.0000, Train Loss: 1.9864, Train Accuracy: 42.80%, Temperatures:(0.17, 834.51)\n",
      "Old & New Losses 1972.5537300109863 1975.402593612671 Probab: tensor(0.9966, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 19, LR: 500.0000, Train Loss: 1.9820, Train Accuracy: 43.80%, Temperatures:(0.17, 826.17)\n",
      "Old & New Losses 1967.3336744308472 1972.0920324325562 Probab: tensor(0.9943, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 20, LR: 500.0000, Train Loss: 1.9763, Train Accuracy: 45.70%, Temperatures:(0.16, 817.91)\n",
      "Old & New Losses 1960.0571393966675 1966.2748575210571 Probab: tensor(0.9924, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 21, LR: 500.0000, Train Loss: 1.9696, Train Accuracy: 44.70%, Temperatures:(0.16, 809.73)\n",
      "Old & New Losses 1952.4543285369873 1963.8073444366455 Probab: tensor(0.9861, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 22, LR: 500.0000, Train Loss: 1.9707, Train Accuracy: 43.40%, Temperatures:(0.16, 801.63)\n",
      "Old & New Losses 1936.1335039138794 1942.2733783721924 Probab: tensor(0.9924, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 23, LR: 500.0000, Train Loss: 1.9573, Train Accuracy: 43.80%, Temperatures:(0.16, 793.61)\n",
      "Old & New Losses 1997.448444366455 2004.5742988586426 Probab: tensor(0.9911, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 24, LR: 500.0000, Train Loss: 1.9430, Train Accuracy: 47.60%, Temperatures:(0.16, 785.68)\n",
      "Old & New Losses 1953.4248113632202 1962.5370502471924 Probab: tensor(0.9885, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 25, LR: 500.0000, Train Loss: 2.0043, Train Accuracy: 39.10%, Temperatures:(0.16, 777.82)\n",
      "Old & New Losses 1949.1767883300781 1946.6558694839478 Probab: tensor(1.0032, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 26, LR: 500.0000, Train Loss: 1.9635, Train Accuracy: 40.30%, Temperatures:(0.15, 770.04)\n",
      "Old & New Losses 1931.0798645019531 1925.9148836135864 Probab: tensor(1.0067, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 27, LR: 500.0000, Train Loss: 1.9404, Train Accuracy: 44.80%, Temperatures:(0.15, 762.34)\n",
      "Old & New Losses 1929.0770292282104 1932.7843189239502 Probab: tensor(0.9951, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 28, LR: 500.0000, Train Loss: 1.9276, Train Accuracy: 45.60%, Temperatures:(0.15, 754.72)\n",
      "Old & New Losses 1935.1959228515625 1942.1567916870117 Probab: tensor(0.9908, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 29, LR: 500.0000, Train Loss: 1.9345, Train Accuracy: 41.70%, Temperatures:(0.15, 747.17)\n",
      "Old & New Losses 1912.4141931533813 1922.079086303711 Probab: tensor(0.9871, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 30, LR: 500.0000, Train Loss: 1.9417, Train Accuracy: 46.00%, Temperatures:(0.15, 739.70)\n",
      "Old & New Losses 1935.731291770935 1942.6922798156738 Probab: tensor(0.9906, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 31, LR: 500.0000, Train Loss: 1.9190, Train Accuracy: 46.80%, Temperatures:(0.15, 732.30)\n",
      "Old & New Losses 1922.4128723144531 1915.568470954895 Probab: tensor(1.0094, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 32, LR: 500.0000, Train Loss: 1.9412, Train Accuracy: 45.20%, Temperatures:(0.14, 724.98)\n",
      "Old & New Losses 1914.4808053970337 1917.519211769104 Probab: tensor(0.9958, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 33, LR: 500.0000, Train Loss: 1.9232, Train Accuracy: 44.10%, Temperatures:(0.14, 717.73)\n",
      "Old & New Losses 1914.119005203247 1913.7086868286133 Probab: tensor(1.0006, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 34, LR: 500.0000, Train Loss: 1.9205, Train Accuracy: 43.60%, Temperatures:(0.14, 710.55)\n",
      "Old & New Losses 1906.258225440979 1918.4976816177368 Probab: tensor(0.9829, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 35, LR: 500.0000, Train Loss: 1.9107, Train Accuracy: 46.10%, Temperatures:(0.14, 703.45)\n",
      "Old & New Losses 1909.637212753296 1911.028265953064 Probab: tensor(0.9980, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 36, LR: 500.0000, Train Loss: 1.9144, Train Accuracy: 46.40%, Temperatures:(0.14, 696.41)\n",
      "Old & New Losses 1954.750895500183 1945.3725814819336 Probab: tensor(1.0136, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 37, LR: 500.0000, Train Loss: 1.9125, Train Accuracy: 42.70%, Temperatures:(0.14, 689.45)\n",
      "Old & New Losses 1899.5566368103027 1904.6012163162231 Probab: tensor(0.9927, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 38, LR: 500.0000, Train Loss: 1.9469, Train Accuracy: 39.50%, Temperatures:(0.14, 682.55)\n",
      "Old & New Losses 1903.4768342971802 1895.9786891937256 Probab: tensor(1.0110, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 39, LR: 500.0000, Train Loss: 1.9070, Train Accuracy: 45.70%, Temperatures:(0.14, 675.73)\n",
      "Old & New Losses 1907.0326089859009 1905.6396484375 Probab: tensor(1.0021, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 40, LR: 500.0000, Train Loss: 1.9025, Train Accuracy: 48.10%, Temperatures:(0.13, 668.97)\n",
      "Old & New Losses 1899.3535041809082 1897.2892761230469 Probab: tensor(1.0031, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 41, LR: 500.0000, Train Loss: 1.9042, Train Accuracy: 44.40%, Temperatures:(0.13, 662.28)\n",
      "Old & New Losses 1885.8729600906372 1897.6104259490967 Probab: tensor(0.9824, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 42, LR: 500.0000, Train Loss: 1.8975, Train Accuracy: 45.40%, Temperatures:(0.13, 655.66)\n",
      "Old & New Losses 1882.7881813049316 1886.9528770446777 Probab: tensor(0.9937, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 43, LR: 500.0000, Train Loss: 1.8933, Train Accuracy: 44.90%, Temperatures:(0.13, 649.10)\n",
      "Old & New Losses 1885.6253623962402 1898.046612739563 Probab: tensor(0.9810, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 44, LR: 500.0000, Train Loss: 1.8880, Train Accuracy: 45.00%, Temperatures:(0.13, 642.61)\n",
      "Old & New Losses 1893.197774887085 1887.8660202026367 Probab: tensor(1.0083, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 45, LR: 500.0000, Train Loss: 1.8992, Train Accuracy: 44.30%, Temperatures:(0.13, 636.19)\n",
      "Old & New Losses 1927.1571636199951 1950.2973556518555 Probab: tensor(0.9643, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 46, LR: 500.0000, Train Loss: 1.8872, Train Accuracy: 43.90%, Temperatures:(0.13, 629.82)\n",
      "Old & New Losses 1899.3172645568848 1895.9918022155762 Probab: tensor(1.0053, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 47, LR: 500.0000, Train Loss: 1.9536, Train Accuracy: 37.00%, Temperatures:(0.12, 623.53)\n",
      "Old & New Losses 1891.905426979065 1893.9286470413208 Probab: tensor(0.9968, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 48, LR: 500.0000, Train Loss: 1.8984, Train Accuracy: 43.40%, Temperatures:(0.12, 617.29)\n",
      "Old & New Losses 1893.5853242874146 1897.2524404525757 Probab: tensor(0.9941, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 49, LR: 500.0000, Train Loss: 1.8879, Train Accuracy: 44.60%, Temperatures:(0.12, 611.12)\n",
      "Old & New Losses 1893.4475183486938 1893.7222957611084 Probab: tensor(0.9996, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 50, LR: 500.0000, Train Loss: 1.8967, Train Accuracy: 43.50%, Temperatures:(0.12, 605.01)\n",
      "Old & New Losses 1881.8877935409546 1884.8713636398315 Probab: tensor(0.9951, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 51, LR: 500.0000, Train Loss: 1.8950, Train Accuracy: 42.80%, Temperatures:(0.12, 598.96)\n",
      "Old & New Losses 1904.1250944137573 1904.4370651245117 Probab: tensor(0.9995, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 52, LR: 250.0000, Train Loss: 1.8901, Train Accuracy: 43.30%, Temperatures:(0.12, 592.97)\n",
      "Old & New Losses 1884.3759298324585 1887.2193098068237 Probab: tensor(0.9952, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 53, LR: 250.0000, Train Loss: 1.8979, Train Accuracy: 44.30%, Temperatures:(0.12, 587.04)\n",
      "Old & New Losses 1897.0171213150024 1897.2100019454956 Probab: tensor(0.9997, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 54, LR: 250.0000, Train Loss: 1.8840, Train Accuracy: 41.80%, Temperatures:(0.12, 581.17)\n",
      "Old & New Losses 1890.9436464309692 1886.649250984192 Probab: tensor(1.0074, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 55, LR: 250.0000, Train Loss: 1.9011, Train Accuracy: 42.60%, Temperatures:(0.12, 575.35)\n",
      "Old & New Losses 1905.4994583129883 1932.2748184204102 Probab: tensor(0.9545, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 56, LR: 250.0000, Train Loss: 1.8916, Train Accuracy: 43.30%, Temperatures:(0.11, 569.60)\n",
      "Old & New Losses 1890.2090787887573 1889.715552330017 Probab: tensor(1.0009, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 57, LR: 250.0000, Train Loss: 1.9318, Train Accuracy: 39.80%, Temperatures:(0.11, 563.91)\n",
      "Old & New Losses 1917.5899028778076 1960.5391025543213 Probab: tensor(0.9267, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 58, LR: 250.0000, Train Loss: 1.8896, Train Accuracy: 44.70%, Temperatures:(0.11, 558.27)\n",
      "Old & New Losses 1895.073652267456 1903.3361673355103 Probab: tensor(0.9853, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 59, LR: 250.0000, Train Loss: 1.9556, Train Accuracy: 38.90%, Temperatures:(0.11, 552.68)\n",
      "Old & New Losses 1896.8876600265503 1899.673581123352 Probab: tensor(0.9950, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 60, LR: 250.0000, Train Loss: 1.8973, Train Accuracy: 43.90%, Temperatures:(0.11, 547.16)\n",
      "Old & New Losses 1892.0522928237915 1885.4628801345825 Probab: tensor(1.0121, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 61, LR: 250.0000, Train Loss: 1.9017, Train Accuracy: 43.10%, Temperatures:(0.11, 541.69)\n",
      "Old & New Losses 1898.686170578003 1906.2058925628662 Probab: tensor(0.9862, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 62, LR: 250.0000, Train Loss: 1.8952, Train Accuracy: 44.30%, Temperatures:(0.11, 536.27)\n",
      "Old & New Losses 1888.8143301010132 1889.9177312850952 Probab: tensor(0.9979, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 63, LR: 250.0000, Train Loss: 1.9002, Train Accuracy: 41.60%, Temperatures:(0.11, 530.91)\n",
      "Old & New Losses 1898.4324932098389 1895.963191986084 Probab: tensor(1.0047, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 64, LR: 250.0000, Train Loss: 1.8913, Train Accuracy: 44.20%, Temperatures:(0.11, 525.60)\n",
      "Old & New Losses 1892.9072618484497 1892.4623727798462 Probab: tensor(1.0008, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 65, LR: 250.0000, Train Loss: 1.8951, Train Accuracy: 42.20%, Temperatures:(0.10, 520.34)\n",
      "Old & New Losses 1897.579550743103 1888.2933855056763 Probab: tensor(1.0180, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 66, LR: 250.0000, Train Loss: 1.8922, Train Accuracy: 45.80%, Temperatures:(0.10, 515.14)\n",
      "Old & New Losses 1887.4781131744385 1895.8340883255005 Probab: tensor(0.9839, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 67, LR: 250.0000, Train Loss: 1.8860, Train Accuracy: 44.10%, Temperatures:(0.10, 509.99)\n",
      "Old & New Losses 1884.04381275177 1894.5578336715698 Probab: tensor(0.9796, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 68, LR: 250.0000, Train Loss: 1.8872, Train Accuracy: 45.50%, Temperatures:(0.10, 504.89)\n",
      "Old & New Losses 1887.1359825134277 1880.827784538269 Probab: tensor(1.0126, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 69, LR: 250.0000, Train Loss: 1.8964, Train Accuracy: 45.00%, Temperatures:(0.10, 499.84)\n",
      "Old & New Losses 1888.0133628845215 1885.7344388961792 Probab: tensor(1.0046, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 70, LR: 250.0000, Train Loss: 1.8889, Train Accuracy: 45.50%, Temperatures:(0.10, 494.84)\n",
      "Old & New Losses 1884.3833208084106 1890.4945850372314 Probab: tensor(0.9877, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 71, LR: 250.0000, Train Loss: 1.8903, Train Accuracy: 43.20%, Temperatures:(0.10, 489.89)\n",
      "Old & New Losses 1888.360619544983 1892.909049987793 Probab: tensor(0.9908, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 72, LR: 250.0000, Train Loss: 1.8863, Train Accuracy: 45.20%, Temperatures:(0.10, 484.99)\n",
      "Old & New Losses 1887.9884481430054 1893.2831287384033 Probab: tensor(0.9891, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 73, LR: 250.0000, Train Loss: 1.8845, Train Accuracy: 44.70%, Temperatures:(0.10, 480.14)\n",
      "Old & New Losses 1889.4414901733398 1882.8142881393433 Probab: tensor(1.0139, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 74, LR: 250.0000, Train Loss: 1.8875, Train Accuracy: 43.60%, Temperatures:(0.10, 475.34)\n",
      "Old & New Losses 1886.2030506134033 1890.6015157699585 Probab: tensor(0.9908, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 75, LR: 250.0000, Train Loss: 1.8928, Train Accuracy: 45.50%, Temperatures:(0.09, 470.59)\n",
      "Old & New Losses 1889.1143798828125 1887.2252702713013 Probab: tensor(1.0040, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 76, LR: 250.0000, Train Loss: 1.8890, Train Accuracy: 43.90%, Temperatures:(0.09, 465.88)\n",
      "Old & New Losses 1881.300449371338 1892.1900987625122 Probab: tensor(0.9769, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 77, LR: 250.0000, Train Loss: 1.8910, Train Accuracy: 42.40%, Temperatures:(0.09, 461.22)\n",
      "Old & New Losses 1891.1596536636353 1893.967866897583 Probab: tensor(0.9939, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 78, LR: 250.0000, Train Loss: 1.8937, Train Accuracy: 44.10%, Temperatures:(0.09, 456.61)\n",
      "Old & New Losses 1894.7701454162598 1899.0280628204346 Probab: tensor(0.9907, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 79, LR: 250.0000, Train Loss: 1.8959, Train Accuracy: 43.50%, Temperatures:(0.09, 452.04)\n",
      "Old & New Losses 1896.3030576705933 1891.345739364624 Probab: tensor(1.0110, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 80, LR: 250.0000, Train Loss: 1.8978, Train Accuracy: 43.50%, Temperatures:(0.09, 447.52)\n",
      "Old & New Losses 1894.8884010314941 1906.6789150238037 Probab: tensor(0.9740, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 81, LR: 250.0000, Train Loss: 1.8953, Train Accuracy: 44.00%, Temperatures:(0.09, 443.05)\n",
      "Old & New Losses 1891.7906284332275 1893.6069011688232 Probab: tensor(0.9959, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 82, LR: 250.0000, Train Loss: 1.8979, Train Accuracy: 42.30%, Temperatures:(0.09, 438.62)\n",
      "Old & New Losses 1896.3011503219604 1890.2060985565186 Probab: tensor(1.0140, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 83, LR: 250.0000, Train Loss: 1.8922, Train Accuracy: 42.20%, Temperatures:(0.09, 434.23)\n",
      "Old & New Losses 1896.806001663208 1909.335732460022 Probab: tensor(0.9716, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 84, LR: 250.0000, Train Loss: 1.8938, Train Accuracy: 46.00%, Temperatures:(0.09, 429.89)\n",
      "Old & New Losses 1898.525357246399 1904.2155742645264 Probab: tensor(0.9869, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 85, LR: 250.0000, Train Loss: 1.9080, Train Accuracy: 42.80%, Temperatures:(0.09, 425.59)\n",
      "Old & New Losses 1897.4815607070923 1901.4207124710083 Probab: tensor(0.9908, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 86, LR: 250.0000, Train Loss: 1.8982, Train Accuracy: 43.60%, Temperatures:(0.08, 421.33)\n",
      "Old & New Losses 1896.4909315109253 1886.7239952087402 Probab: tensor(1.0235, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 87, LR: 250.0000, Train Loss: 1.9042, Train Accuracy: 41.80%, Temperatures:(0.08, 417.12)\n",
      "Old & New Losses 1891.416311264038 1894.6634531021118 Probab: tensor(0.9922, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 88, LR: 250.0000, Train Loss: 1.8937, Train Accuracy: 45.20%, Temperatures:(0.08, 412.95)\n",
      "Old & New Losses 1898.8806009292603 1893.2034969329834 Probab: tensor(1.0138, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 89, LR: 250.0000, Train Loss: 1.8878, Train Accuracy: 43.00%, Temperatures:(0.08, 408.82)\n",
      "Old & New Losses 1891.8856382369995 1894.495129585266 Probab: tensor(0.9936, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 90, LR: 250.0000, Train Loss: 1.8956, Train Accuracy: 44.50%, Temperatures:(0.08, 404.73)\n",
      "Old & New Losses 1887.9050016403198 1893.661379814148 Probab: tensor(0.9859, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 91, LR: 250.0000, Train Loss: 1.8974, Train Accuracy: 42.80%, Temperatures:(0.08, 400.68)\n",
      "Old & New Losses 1894.378662109375 1890.3993368148804 Probab: tensor(1.0100, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 92, LR: 250.0000, Train Loss: 1.8893, Train Accuracy: 43.70%, Temperatures:(0.08, 396.68)\n",
      "Old & New Losses 1893.1407928466797 1898.4150886535645 Probab: tensor(0.9868, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 93, LR: 250.0000, Train Loss: 1.8934, Train Accuracy: 43.80%, Temperatures:(0.08, 392.71)\n",
      "Old & New Losses 1895.7223892211914 1908.41805934906 Probab: tensor(0.9682, device='cuda:0')\n",
      "Epoch 94, LR: 250.0000, Train Loss: 1.9052, Train Accuracy: 42.40%, Temperatures:(0.08, 388.78)\n",
      "Old & New Losses 1894.519329071045 1898.1150388717651 Probab: tensor(0.9908, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 95, LR: 250.0000, Train Loss: 1.8947, Train Accuracy: 44.00%, Temperatures:(0.08, 384.90)\n",
      "Old & New Losses 1896.597981452942 1894.5902585983276 Probab: tensor(1.0052, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 96, LR: 250.0000, Train Loss: 1.8999, Train Accuracy: 44.10%, Temperatures:(0.08, 381.05)\n",
      "Old & New Losses 1892.7724361419678 1900.3877639770508 Probab: tensor(0.9802, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 97, LR: 250.0000, Train Loss: 1.9038, Train Accuracy: 44.80%, Temperatures:(0.08, 377.24)\n",
      "Old & New Losses 1895.286202430725 1897.316575050354 Probab: tensor(0.9946, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 98, LR: 250.0000, Train Loss: 1.8961, Train Accuracy: 45.10%, Temperatures:(0.07, 373.46)\n",
      "Old & New Losses 1899.1749286651611 1895.7915306091309 Probab: tensor(1.0091, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 99, LR: 250.0000, Train Loss: 1.8940, Train Accuracy: 45.90%, Temperatures:(0.07, 369.73)\n",
      "Old & New Losses 1892.4816846847534 1899.0368843078613 Probab: tensor(0.9824, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 100, LR: 250.0000, Train Loss: 1.9021, Train Accuracy: 44.70%, Temperatures:(0.07, 366.03)\n",
      "Old & New Losses 1901.514172554016 1896.8994617462158 Probab: tensor(1.0127, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 101, LR: 250.0000, Train Loss: 1.9010, Train Accuracy: 44.70%, Temperatures:(0.07, 362.37)\n",
      "Old & New Losses 1893.2360410690308 1904.7738313674927 Probab: tensor(0.9687, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 102, LR: 125.0000, Train Loss: 1.9021, Train Accuracy: 44.30%, Temperatures:(0.07, 358.75)\n",
      "Old & New Losses 1898.9614248275757 1900.435209274292 Probab: tensor(0.9959, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 103, LR: 125.0000, Train Loss: 1.9048, Train Accuracy: 45.40%, Temperatures:(0.07, 355.16)\n",
      "Old & New Losses 1897.7981805801392 1907.6024293899536 Probab: tensor(0.9728, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 104, LR: 125.0000, Train Loss: 1.9017, Train Accuracy: 46.50%, Temperatures:(0.07, 351.61)\n",
      "Old & New Losses 1897.2373008728027 1903.3541679382324 Probab: tensor(0.9828, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 105, LR: 125.0000, Train Loss: 1.9051, Train Accuracy: 43.90%, Temperatures:(0.07, 348.09)\n",
      "Old & New Losses 1896.176815032959 1905.648946762085 Probab: tensor(0.9732, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 106, LR: 125.0000, Train Loss: 1.9053, Train Accuracy: 42.70%, Temperatures:(0.07, 344.61)\n",
      "Old & New Losses 1908.7210893630981 1912.7347469329834 Probab: tensor(0.9884, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 107, LR: 125.0000, Train Loss: 1.9027, Train Accuracy: 43.10%, Temperatures:(0.07, 341.17)\n",
      "Old & New Losses 1898.88596534729 1904.4169187545776 Probab: tensor(0.9839, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 108, LR: 125.0000, Train Loss: 1.9115, Train Accuracy: 41.30%, Temperatures:(0.07, 337.75)\n",
      "Old & New Losses 1913.043737411499 1915.9120321273804 Probab: tensor(0.9915, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 109, LR: 125.0000, Train Loss: 1.9106, Train Accuracy: 42.90%, Temperatures:(0.07, 334.38)\n",
      "Old & New Losses 1913.1947755813599 1914.8668050765991 Probab: tensor(0.9950, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 110, LR: 125.0000, Train Loss: 1.9209, Train Accuracy: 41.60%, Temperatures:(0.07, 331.03)\n",
      "Old & New Losses 1913.4575128555298 1929.4040203094482 Probab: tensor(0.9530, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 111, LR: 125.0000, Train Loss: 1.9171, Train Accuracy: 41.70%, Temperatures:(0.07, 327.72)\n",
      "Old & New Losses 1930.3419589996338 1928.4616708755493 Probab: tensor(1.0058, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 112, LR: 125.0000, Train Loss: 1.9358, Train Accuracy: 38.80%, Temperatures:(0.06, 324.45)\n",
      "Old & New Losses 1924.9992370605469 1922.8957891464233 Probab: tensor(1.0065, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 113, LR: 125.0000, Train Loss: 1.9245, Train Accuracy: 40.10%, Temperatures:(0.06, 321.20)\n",
      "Old & New Losses 1927.9378652572632 1929.4363260269165 Probab: tensor(0.9953, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 114, LR: 125.0000, Train Loss: 1.9285, Train Accuracy: 42.30%, Temperatures:(0.06, 317.99)\n",
      "Old & New Losses 1926.275134086609 1928.4287691116333 Probab: tensor(0.9933, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 115, LR: 125.0000, Train Loss: 1.9213, Train Accuracy: 41.20%, Temperatures:(0.06, 314.81)\n",
      "Old & New Losses 1922.4460124969482 1924.085259437561 Probab: tensor(0.9948, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 116, LR: 125.0000, Train Loss: 1.9329, Train Accuracy: 40.60%, Temperatures:(0.06, 311.66)\n",
      "Old & New Losses 1920.2101230621338 1922.257900238037 Probab: tensor(0.9935, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 117, LR: 125.0000, Train Loss: 1.9268, Train Accuracy: 40.80%, Temperatures:(0.06, 308.54)\n",
      "Old & New Losses 1925.8126020431519 1926.032304763794 Probab: tensor(0.9993, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 118, LR: 125.0000, Train Loss: 1.9286, Train Accuracy: 40.20%, Temperatures:(0.06, 305.46)\n",
      "Old & New Losses 1927.3561239242554 1921.7545986175537 Probab: tensor(1.0185, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 119, LR: 125.0000, Train Loss: 1.9260, Train Accuracy: 41.40%, Temperatures:(0.06, 302.40)\n",
      "Old & New Losses 1923.8612651824951 1932.3290586471558 Probab: tensor(0.9724, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 120, LR: 125.0000, Train Loss: 1.9339, Train Accuracy: 38.60%, Temperatures:(0.06, 299.38)\n",
      "Old & New Losses 1933.0658912658691 1935.4487657546997 Probab: tensor(0.9921, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 121, LR: 125.0000, Train Loss: 1.9341, Train Accuracy: 39.60%, Temperatures:(0.06, 296.39)\n",
      "Old & New Losses 1936.5925788879395 1940.9910440444946 Probab: tensor(0.9853, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 122, LR: 125.0000, Train Loss: 1.9363, Train Accuracy: 38.60%, Temperatures:(0.06, 293.42)\n",
      "Old & New Losses 1932.158350944519 1933.4157705307007 Probab: tensor(0.9957, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 123, LR: 125.0000, Train Loss: 1.9327, Train Accuracy: 40.10%, Temperatures:(0.06, 290.49)\n",
      "Old & New Losses 1932.665228843689 1934.5118999481201 Probab: tensor(0.9937, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 124, LR: 125.0000, Train Loss: 1.9310, Train Accuracy: 39.60%, Temperatures:(0.06, 287.58)\n",
      "Old & New Losses 1930.2418231964111 1931.2618970870972 Probab: tensor(0.9965, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 125, LR: 125.0000, Train Loss: 1.9281, Train Accuracy: 40.20%, Temperatures:(0.06, 284.71)\n",
      "Old & New Losses 1933.9426755905151 1935.1201057434082 Probab: tensor(0.9959, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 126, LR: 125.0000, Train Loss: 1.9379, Train Accuracy: 41.50%, Temperatures:(0.06, 281.86)\n",
      "Old & New Losses 1930.907964706421 1931.5961599349976 Probab: tensor(0.9976, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 127, LR: 125.0000, Train Loss: 1.9346, Train Accuracy: 39.40%, Temperatures:(0.06, 279.04)\n",
      "Old & New Losses 1923.4492778778076 1937.68310546875 Probab: tensor(0.9503, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 128, LR: 125.0000, Train Loss: 1.9325, Train Accuracy: 38.00%, Temperatures:(0.06, 276.25)\n",
      "Old & New Losses 1933.1860542297363 1931.2450885772705 Probab: tensor(1.0071, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 129, LR: 125.0000, Train Loss: 1.9379, Train Accuracy: 40.30%, Temperatures:(0.05, 273.49)\n",
      "Old & New Losses 1931.7405223846436 1936.3152980804443 Probab: tensor(0.9834, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 130, LR: 125.0000, Train Loss: 1.9262, Train Accuracy: 39.50%, Temperatures:(0.05, 270.75)\n",
      "Old & New Losses 1931.7604303359985 1922.8293895721436 Probab: tensor(1.0335, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 131, LR: 125.0000, Train Loss: 1.9380, Train Accuracy: 38.40%, Temperatures:(0.05, 268.05)\n",
      "Old & New Losses 1925.611972808838 1932.3447942733765 Probab: tensor(0.9752, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 132, LR: 125.0000, Train Loss: 1.9266, Train Accuracy: 39.80%, Temperatures:(0.05, 265.37)\n",
      "Old & New Losses 1932.6242208480835 1934.2613220214844 Probab: tensor(0.9938, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 133, LR: 125.0000, Train Loss: 1.9276, Train Accuracy: 39.80%, Temperatures:(0.05, 262.71)\n",
      "Old & New Losses 1926.0998964309692 1923.6929416656494 Probab: tensor(1.0092, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 134, LR: 125.0000, Train Loss: 1.9315, Train Accuracy: 39.90%, Temperatures:(0.05, 260.09)\n",
      "Old & New Losses 1924.5704412460327 1925.8226156234741 Probab: tensor(0.9952, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 135, LR: 125.0000, Train Loss: 1.9229, Train Accuracy: 39.50%, Temperatures:(0.05, 257.48)\n",
      "Old & New Losses 1920.4264879226685 1921.3098287582397 Probab: tensor(0.9966, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 136, LR: 125.0000, Train Loss: 1.9275, Train Accuracy: 39.50%, Temperatures:(0.05, 254.91)\n",
      "Old & New Losses 1927.9502630233765 1932.9144954681396 Probab: tensor(0.9807, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 137, LR: 125.0000, Train Loss: 1.9294, Train Accuracy: 40.10%, Temperatures:(0.05, 252.36)\n",
      "Old & New Losses 1929.1014671325684 1927.4367094039917 Probab: tensor(1.0066, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 138, LR: 125.0000, Train Loss: 1.9287, Train Accuracy: 39.90%, Temperatures:(0.05, 249.84)\n",
      "Old & New Losses 1922.4480390548706 1924.2128133773804 Probab: tensor(0.9930, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 139, LR: 125.0000, Train Loss: 1.9206, Train Accuracy: 41.60%, Temperatures:(0.05, 247.34)\n",
      "Old & New Losses 1927.4957180023193 1929.803967475891 Probab: tensor(0.9907, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 140, LR: 125.0000, Train Loss: 1.9222, Train Accuracy: 39.50%, Temperatures:(0.05, 244.87)\n",
      "Old & New Losses 1930.0981760025024 1932.206630706787 Probab: tensor(0.9914, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 141, LR: 125.0000, Train Loss: 1.9285, Train Accuracy: 40.40%, Temperatures:(0.05, 242.42)\n",
      "Old & New Losses 1930.9449195861816 1932.722568511963 Probab: tensor(0.9927, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 142, LR: 125.0000, Train Loss: 1.9365, Train Accuracy: 38.10%, Temperatures:(0.05, 239.99)\n",
      "Old & New Losses 1924.450397491455 1929.4908046722412 Probab: tensor(0.9792, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 143, LR: 125.0000, Train Loss: 1.9320, Train Accuracy: 38.90%, Temperatures:(0.05, 237.59)\n",
      "Old & New Losses 1925.1229763031006 1933.8618516921997 Probab: tensor(0.9639, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 144, LR: 125.0000, Train Loss: 1.9285, Train Accuracy: 39.20%, Temperatures:(0.05, 235.22)\n",
      "Old & New Losses 1928.236484527588 1923.7195253372192 Probab: tensor(1.0194, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 145, LR: 125.0000, Train Loss: 1.9276, Train Accuracy: 40.10%, Temperatures:(0.05, 232.86)\n",
      "Old & New Losses 1926.1667728424072 1940.1710033416748 Probab: tensor(0.9416, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 146, LR: 125.0000, Train Loss: 1.9338, Train Accuracy: 40.10%, Temperatures:(0.05, 230.54)\n",
      "Old & New Losses 1938.5669231414795 1948.378562927246 Probab: tensor(0.9583, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 147, LR: 125.0000, Train Loss: 1.9421, Train Accuracy: 39.90%, Temperatures:(0.05, 228.23)\n",
      "Old & New Losses 1945.5173015594482 1945.2228546142578 Probab: tensor(1.0013, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 148, LR: 125.0000, Train Loss: 1.9466, Train Accuracy: 39.20%, Temperatures:(0.05, 225.95)\n",
      "Old & New Losses 1943.1105852127075 1949.5841264724731 Probab: tensor(0.9718, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 149, LR: 125.0000, Train Loss: 1.9493, Train Accuracy: 39.30%, Temperatures:(0.04, 223.69)\n",
      "Old & New Losses 1939.4285678863525 1939.2712116241455 Probab: tensor(1.0007, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 150, LR: 125.0000, Train Loss: 1.9425, Train Accuracy: 38.90%, Temperatures:(0.04, 221.45)\n",
      "Old & New Losses 1938.3940696716309 1940.6899213790894 Probab: tensor(0.9897, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 151, LR: 125.0000, Train Loss: 1.9445, Train Accuracy: 38.60%, Temperatures:(0.04, 219.24)\n",
      "Old & New Losses 1937.1458292007446 1938.3360147476196 Probab: tensor(0.9946, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 152, LR: 62.5000, Train Loss: 1.9386, Train Accuracy: 39.80%, Temperatures:(0.04, 217.04)\n",
      "Old & New Losses 1934.6128702163696 1933.2631826400757 Probab: tensor(1.0062, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 153, LR: 62.5000, Train Loss: 1.9399, Train Accuracy: 39.40%, Temperatures:(0.04, 214.87)\n",
      "Old & New Losses 1933.0710172653198 1934.0864419937134 Probab: tensor(0.9953, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 154, LR: 62.5000, Train Loss: 1.9301, Train Accuracy: 40.70%, Temperatures:(0.04, 212.73)\n",
      "Old & New Losses 1929.9983978271484 1926.4353513717651 Probab: tensor(1.0169, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 155, LR: 62.5000, Train Loss: 1.9326, Train Accuracy: 40.00%, Temperatures:(0.04, 210.60)\n",
      "Old & New Losses 1934.2317581176758 1938.0375146865845 Probab: tensor(0.9821, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 156, LR: 62.5000, Train Loss: 1.9325, Train Accuracy: 40.50%, Temperatures:(0.04, 208.49)\n",
      "Old & New Losses 1932.5757026672363 1933.3568811416626 Probab: tensor(0.9963, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 157, LR: 62.5000, Train Loss: 1.9403, Train Accuracy: 41.00%, Temperatures:(0.04, 206.41)\n",
      "Old & New Losses 1933.5368871688843 1935.1283311843872 Probab: tensor(0.9923, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 158, LR: 62.5000, Train Loss: 1.9378, Train Accuracy: 40.80%, Temperatures:(0.04, 204.34)\n",
      "Old & New Losses 1927.2819757461548 1932.0756196975708 Probab: tensor(0.9768, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 159, LR: 62.5000, Train Loss: 1.9354, Train Accuracy: 39.20%, Temperatures:(0.04, 202.30)\n",
      "Old & New Losses 1931.6506385803223 1933.8703155517578 Probab: tensor(0.9891, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 160, LR: 62.5000, Train Loss: 1.9331, Train Accuracy: 41.10%, Temperatures:(0.04, 200.28)\n",
      "Old & New Losses 1934.2095851898193 1934.7646236419678 Probab: tensor(0.9972, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 161, LR: 62.5000, Train Loss: 1.9322, Train Accuracy: 41.80%, Temperatures:(0.04, 198.27)\n",
      "Old & New Losses 1936.1296892166138 1935.8174800872803 Probab: tensor(1.0016, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 162, LR: 62.5000, Train Loss: 1.9365, Train Accuracy: 38.50%, Temperatures:(0.04, 196.29)\n",
      "Old & New Losses 1922.7105379104614 1935.327410697937 Probab: tensor(0.9377, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 163, LR: 62.5000, Train Loss: 1.9336, Train Accuracy: 40.80%, Temperatures:(0.04, 194.33)\n",
      "Old & New Losses 1940.154790878296 1945.9868669509888 Probab: tensor(0.9704, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 164, LR: 62.5000, Train Loss: 1.9402, Train Accuracy: 40.80%, Temperatures:(0.04, 192.39)\n",
      "Old & New Losses 1937.7033710479736 1951.4367580413818 Probab: tensor(0.9311, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 165, LR: 62.5000, Train Loss: 1.9399, Train Accuracy: 39.50%, Temperatures:(0.04, 190.46)\n",
      "Old & New Losses 1953.5449743270874 1956.598401069641 Probab: tensor(0.9841, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 166, LR: 62.5000, Train Loss: 1.9490, Train Accuracy: 39.90%, Temperatures:(0.04, 188.56)\n",
      "Old & New Losses 1952.4962902069092 1951.2050151824951 Probab: tensor(1.0069, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 167, LR: 62.5000, Train Loss: 1.9558, Train Accuracy: 39.70%, Temperatures:(0.04, 186.67)\n",
      "Old & New Losses 1954.4404745101929 1951.9951343536377 Probab: tensor(1.0132, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 168, LR: 62.5000, Train Loss: 1.9532, Train Accuracy: 40.30%, Temperatures:(0.04, 184.80)\n",
      "Old & New Losses 1949.2446184158325 1956.2907218933105 Probab: tensor(0.9626, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 169, LR: 62.5000, Train Loss: 1.9560, Train Accuracy: 39.90%, Temperatures:(0.04, 182.96)\n",
      "Old & New Losses 1952.1585702896118 1952.4728059768677 Probab: tensor(0.9983, device='cuda:0')\n",
      "Epoch 170, LR: 62.5000, Train Loss: 1.9587, Train Accuracy: 37.90%, Temperatures:(0.04, 181.13)\n",
      "Old & New Losses 1949.1249322891235 1970.0957536697388 Probab: tensor(0.8907, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 171, LR: 62.5000, Train Loss: 1.9448, Train Accuracy: 38.00%, Temperatures:(0.04, 179.32)\n",
      "Old & New Losses 1968.1146144866943 1957.4685096740723 Probab: tensor(1.0612, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 172, LR: 62.5000, Train Loss: 1.9724, Train Accuracy: 38.10%, Temperatures:(0.04, 177.52)\n",
      "Old & New Losses 1953.6786079406738 1975.3402471542358 Probab: tensor(0.8851, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 173, LR: 62.5000, Train Loss: 1.9604, Train Accuracy: 39.10%, Temperatures:(0.04, 175.75)\n",
      "Old & New Losses 1971.420407295227 1978.8310527801514 Probab: tensor(0.9587, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 174, LR: 62.5000, Train Loss: 1.9735, Train Accuracy: 37.60%, Temperatures:(0.03, 173.99)\n",
      "Old & New Losses 1970.5241918563843 1968.3822393417358 Probab: tensor(1.0124, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 175, LR: 62.5000, Train Loss: 1.9729, Train Accuracy: 37.40%, Temperatures:(0.03, 172.25)\n",
      "Old & New Losses 1971.09854221344 1970.2972173690796 Probab: tensor(1.0047, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 176, LR: 62.5000, Train Loss: 1.9753, Train Accuracy: 38.00%, Temperatures:(0.03, 170.53)\n",
      "Old & New Losses 1965.384602546692 1994.1903352737427 Probab: tensor(0.8446, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 177, LR: 62.5000, Train Loss: 1.9640, Train Accuracy: 38.10%, Temperatures:(0.03, 168.82)\n",
      "Old & New Losses 1993.1532144546509 2007.5125694274902 Probab: tensor(0.9185, device='cuda:0')\n",
      "Epoch 178, LR: 62.5000, Train Loss: 2.0000, Train Accuracy: 35.70%, Temperatures:(0.03, 167.13)\n",
      "Old & New Losses 1986.796259880066 1990.064263343811 Probab: tensor(0.9806, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 179, LR: 62.5000, Train Loss: 1.9955, Train Accuracy: 36.50%, Temperatures:(0.03, 165.46)\n",
      "Old & New Losses 1982.992172241211 1987.2852563858032 Probab: tensor(0.9744, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 180, LR: 62.5000, Train Loss: 1.9952, Train Accuracy: 36.70%, Temperatures:(0.03, 163.81)\n",
      "Old & New Losses 1984.4924211502075 1988.5165691375732 Probab: tensor(0.9757, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 181, LR: 62.5000, Train Loss: 1.9951, Train Accuracy: 37.10%, Temperatures:(0.03, 162.17)\n",
      "Old & New Losses 1984.9677085876465 1983.7925434112549 Probab: tensor(1.0073, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 182, LR: 62.5000, Train Loss: 1.9945, Train Accuracy: 35.60%, Temperatures:(0.03, 160.55)\n",
      "Old & New Losses 1987.8404140472412 1995.6456422805786 Probab: tensor(0.9525, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 183, LR: 62.5000, Train Loss: 1.9877, Train Accuracy: 37.30%, Temperatures:(0.03, 158.94)\n",
      "Old & New Losses 1986.9815111160278 2000.2131462097168 Probab: tensor(0.9201, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 184, LR: 62.5000, Train Loss: 1.9927, Train Accuracy: 36.40%, Temperatures:(0.03, 157.35)\n",
      "Old & New Losses 1988.2162809371948 2008.2001686096191 Probab: tensor(0.8807, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 185, LR: 62.5000, Train Loss: 1.9898, Train Accuracy: 34.90%, Temperatures:(0.03, 155.78)\n",
      "Old & New Losses 1994.185209274292 2002.2425651550293 Probab: tensor(0.9496, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 186, LR: 62.5000, Train Loss: 1.9914, Train Accuracy: 35.40%, Temperatures:(0.03, 154.22)\n",
      "Old & New Losses 1992.6120042800903 1986.3736629486084 Probab: tensor(1.0413, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 187, LR: 62.5000, Train Loss: 1.9958, Train Accuracy: 35.90%, Temperatures:(0.03, 152.68)\n",
      "Old & New Losses 1985.4339361190796 1987.6538515090942 Probab: tensor(0.9856, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 188, LR: 62.5000, Train Loss: 1.9905, Train Accuracy: 35.80%, Temperatures:(0.03, 151.15)\n",
      "Old & New Losses 1988.2097244262695 1991.295576095581 Probab: tensor(0.9798, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 189, LR: 62.5000, Train Loss: 1.9912, Train Accuracy: 36.00%, Temperatures:(0.03, 149.64)\n",
      "Old & New Losses 1989.3484115600586 1981.7440509796143 Probab: tensor(1.0521, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 190, LR: 62.5000, Train Loss: 1.9917, Train Accuracy: 36.70%, Temperatures:(0.03, 148.14)\n",
      "Old & New Losses 1989.4022941589355 1992.9770231246948 Probab: tensor(0.9762, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 191, LR: 62.5000, Train Loss: 1.9899, Train Accuracy: 34.90%, Temperatures:(0.03, 146.66)\n",
      "Old & New Losses 1991.3443326950073 1985.06760597229 Probab: tensor(1.0437, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 192, LR: 62.5000, Train Loss: 1.9920, Train Accuracy: 35.20%, Temperatures:(0.03, 145.20)\n",
      "Old & New Losses 1989.1916513442993 1989.6320104599 Probab: tensor(0.9970, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 193, LR: 62.5000, Train Loss: 1.9898, Train Accuracy: 34.60%, Temperatures:(0.03, 143.74)\n",
      "Old & New Losses 1983.870506286621 1975.5455255508423 Probab: tensor(1.0596, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 194, LR: 62.5000, Train Loss: 1.9876, Train Accuracy: 34.70%, Temperatures:(0.03, 142.31)\n",
      "Old & New Losses 1980.8961153030396 1982.6865196228027 Probab: tensor(0.9875, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 195, LR: 62.5000, Train Loss: 1.9758, Train Accuracy: 35.80%, Temperatures:(0.03, 140.88)\n",
      "Old & New Losses 1976.6093492507935 1975.2904176712036 Probab: tensor(1.0094, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 196, LR: 62.5000, Train Loss: 1.9857, Train Accuracy: 35.80%, Temperatures:(0.03, 139.48)\n",
      "Old & New Losses 1977.9068231582642 1979.623794555664 Probab: tensor(0.9878, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 197, LR: 62.5000, Train Loss: 1.9782, Train Accuracy: 35.80%, Temperatures:(0.03, 138.08)\n",
      "Old & New Losses 1997.6015090942383 2002.9218196868896 Probab: tensor(0.9622, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 198, LR: 62.5000, Train Loss: 1.9837, Train Accuracy: 35.80%, Temperatures:(0.03, 136.70)\n",
      "Old & New Losses 1982.411503791809 1992.531418800354 Probab: tensor(0.9286, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 199, LR: 62.5000, Train Loss: 1.9990, Train Accuracy: 32.00%, Temperatures:(0.03, 135.33)\n",
      "Old & New Losses 2003.6191940307617 2001.420497894287 Probab: tensor(1.0164, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 200, LR: 62.5000, Train Loss: 1.9951, Train Accuracy: 33.60%, Temperatures:(0.03, 133.98)\n",
      "Old & New Losses 1994.6503639221191 1994.4849014282227 Probab: tensor(1.0012, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 201, LR: 62.5000, Train Loss: 2.0019, Train Accuracy: 32.80%, Temperatures:(0.03, 132.64)\n",
      "Old & New Losses 1995.8012104034424 1988.572597503662 Probab: tensor(1.0560, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 202, LR: 31.2500, Train Loss: 1.9959, Train Accuracy: 33.60%, Temperatures:(0.03, 131.31)\n",
      "Old & New Losses 1996.6638088226318 1997.7717399597168 Probab: tensor(0.9916, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 203, LR: 31.2500, Train Loss: 1.9936, Train Accuracy: 35.40%, Temperatures:(0.03, 130.00)\n",
      "Old & New Losses 1994.600534439087 2001.6887187957764 Probab: tensor(0.9469, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 204, LR: 31.2500, Train Loss: 2.0045, Train Accuracy: 33.30%, Temperatures:(0.03, 128.70)\n",
      "Old & New Losses 1993.1237697601318 1997.2244501113892 Probab: tensor(0.9686, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 205, LR: 31.2500, Train Loss: 2.0010, Train Accuracy: 34.50%, Temperatures:(0.03, 127.41)\n",
      "Old & New Losses 1988.4743690490723 2002.528190612793 Probab: tensor(0.8956, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 206, LR: 31.2500, Train Loss: 1.9940, Train Accuracy: 35.00%, Temperatures:(0.03, 126.14)\n",
      "Old & New Losses 2006.7775249481201 2008.0246925354004 Probab: tensor(0.9902, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 207, LR: 31.2500, Train Loss: 2.0046, Train Accuracy: 34.80%, Temperatures:(0.02, 124.88)\n",
      "Old & New Losses 2006.9117546081543 2001.570463180542 Probab: tensor(1.0437, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 208, LR: 31.2500, Train Loss: 2.0030, Train Accuracy: 33.30%, Temperatures:(0.02, 123.63)\n",
      "Old & New Losses 1995.8300590515137 2006.8953037261963 Probab: tensor(0.9144, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 209, LR: 31.2500, Train Loss: 2.0026, Train Accuracy: 35.20%, Temperatures:(0.02, 122.39)\n",
      "Old & New Losses 1999.9359846115112 2002.1553039550781 Probab: tensor(0.9820, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 210, LR: 31.2500, Train Loss: 2.0129, Train Accuracy: 33.30%, Temperatures:(0.02, 121.17)\n",
      "Old & New Losses 1992.6749467849731 2003.8371086120605 Probab: tensor(0.9120, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 211, LR: 31.2500, Train Loss: 2.0007, Train Accuracy: 35.20%, Temperatures:(0.02, 119.96)\n",
      "Old & New Losses 2003.1135082244873 2002.046823501587 Probab: tensor(1.0089, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 212, LR: 31.2500, Train Loss: 2.0107, Train Accuracy: 34.10%, Temperatures:(0.02, 118.76)\n",
      "Old & New Losses 1989.797592163086 2000.2985000610352 Probab: tensor(0.9154, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 213, LR: 31.2500, Train Loss: 2.0009, Train Accuracy: 35.60%, Temperatures:(0.02, 117.57)\n",
      "Old & New Losses 1996.1405992507935 1998.3152151107788 Probab: tensor(0.9817, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 214, LR: 31.2500, Train Loss: 2.0044, Train Accuracy: 35.80%, Temperatures:(0.02, 116.39)\n",
      "Old & New Losses 1996.6800212860107 2003.476619720459 Probab: tensor(0.9433, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 215, LR: 31.2500, Train Loss: 2.0014, Train Accuracy: 36.00%, Temperatures:(0.02, 115.23)\n",
      "Old & New Losses 2005.751609802246 1997.1749782562256 Probab: tensor(1.0773, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 216, LR: 31.2500, Train Loss: 2.0075, Train Accuracy: 36.20%, Temperatures:(0.02, 114.08)\n",
      "Old & New Losses 2007.164716720581 2001.058578491211 Probab: tensor(1.0550, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 217, LR: 31.2500, Train Loss: 2.0021, Train Accuracy: 35.80%, Temperatures:(0.02, 112.94)\n",
      "Old & New Losses 1996.7474937438965 2032.3514938354492 Probab: tensor(0.7296, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 218, LR: 31.2500, Train Loss: 2.0043, Train Accuracy: 36.10%, Temperatures:(0.02, 111.81)\n",
      "Old & New Losses 2029.5157432556152 2028.0718803405762 Probab: tensor(1.0130, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 219, LR: 31.2500, Train Loss: 2.0316, Train Accuracy: 34.00%, Temperatures:(0.02, 110.69)\n",
      "Old & New Losses 2025.604009628296 2020.7500457763672 Probab: tensor(1.0448, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 220, LR: 31.2500, Train Loss: 2.0176, Train Accuracy: 35.60%, Temperatures:(0.02, 109.58)\n",
      "Old & New Losses 2010.8044147491455 2023.343563079834 Probab: tensor(0.8919, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 221, LR: 31.2500, Train Loss: 2.0256, Train Accuracy: 34.00%, Temperatures:(0.02, 108.49)\n",
      "Old & New Losses 2021.4223861694336 2027.8868675231934 Probab: tensor(0.9422, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 222, LR: 31.2500, Train Loss: 2.0261, Train Accuracy: 35.20%, Temperatures:(0.02, 107.40)\n",
      "Old & New Losses 2021.2390422821045 2020.8492279052734 Probab: tensor(1.0036, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 223, LR: 31.2500, Train Loss: 2.0212, Train Accuracy: 35.20%, Temperatures:(0.02, 106.33)\n",
      "Old & New Losses 2021.8896865844727 2019.7219848632812 Probab: tensor(1.0206, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 224, LR: 31.2500, Train Loss: 2.0295, Train Accuracy: 34.30%, Temperatures:(0.02, 105.26)\n",
      "Old & New Losses 2019.0517902374268 2018.2111263275146 Probab: tensor(1.0080, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 225, LR: 31.2500, Train Loss: 2.0147, Train Accuracy: 36.40%, Temperatures:(0.02, 104.21)\n",
      "Old & New Losses 2013.8671398162842 2008.5275173187256 Probab: tensor(1.0526, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 226, LR: 31.2500, Train Loss: 2.0202, Train Accuracy: 35.70%, Temperatures:(0.02, 103.17)\n",
      "Old & New Losses 2008.7809562683105 2014.096736907959 Probab: tensor(0.9498, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 227, LR: 31.2500, Train Loss: 2.0158, Train Accuracy: 35.80%, Temperatures:(0.02, 102.14)\n",
      "Old & New Losses 2008.678674697876 2014.0302181243896 Probab: tensor(0.9490, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 228, LR: 31.2500, Train Loss: 2.0105, Train Accuracy: 36.40%, Temperatures:(0.02, 101.12)\n",
      "Old & New Losses 2003.2668113708496 2008.8038444519043 Probab: tensor(0.9467, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 229, LR: 31.2500, Train Loss: 2.0238, Train Accuracy: 34.90%, Temperatures:(0.02, 100.11)\n",
      "Old & New Losses 2005.8107376098633 2011.575698852539 Probab: tensor(0.9440, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 230, LR: 31.2500, Train Loss: 2.0146, Train Accuracy: 35.00%, Temperatures:(0.02, 99.10)\n",
      "Old & New Losses 2011.594295501709 2007.728099822998 Probab: tensor(1.0398, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 231, LR: 31.2500, Train Loss: 2.0108, Train Accuracy: 35.80%, Temperatures:(0.02, 98.11)\n",
      "Old & New Losses 2003.277063369751 2002.3243427276611 Probab: tensor(1.0098, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 232, LR: 31.2500, Train Loss: 2.0078, Train Accuracy: 36.40%, Temperatures:(0.02, 97.13)\n",
      "Old & New Losses 1997.1076250076294 1999.0754127502441 Probab: tensor(0.9799, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 233, LR: 31.2500, Train Loss: 2.0010, Train Accuracy: 37.30%, Temperatures:(0.02, 96.16)\n",
      "Old & New Losses 2000.319480895996 2009.4871520996094 Probab: tensor(0.9091, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 234, LR: 31.2500, Train Loss: 2.0012, Train Accuracy: 37.10%, Temperatures:(0.02, 95.20)\n",
      "Old & New Losses 2000.931978225708 2004.2085647583008 Probab: tensor(0.9662, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 235, LR: 31.2500, Train Loss: 2.0096, Train Accuracy: 34.90%, Temperatures:(0.02, 94.25)\n",
      "Old & New Losses 2006.5345764160156 2003.9958953857422 Probab: tensor(1.0273, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 236, LR: 31.2500, Train Loss: 2.0041, Train Accuracy: 35.40%, Temperatures:(0.02, 93.31)\n",
      "Old & New Losses 2001.605749130249 2003.305196762085 Probab: tensor(0.9820, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 237, LR: 31.2500, Train Loss: 2.0012, Train Accuracy: 36.00%, Temperatures:(0.02, 92.37)\n",
      "Old & New Losses 1998.900294303894 2007.9100131988525 Probab: tensor(0.9071, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 238, LR: 31.2500, Train Loss: 2.0004, Train Accuracy: 36.10%, Temperatures:(0.02, 91.45)\n",
      "Old & New Losses 2000.4100799560547 2008.467435836792 Probab: tensor(0.9157, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 239, LR: 31.2500, Train Loss: 2.0065, Train Accuracy: 37.40%, Temperatures:(0.02, 90.53)\n",
      "Old & New Losses 2006.4303874969482 2001.826286315918 Probab: tensor(1.0522, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 240, LR: 31.2500, Train Loss: 2.0007, Train Accuracy: 36.00%, Temperatures:(0.02, 89.63)\n",
      "Old & New Losses 2005.9199333190918 2001.4162063598633 Probab: tensor(1.0515, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 241, LR: 31.2500, Train Loss: 2.0065, Train Accuracy: 35.20%, Temperatures:(0.02, 88.73)\n",
      "Old & New Losses 2005.415916442871 2016.4363384246826 Probab: tensor(0.8832, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 242, LR: 31.2500, Train Loss: 2.0038, Train Accuracy: 35.10%, Temperatures:(0.02, 87.85)\n",
      "Old & New Losses 2013.998031616211 2017.531394958496 Probab: tensor(0.9606, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 243, LR: 31.2500, Train Loss: 2.0161, Train Accuracy: 34.70%, Temperatures:(0.02, 86.97)\n",
      "Old & New Losses 2012.6709938049316 2015.1708126068115 Probab: tensor(0.9717, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 244, LR: 31.2500, Train Loss: 2.0138, Train Accuracy: 34.60%, Temperatures:(0.02, 86.10)\n",
      "Old & New Losses 2006.8912506103516 2006.5901279449463 Probab: tensor(1.0035, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 245, LR: 31.2500, Train Loss: 2.0137, Train Accuracy: 33.50%, Temperatures:(0.02, 85.24)\n",
      "Old & New Losses 2011.2395286560059 2012.9480361938477 Probab: tensor(0.9802, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 246, LR: 31.2500, Train Loss: 2.0051, Train Accuracy: 35.40%, Temperatures:(0.02, 84.38)\n",
      "Old & New Losses 2017.4496173858643 2012.2137069702148 Probab: tensor(1.0640, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 247, LR: 31.2500, Train Loss: 2.0096, Train Accuracy: 35.60%, Temperatures:(0.02, 83.54)\n",
      "Old & New Losses 2017.467737197876 2042.264699935913 Probab: tensor(0.7432, device='cuda:0')\n",
      "Epoch 248, LR: 31.2500, Train Loss: 2.0111, Train Accuracy: 34.70%, Temperatures:(0.02, 82.70)\n",
      "Old & New Losses 2012.79616355896 2009.0012550354004 Probab: tensor(1.0470, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 249, LR: 31.2500, Train Loss: 2.0273, Train Accuracy: 31.50%, Temperatures:(0.02, 81.88)\n",
      "Old & New Losses 2006.6428184509277 2011.950969696045 Probab: tensor(0.9372, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 250, LR: 31.2500, Train Loss: 2.0114, Train Accuracy: 34.40%, Temperatures:(0.02, 81.06)\n",
      "Old & New Losses 2009.8333358764648 2017.5118446350098 Probab: tensor(0.9096, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 251, LR: 31.2500, Train Loss: 2.0094, Train Accuracy: 36.10%, Temperatures:(0.02, 80.25)\n",
      "Old & New Losses 2012.793779373169 2007.9426765441895 Probab: tensor(1.0623, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 252, LR: 15.6250, Train Loss: 2.0128, Train Accuracy: 35.50%, Temperatures:(0.02, 79.45)\n",
      "Old & New Losses 2008.0277919769287 2013.1134986877441 Probab: tensor(0.9380, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 253, LR: 15.6250, Train Loss: 2.0128, Train Accuracy: 34.60%, Temperatures:(0.02, 78.65)\n",
      "Old & New Losses 2010.3232860565186 2018.9270973205566 Probab: tensor(0.8964, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 254, LR: 15.6250, Train Loss: 2.0137, Train Accuracy: 35.60%, Temperatures:(0.02, 77.86)\n",
      "Old & New Losses 2011.246681213379 2014.7652626037598 Probab: tensor(0.9558, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 255, LR: 15.6250, Train Loss: 2.0141, Train Accuracy: 35.70%, Temperatures:(0.02, 77.09)\n",
      "Old & New Losses 2008.284568786621 2019.8464393615723 Probab: tensor(0.8607, device='cuda:0')\n",
      "Epoch 256, LR: 15.6250, Train Loss: 2.0188, Train Accuracy: 34.50%, Temperatures:(0.02, 76.31)\n",
      "Old & New Losses 2008.6205005645752 2012.2764110565186 Probab: tensor(0.9532, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 257, LR: 15.6250, Train Loss: 2.0126, Train Accuracy: 35.90%, Temperatures:(0.02, 75.55)\n",
      "Old & New Losses 2006.1962604522705 2022.5913524627686 Probab: tensor(0.8049, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 258, LR: 15.6250, Train Loss: 2.0074, Train Accuracy: 35.00%, Temperatures:(0.01, 74.80)\n",
      "Old & New Losses 2018.498182296753 2027.0791053771973 Probab: tensor(0.8916, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 259, LR: 15.6250, Train Loss: 2.0177, Train Accuracy: 34.80%, Temperatures:(0.01, 74.05)\n",
      "Old & New Losses 2020.0836658477783 2031.292200088501 Probab: tensor(0.8595, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 260, LR: 15.6250, Train Loss: 2.0322, Train Accuracy: 34.00%, Temperatures:(0.01, 73.31)\n",
      "Old & New Losses 2031.3913822174072 2019.8428630828857 Probab: tensor(1.1706, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 261, LR: 15.6250, Train Loss: 2.0385, Train Accuracy: 32.70%, Temperatures:(0.01, 72.57)\n",
      "Old & New Losses 2014.0154361724854 2022.146224975586 Probab: tensor(0.8940, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 262, LR: 15.6250, Train Loss: 2.0288, Train Accuracy: 35.50%, Temperatures:(0.01, 71.85)\n",
      "Old & New Losses 2026.7226696014404 2020.4980373382568 Probab: tensor(1.0905, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 263, LR: 15.6250, Train Loss: 2.0255, Train Accuracy: 34.00%, Temperatures:(0.01, 71.13)\n",
      "Old & New Losses 2020.017147064209 2031.5062999725342 Probab: tensor(0.8508, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 264, LR: 15.6250, Train Loss: 2.0211, Train Accuracy: 34.20%, Temperatures:(0.01, 70.42)\n",
      "Old & New Losses 2023.8807201385498 2025.1328945159912 Probab: tensor(0.9824, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 265, LR: 15.6250, Train Loss: 2.0269, Train Accuracy: 34.50%, Temperatures:(0.01, 69.72)\n",
      "Old & New Losses 2030.015468597412 2028.592824935913 Probab: tensor(1.0206, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 266, LR: 15.6250, Train Loss: 2.0311, Train Accuracy: 33.50%, Temperatures:(0.01, 69.02)\n",
      "Old & New Losses 2024.3308544158936 2028.5074710845947 Probab: tensor(0.9413, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 267, LR: 15.6250, Train Loss: 2.0340, Train Accuracy: 34.40%, Temperatures:(0.01, 68.33)\n",
      "Old & New Losses 2025.270700454712 2029.4303894042969 Probab: tensor(0.9409, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 268, LR: 15.6250, Train Loss: 2.0328, Train Accuracy: 34.10%, Temperatures:(0.01, 67.64)\n",
      "Old & New Losses 2035.5596542358398 2031.569480895996 Probab: tensor(1.0608, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 269, LR: 15.6250, Train Loss: 2.0293, Train Accuracy: 34.40%, Temperatures:(0.01, 66.97)\n",
      "Old & New Losses 2030.2486419677734 2026.111125946045 Probab: tensor(1.0637, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 270, LR: 15.6250, Train Loss: 2.0342, Train Accuracy: 34.20%, Temperatures:(0.01, 66.30)\n",
      "Old & New Losses 2028.9313793182373 2036.9858741760254 Probab: tensor(0.8856, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 271, LR: 15.6250, Train Loss: 2.0266, Train Accuracy: 34.60%, Temperatures:(0.01, 65.64)\n",
      "Old & New Losses 2031.2378406524658 2029.759168624878 Probab: tensor(1.0228, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 272, LR: 15.6250, Train Loss: 2.0372, Train Accuracy: 33.70%, Temperatures:(0.01, 64.98)\n",
      "Old & New Losses 2025.710105895996 2027.900218963623 Probab: tensor(0.9669, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 273, LR: 15.6250, Train Loss: 2.0311, Train Accuracy: 34.30%, Temperatures:(0.01, 64.33)\n",
      "Old & New Losses 2025.761365890503 2034.9791049957275 Probab: tensor(0.8665, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 274, LR: 15.6250, Train Loss: 2.0354, Train Accuracy: 34.60%, Temperatures:(0.01, 63.69)\n",
      "Old & New Losses 2031.8574905395508 2028.179407119751 Probab: tensor(1.0595, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 275, LR: 15.6250, Train Loss: 2.0309, Train Accuracy: 34.60%, Temperatures:(0.01, 63.05)\n",
      "Old & New Losses 2022.5551128387451 2026.6392230987549 Probab: tensor(0.9373, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 276, LR: 15.6250, Train Loss: 2.0202, Train Accuracy: 34.90%, Temperatures:(0.01, 62.42)\n",
      "Old & New Losses 2020.6799507141113 2028.7199020385742 Probab: tensor(0.8791, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 277, LR: 15.6250, Train Loss: 2.0260, Train Accuracy: 34.90%, Temperatures:(0.01, 61.79)\n",
      "Old & New Losses 2023.2462882995605 2025.4278182983398 Probab: tensor(0.9653, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 278, LR: 15.6250, Train Loss: 2.0302, Train Accuracy: 35.00%, Temperatures:(0.01, 61.18)\n",
      "Old & New Losses 2026.1316299438477 2037.099838256836 Probab: tensor(0.8359, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 279, LR: 15.6250, Train Loss: 2.0269, Train Accuracy: 33.70%, Temperatures:(0.01, 60.56)\n",
      "Old & New Losses 2030.336618423462 2024.3816375732422 Probab: tensor(1.1033, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 280, LR: 15.6250, Train Loss: 2.0339, Train Accuracy: 34.10%, Temperatures:(0.01, 59.96)\n",
      "Old & New Losses 2028.2537937164307 2033.8091850280762 Probab: tensor(0.9115, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 281, LR: 15.6250, Train Loss: 2.0273, Train Accuracy: 34.20%, Temperatures:(0.01, 59.36)\n",
      "Old & New Losses 2037.5401973724365 2038.559913635254 Probab: tensor(0.9830, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 282, LR: 15.6250, Train Loss: 2.0285, Train Accuracy: 34.60%, Temperatures:(0.01, 58.77)\n",
      "Old & New Losses 2033.6580276489258 2048.200845718384 Probab: tensor(0.7808, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 283, LR: 15.6250, Train Loss: 2.0391, Train Accuracy: 33.90%, Temperatures:(0.01, 58.18)\n",
      "Old & New Losses 2043.4606075286865 2044.1582202911377 Probab: tensor(0.9881, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 284, LR: 15.6250, Train Loss: 2.0508, Train Accuracy: 30.80%, Temperatures:(0.01, 57.60)\n",
      "Old & New Losses 2050.9417057037354 2072.5131034851074 Probab: tensor(0.6876, device='cuda:0')\n",
      "Epoch 285, LR: 15.6250, Train Loss: 2.0491, Train Accuracy: 33.10%, Temperatures:(0.01, 57.02)\n",
      "Old & New Losses 2044.1834926605225 2048.043727874756 Probab: tensor(0.9345, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 286, LR: 15.6250, Train Loss: 2.0570, Train Accuracy: 33.30%, Temperatures:(0.01, 56.45)\n",
      "Old & New Losses 2044.4886684417725 2053.4050464630127 Probab: tensor(0.8539, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 287, LR: 15.6250, Train Loss: 2.0448, Train Accuracy: 32.00%, Temperatures:(0.01, 55.89)\n",
      "Old & New Losses 2045.1397895812988 2049.43585395813 Probab: tensor(0.9260, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 288, LR: 15.6250, Train Loss: 2.0527, Train Accuracy: 33.10%, Temperatures:(0.01, 55.33)\n",
      "Old & New Losses 2042.2496795654297 2040.1546955108643 Probab: tensor(1.0386, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 289, LR: 15.6250, Train Loss: 2.0490, Train Accuracy: 34.70%, Temperatures:(0.01, 54.77)\n",
      "Old & New Losses 2047.3525524139404 2047.0402240753174 Probab: tensor(1.0057, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 290, LR: 15.6250, Train Loss: 2.0415, Train Accuracy: 34.20%, Temperatures:(0.01, 54.23)\n",
      "Old & New Losses 2034.7285270690918 2043.203592300415 Probab: tensor(0.8553, device='cuda:0')\n",
      "Epoch 291, LR: 15.6250, Train Loss: 2.0403, Train Accuracy: 33.90%, Temperatures:(0.01, 53.68)\n",
      "Old & New Losses 2037.6825332641602 2040.32564163208 Probab: tensor(0.9520, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 292, LR: 15.6250, Train Loss: 2.0344, Train Accuracy: 33.50%, Temperatures:(0.01, 53.15)\n",
      "Old & New Losses 2041.4197444915771 2039.9987697601318 Probab: tensor(1.0271, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 293, LR: 15.6250, Train Loss: 2.0403, Train Accuracy: 32.90%, Temperatures:(0.01, 52.62)\n",
      "Old & New Losses 2040.799856185913 2057.7054023742676 Probab: tensor(0.7252, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 294, LR: 15.6250, Train Loss: 2.0452, Train Accuracy: 33.00%, Temperatures:(0.01, 52.09)\n",
      "Old & New Losses 2030.888557434082 2041.973352432251 Probab: tensor(0.8083, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 295, LR: 15.6250, Train Loss: 2.0582, Train Accuracy: 30.10%, Temperatures:(0.01, 51.57)\n",
      "Old & New Losses 2034.3379974365234 2034.6064567565918 Probab: tensor(0.9948, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 296, LR: 15.6250, Train Loss: 2.0389, Train Accuracy: 33.60%, Temperatures:(0.01, 51.05)\n",
      "Old & New Losses 2033.7281227111816 2030.7977199554443 Probab: tensor(1.0591, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 297, LR: 15.6250, Train Loss: 2.0331, Train Accuracy: 33.50%, Temperatures:(0.01, 50.54)\n",
      "Old & New Losses 2030.5097103118896 2024.6877670288086 Probab: tensor(1.1221, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 298, LR: 15.6250, Train Loss: 2.0336, Train Accuracy: 33.40%, Temperatures:(0.01, 50.04)\n",
      "Old & New Losses 2031.4106941223145 2029.057264328003 Probab: tensor(1.0482, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 299, LR: 15.6250, Train Loss: 2.0275, Train Accuracy: 34.00%, Temperatures:(0.01, 49.54)\n",
      "Old & New Losses 2019.8676586151123 2019.946575164795 Probab: tensor(0.9984, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 300, LR: 15.6250, Train Loss: 2.0376, Train Accuracy: 33.90%, Temperatures:(0.01, 49.04)\n",
      "Old & New Losses 2020.2045440673828 2049.1812229156494 Probab: tensor(0.5538, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 301, LR: 15.6250, Train Loss: 2.0224, Train Accuracy: 35.10%, Temperatures:(0.01, 48.55)\n",
      "Old & New Losses 2044.5973873138428 2046.0870265960693 Probab: tensor(0.9698, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 302, LR: 7.8125, Train Loss: 2.0459, Train Accuracy: 32.50%, Temperatures:(0.01, 48.06)\n",
      "Old & New Losses 2034.6245765686035 2055.354595184326 Probab: tensor(0.6497, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 303, LR: 7.8125, Train Loss: 2.0511, Train Accuracy: 32.10%, Temperatures:(0.01, 47.58)\n",
      "Old & New Losses 2037.6458168029785 2073.3113288879395 Probab: tensor(0.4726, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 304, LR: 7.8125, Train Loss: 2.0517, Train Accuracy: 32.90%, Temperatures:(0.01, 47.11)\n",
      "Old & New Losses 2060.966730117798 2078.845739364624 Probab: tensor(0.6842, device='cuda:0')\n",
      "Epoch 305, LR: 7.8125, Train Loss: 2.0668, Train Accuracy: 31.50%, Temperatures:(0.01, 46.64)\n",
      "Old & New Losses 2056.016445159912 2059.678316116333 Probab: tensor(0.9245, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 306, LR: 7.8125, Train Loss: 2.0641, Train Accuracy: 33.70%, Temperatures:(0.01, 46.17)\n",
      "Old & New Losses 2057.7244758605957 2051.7404079437256 Probab: tensor(1.1384, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 307, LR: 7.8125, Train Loss: 2.0574, Train Accuracy: 33.80%, Temperatures:(0.01, 45.71)\n",
      "Old & New Losses 2048.4652519226074 2058.18247795105 Probab: tensor(0.8085, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 308, LR: 7.8125, Train Loss: 2.0565, Train Accuracy: 33.20%, Temperatures:(0.01, 45.25)\n",
      "Old & New Losses 2051.807641983032 2044.0714359283447 Probab: tensor(1.1864, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 309, LR: 7.8125, Train Loss: 2.0575, Train Accuracy: 32.70%, Temperatures:(0.01, 44.80)\n",
      "Old & New Losses 2045.816421508789 2057.1563243865967 Probab: tensor(0.7764, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 310, LR: 7.8125, Train Loss: 2.0519, Train Accuracy: 32.60%, Temperatures:(0.01, 44.35)\n",
      "Old & New Losses 2049.476146697998 2056.3411712646484 Probab: tensor(0.8566, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 311, LR: 7.8125, Train Loss: 2.0501, Train Accuracy: 33.50%, Temperatures:(0.01, 43.91)\n",
      "Old & New Losses 2053.0617237091064 2052.856922149658 Probab: tensor(1.0047, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 312, LR: 7.8125, Train Loss: 2.0538, Train Accuracy: 34.00%, Temperatures:(0.01, 43.47)\n",
      "Old & New Losses 2055.88960647583 2050.485610961914 Probab: tensor(1.1324, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 313, LR: 7.8125, Train Loss: 2.0554, Train Accuracy: 33.80%, Temperatures:(0.01, 43.03)\n",
      "Old & New Losses 2053.481101989746 2067.319869995117 Probab: tensor(0.7250, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 314, LR: 7.8125, Train Loss: 2.0498, Train Accuracy: 33.60%, Temperatures:(0.01, 42.60)\n",
      "Old & New Losses 2055.5739402770996 2054.24427986145 Probab: tensor(1.0317, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 315, LR: 7.8125, Train Loss: 2.0600, Train Accuracy: 32.40%, Temperatures:(0.01, 42.18)\n",
      "Old & New Losses 2058.3834648132324 2063.131809234619 Probab: tensor(0.8935, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 316, LR: 7.8125, Train Loss: 2.0558, Train Accuracy: 32.10%, Temperatures:(0.01, 41.76)\n",
      "Old & New Losses 2055.6581020355225 2055.9957027435303 Probab: tensor(0.9919, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 317, LR: 7.8125, Train Loss: 2.0587, Train Accuracy: 33.60%, Temperatures:(0.01, 41.34)\n",
      "Old & New Losses 2060.5618953704834 2056.541919708252 Probab: tensor(1.1021, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 318, LR: 7.8125, Train Loss: 2.0561, Train Accuracy: 33.80%, Temperatures:(0.01, 40.93)\n",
      "Old & New Losses 2055.474281311035 2051.140546798706 Probab: tensor(1.1117, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 319, LR: 7.8125, Train Loss: 2.0566, Train Accuracy: 32.80%, Temperatures:(0.01, 40.52)\n",
      "Old & New Losses 2052.6647567749023 2076.530933380127 Probab: tensor(0.5549, device='cuda:0')\n",
      "Epoch 320, LR: 7.8125, Train Loss: 2.0533, Train Accuracy: 32.70%, Temperatures:(0.01, 40.11)\n",
      "Old & New Losses 2050.874948501587 2060.7149600982666 Probab: tensor(0.7825, device='cuda:0')\n",
      "Epoch 321, LR: 7.8125, Train Loss: 2.0529, Train Accuracy: 32.70%, Temperatures:(0.01, 39.71)\n",
      "Old & New Losses 2052.6669025421143 2055.687427520752 Probab: tensor(0.9268, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 322, LR: 7.8125, Train Loss: 2.0495, Train Accuracy: 32.70%, Temperatures:(0.01, 39.31)\n",
      "Old & New Losses 2054.0173053741455 2058.6984157562256 Probab: tensor(0.8877, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 323, LR: 7.8125, Train Loss: 2.0511, Train Accuracy: 33.20%, Temperatures:(0.01, 38.92)\n",
      "Old & New Losses 2053.539514541626 2049.419403076172 Probab: tensor(1.1117, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 324, LR: 7.8125, Train Loss: 2.0586, Train Accuracy: 32.50%, Temperatures:(0.01, 38.53)\n",
      "Old & New Losses 2044.5225238800049 2066.3483142852783 Probab: tensor(0.5675, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 325, LR: 7.8125, Train Loss: 2.0469, Train Accuracy: 33.80%, Temperatures:(0.01, 38.15)\n",
      "Old & New Losses 2047.9509830474854 2050.4515171051025 Probab: tensor(0.9365, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 326, LR: 7.8125, Train Loss: 2.0612, Train Accuracy: 32.40%, Temperatures:(0.01, 37.76)\n",
      "Old & New Losses 2044.9833869934082 2050.391435623169 Probab: tensor(0.8666, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 327, LR: 7.8125, Train Loss: 2.0507, Train Accuracy: 32.20%, Temperatures:(0.01, 37.39)\n",
      "Old & New Losses 2044.5449352264404 2044.5334911346436 Probab: tensor(1.0003, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 328, LR: 7.8125, Train Loss: 2.0455, Train Accuracy: 32.50%, Temperatures:(0.01, 37.01)\n",
      "Old & New Losses 2042.2368049621582 2042.1319007873535 Probab: tensor(1.0028, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 329, LR: 7.8125, Train Loss: 2.0422, Train Accuracy: 33.40%, Temperatures:(0.01, 36.64)\n",
      "Old & New Losses 2041.2635803222656 2050.3475666046143 Probab: tensor(0.7804, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 330, LR: 7.8125, Train Loss: 2.0373, Train Accuracy: 32.70%, Temperatures:(0.01, 36.28)\n",
      "Old & New Losses 2040.3919219970703 2046.9348430633545 Probab: tensor(0.8350, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 331, LR: 7.8125, Train Loss: 2.0478, Train Accuracy: 32.50%, Temperatures:(0.01, 35.91)\n",
      "Old & New Losses 2042.6650047302246 2047.5869178771973 Probab: tensor(0.8719, device='cuda:0')\n",
      "Epoch 332, LR: 7.8125, Train Loss: 2.0480, Train Accuracy: 31.50%, Temperatures:(0.01, 35.55)\n",
      "Old & New Losses 2044.0423488616943 2043.210744857788 Probab: tensor(1.0237, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 333, LR: 7.8125, Train Loss: 2.0464, Train Accuracy: 32.70%, Temperatures:(0.01, 35.20)\n",
      "Old & New Losses 2045.9527969360352 2063.981294631958 Probab: tensor(0.5992, device='cuda:0')\n",
      "Epoch 334, LR: 7.8125, Train Loss: 2.0424, Train Accuracy: 33.10%, Temperatures:(0.01, 34.85)\n",
      "Old & New Losses 2041.6843891143799 2035.4886054992676 Probab: tensor(1.1946, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 335, LR: 7.8125, Train Loss: 2.0494, Train Accuracy: 33.00%, Temperatures:(0.01, 34.50)\n",
      "Old & New Losses 2032.1528911590576 2041.3517951965332 Probab: tensor(0.7659, device='cuda:0')\n",
      "Epoch 336, LR: 7.8125, Train Loss: 2.0394, Train Accuracy: 34.20%, Temperatures:(0.01, 34.15)\n",
      "Old & New Losses 2033.0085754394531 2035.6919765472412 Probab: tensor(0.9244, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 337, LR: 7.8125, Train Loss: 2.0366, Train Accuracy: 33.20%, Temperatures:(0.01, 33.81)\n",
      "Old & New Losses 2031.2104225158691 2027.8737545013428 Probab: tensor(1.1037, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 338, LR: 7.8125, Train Loss: 2.0322, Train Accuracy: 32.80%, Temperatures:(0.01, 33.47)\n",
      "Old & New Losses 2038.895845413208 2053.3289909362793 Probab: tensor(0.6497, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 339, LR: 7.8125, Train Loss: 2.0304, Train Accuracy: 35.00%, Temperatures:(0.01, 33.14)\n",
      "Old & New Losses 2037.503719329834 2071.7391967773438 Probab: tensor(0.3559, device='cuda:0')\n",
      "Epoch 340, LR: 7.8125, Train Loss: 2.0387, Train Accuracy: 34.80%, Temperatures:(0.01, 32.81)\n",
      "Old & New Losses 2032.609224319458 2021.4314460754395 Probab: tensor(1.4060, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 341, LR: 7.8125, Train Loss: 2.0406, Train Accuracy: 33.10%, Temperatures:(0.01, 32.48)\n",
      "Old & New Losses 2022.4254131317139 2027.54807472229 Probab: tensor(0.8541, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 342, LR: 7.8125, Train Loss: 2.0238, Train Accuracy: 35.60%, Temperatures:(0.01, 32.15)\n",
      "Old & New Losses 2016.3850784301758 2022.4504470825195 Probab: tensor(0.8281, device='cuda:0')\n",
      "Epoch 343, LR: 7.8125, Train Loss: 2.0201, Train Accuracy: 35.30%, Temperatures:(0.01, 31.83)\n",
      "Old & New Losses 2016.1876678466797 2016.988754272461 Probab: tensor(0.9751, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 344, LR: 7.8125, Train Loss: 2.0162, Train Accuracy: 34.80%, Temperatures:(0.01, 31.51)\n",
      "Old & New Losses 2010.502815246582 2017.7786350250244 Probab: tensor(0.7938, device='cuda:0')\n",
      "Epoch 345, LR: 7.8125, Train Loss: 2.0132, Train Accuracy: 35.40%, Temperatures:(0.01, 31.20)\n",
      "Old & New Losses 2018.4192657470703 2010.725498199463 Probab: tensor(1.2797, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 346, LR: 7.8125, Train Loss: 2.0135, Train Accuracy: 36.10%, Temperatures:(0.01, 30.89)\n",
      "Old & New Losses 2012.6979351043701 2014.5263671875 Probab: tensor(0.9425, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 347, LR: 7.8125, Train Loss: 2.0137, Train Accuracy: 34.30%, Temperatures:(0.01, 30.58)\n",
      "Old & New Losses 2016.9792175292969 2017.3945426940918 Probab: tensor(0.9865, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 348, LR: 7.8125, Train Loss: 2.0122, Train Accuracy: 34.20%, Temperatures:(0.01, 30.27)\n",
      "Old & New Losses 2009.3028545379639 2005.3391456604004 Probab: tensor(1.1399, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 349, LR: 7.8125, Train Loss: 2.0173, Train Accuracy: 35.90%, Temperatures:(0.01, 29.97)\n",
      "Old & New Losses 2008.5501670837402 2007.4913501739502 Probab: tensor(1.0360, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 350, LR: 7.8125, Train Loss: 2.0113, Train Accuracy: 34.40%, Temperatures:(0.01, 29.67)\n",
      "Old & New Losses 2011.1804008483887 2014.1704082489014 Probab: tensor(0.9041, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 351, LR: 7.8125, Train Loss: 2.0104, Train Accuracy: 34.50%, Temperatures:(0.01, 29.37)\n",
      "Old & New Losses 2011.7225646972656 2014.979362487793 Probab: tensor(0.8950, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 352, LR: 3.9062, Train Loss: 2.0164, Train Accuracy: 31.70%, Temperatures:(0.01, 29.08)\n",
      "Old & New Losses 2012.8638744354248 2022.4010944366455 Probab: tensor(0.7204, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 353, LR: 3.9062, Train Loss: 2.0136, Train Accuracy: 33.20%, Temperatures:(0.01, 28.79)\n",
      "Old & New Losses 2013.1266117095947 2018.775463104248 Probab: tensor(0.8218, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 354, LR: 3.9062, Train Loss: 2.0191, Train Accuracy: 31.30%, Temperatures:(0.01, 28.50)\n",
      "Old & New Losses 2015.6910419464111 2017.5199508666992 Probab: tensor(0.9378, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 355, LR: 3.9062, Train Loss: 2.0196, Train Accuracy: 30.20%, Temperatures:(0.01, 28.22)\n",
      "Old & New Losses 2008.2464218139648 2014.1780376434326 Probab: tensor(0.8104, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 356, LR: 3.9062, Train Loss: 2.0170, Train Accuracy: 32.80%, Temperatures:(0.01, 27.93)\n",
      "Old & New Losses 2018.7819004058838 2025.7599353790283 Probab: tensor(0.7790, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 357, LR: 3.9062, Train Loss: 2.0212, Train Accuracy: 31.70%, Temperatures:(0.01, 27.65)\n",
      "Old & New Losses 2014.5773887634277 2021.259069442749 Probab: tensor(0.7854, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 358, LR: 3.9062, Train Loss: 2.0179, Train Accuracy: 32.00%, Temperatures:(0.01, 27.38)\n",
      "Old & New Losses 2009.2682838439941 2020.399808883667 Probab: tensor(0.6659, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 359, LR: 3.9062, Train Loss: 2.0105, Train Accuracy: 31.40%, Temperatures:(0.01, 27.10)\n",
      "Old & New Losses 2012.8538608551025 2014.374017715454 Probab: tensor(0.9455, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 360, LR: 3.9062, Train Loss: 2.0224, Train Accuracy: 33.40%, Temperatures:(0.01, 26.83)\n",
      "Old & New Losses 2016.688585281372 2020.6503868103027 Probab: tensor(0.8627, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 361, LR: 3.9062, Train Loss: 2.0245, Train Accuracy: 32.80%, Temperatures:(0.01, 26.56)\n",
      "Old & New Losses 2014.9688720703125 2021.5826034545898 Probab: tensor(0.7796, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 362, LR: 3.9062, Train Loss: 2.0248, Train Accuracy: 32.60%, Temperatures:(0.01, 26.30)\n",
      "Old & New Losses 2013.73291015625 2023.7414836883545 Probab: tensor(0.6835, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 363, LR: 3.9062, Train Loss: 2.0190, Train Accuracy: 33.90%, Temperatures:(0.01, 26.04)\n",
      "Old & New Losses 2016.1986351013184 2025.5463123321533 Probab: tensor(0.6984, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 364, LR: 3.9062, Train Loss: 2.0263, Train Accuracy: 33.30%, Temperatures:(0.01, 25.78)\n",
      "Old & New Losses 2017.2898769378662 2029.716968536377 Probab: tensor(0.6175, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 365, LR: 3.9062, Train Loss: 2.0199, Train Accuracy: 35.10%, Temperatures:(0.01, 25.52)\n",
      "Old & New Losses 2028.3286571502686 2028.266429901123 Probab: tensor(1.0024, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 366, LR: 3.9062, Train Loss: 2.0308, Train Accuracy: 33.70%, Temperatures:(0.01, 25.26)\n",
      "Old & New Losses 2025.2556800842285 2032.3693752288818 Probab: tensor(0.7546, device='cuda:0')\n",
      "Epoch 367, LR: 3.9062, Train Loss: 2.0320, Train Accuracy: 33.30%, Temperatures:(0.01, 25.01)\n",
      "Old & New Losses 2024.404764175415 2027.8186798095703 Probab: tensor(0.8724, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 368, LR: 3.9062, Train Loss: 2.0352, Train Accuracy: 34.10%, Temperatures:(0.00, 24.76)\n",
      "Old & New Losses 2024.6973037719727 2030.3618907928467 Probab: tensor(0.7955, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 369, LR: 3.9062, Train Loss: 2.0293, Train Accuracy: 33.50%, Temperatures:(0.00, 24.51)\n",
      "Old & New Losses 2026.7305374145508 2029.158115386963 Probab: tensor(0.9057, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 370, LR: 3.9062, Train Loss: 2.0321, Train Accuracy: 35.20%, Temperatures:(0.00, 24.27)\n",
      "Old & New Losses 2026.172161102295 2034.4579219818115 Probab: tensor(0.7107, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 371, LR: 3.9062, Train Loss: 2.0358, Train Accuracy: 35.30%, Temperatures:(0.00, 24.02)\n",
      "Old & New Losses 2028.0919075012207 2035.369634628296 Probab: tensor(0.7387, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 372, LR: 3.9062, Train Loss: 2.0313, Train Accuracy: 34.60%, Temperatures:(0.00, 23.78)\n",
      "Old & New Losses 2028.1171798706055 2034.468412399292 Probab: tensor(0.7656, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 373, LR: 3.9062, Train Loss: 2.0300, Train Accuracy: 34.90%, Temperatures:(0.00, 23.55)\n",
      "Old & New Losses 2035.9442234039307 2027.4128913879395 Probab: tensor(1.4367, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 374, LR: 3.9062, Train Loss: 2.0315, Train Accuracy: 35.60%, Temperatures:(0.00, 23.31)\n",
      "Old & New Losses 2023.7364768981934 2023.454189300537 Probab: tensor(1.0122, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 375, LR: 3.9062, Train Loss: 2.0358, Train Accuracy: 34.40%, Temperatures:(0.00, 23.08)\n",
      "Old & New Losses 2030.7157039642334 2040.4975414276123 Probab: tensor(0.6545, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 376, LR: 3.9062, Train Loss: 2.0264, Train Accuracy: 35.40%, Temperatures:(0.00, 22.85)\n",
      "Old & New Losses 2035.5548858642578 2048.4097003936768 Probab: tensor(0.5697, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 377, LR: 3.9062, Train Loss: 2.0402, Train Accuracy: 32.10%, Temperatures:(0.00, 22.62)\n",
      "Old & New Losses 2038.77854347229 2052.067518234253 Probab: tensor(0.5557, device='cuda:0')\n",
      "Epoch 378, LR: 3.9062, Train Loss: 2.0542, Train Accuracy: 33.90%, Temperatures:(0.00, 22.39)\n",
      "Old & New Losses 2039.501428604126 2041.6936874389648 Probab: tensor(0.9067, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 379, LR: 3.9062, Train Loss: 2.0441, Train Accuracy: 35.30%, Temperatures:(0.00, 22.17)\n",
      "Old & New Losses 2043.879747390747 2034.6951484680176 Probab: tensor(1.5133, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 380, LR: 3.9062, Train Loss: 2.0425, Train Accuracy: 35.50%, Temperatures:(0.00, 21.95)\n",
      "Old & New Losses 2024.3117809295654 2028.440237045288 Probab: tensor(0.8285, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 381, LR: 3.9062, Train Loss: 2.0265, Train Accuracy: 36.10%, Temperatures:(0.00, 21.73)\n",
      "Old & New Losses 2031.5954685211182 2033.7259769439697 Probab: tensor(0.9066, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 382, LR: 3.9062, Train Loss: 2.0306, Train Accuracy: 34.50%, Temperatures:(0.00, 21.51)\n",
      "Old & New Losses 2037.2743606567383 2029.3099880218506 Probab: tensor(1.4481, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 383, LR: 3.9062, Train Loss: 2.0358, Train Accuracy: 32.40%, Temperatures:(0.00, 21.30)\n",
      "Old & New Losses 2035.395622253418 2035.1736545562744 Probab: tensor(1.0105, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 384, LR: 3.9062, Train Loss: 2.0279, Train Accuracy: 35.40%, Temperatures:(0.00, 21.08)\n",
      "Old & New Losses 2031.8970680236816 2030.1969051361084 Probab: tensor(1.0840, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 385, LR: 3.9062, Train Loss: 2.0353, Train Accuracy: 33.10%, Temperatures:(0.00, 20.87)\n",
      "Old & New Losses 2028.91206741333 2034.4617366790771 Probab: tensor(0.7665, device='cuda:0')\n",
      "Epoch 386, LR: 3.9062, Train Loss: 2.0388, Train Accuracy: 33.40%, Temperatures:(0.00, 20.66)\n",
      "Old & New Losses 2035.949468612671 2033.1141948699951 Probab: tensor(1.1471, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 387, LR: 3.9062, Train Loss: 2.0304, Train Accuracy: 33.20%, Temperatures:(0.00, 20.46)\n",
      "Old & New Losses 2029.6905040740967 2025.909662246704 Probab: tensor(1.2030, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 388, LR: 3.9062, Train Loss: 2.0250, Train Accuracy: 34.80%, Temperatures:(0.00, 20.25)\n",
      "Old & New Losses 2029.231071472168 2030.074119567871 Probab: tensor(0.9592, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 389, LR: 3.9062, Train Loss: 2.0310, Train Accuracy: 32.30%, Temperatures:(0.00, 20.05)\n",
      "Old & New Losses 2030.8706760406494 2028.9061069488525 Probab: tensor(1.1030, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 390, LR: 3.9062, Train Loss: 2.0284, Train Accuracy: 33.40%, Temperatures:(0.00, 19.85)\n",
      "Old & New Losses 2026.747226715088 2031.4583778381348 Probab: tensor(0.7887, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 391, LR: 3.9062, Train Loss: 2.0198, Train Accuracy: 34.00%, Temperatures:(0.00, 19.65)\n",
      "Old & New Losses 2029.146671295166 2037.3733043670654 Probab: tensor(0.6579, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 392, LR: 3.9062, Train Loss: 2.0356, Train Accuracy: 33.60%, Temperatures:(0.00, 19.45)\n",
      "Old & New Losses 2029.128074645996 2033.9686870574951 Probab: tensor(0.7797, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 393, LR: 3.9062, Train Loss: 2.0359, Train Accuracy: 33.40%, Temperatures:(0.00, 19.26)\n",
      "Old & New Losses 2032.1965217590332 2035.4371070861816 Probab: tensor(0.8451, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 394, LR: 3.9062, Train Loss: 2.0330, Train Accuracy: 34.20%, Temperatures:(0.00, 19.07)\n",
      "Old & New Losses 2029.813289642334 2038.377046585083 Probab: tensor(0.6382, device='cuda:0')\n",
      "Epoch 395, LR: 3.9062, Train Loss: 2.0365, Train Accuracy: 33.70%, Temperatures:(0.00, 18.88)\n",
      "Old & New Losses 2035.646915435791 2027.7400016784668 Probab: tensor(1.5203, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 396, LR: 3.9062, Train Loss: 2.0341, Train Accuracy: 33.20%, Temperatures:(0.00, 18.69)\n",
      "Old & New Losses 2031.5871238708496 2031.8644046783447 Probab: tensor(0.9853, device='cuda:0')\n",
      "Epoch 397, LR: 3.9062, Train Loss: 2.0264, Train Accuracy: 34.00%, Temperatures:(0.00, 18.50)\n",
      "Old & New Losses 2026.026964187622 2027.2555351257324 Probab: tensor(0.9357, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 398, LR: 3.9062, Train Loss: 2.0241, Train Accuracy: 33.30%, Temperatures:(0.00, 18.32)\n",
      "Old & New Losses 2025.6531238555908 2026.8521308898926 Probab: tensor(0.9366, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 399, LR: 3.9062, Train Loss: 2.0273, Train Accuracy: 34.00%, Temperatures:(0.00, 18.13)\n",
      "Old & New Losses 2031.8357944488525 2027.3704528808594 Probab: tensor(1.2792, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 400, LR: 3.9062, Train Loss: 2.0282, Train Accuracy: 34.40%, Temperatures:(0.00, 17.95)\n",
      "Old & New Losses 2019.0587043762207 2030.7424068450928 Probab: tensor(0.5216, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 401, LR: 3.9062, Train Loss: 2.0359, Train Accuracy: 33.90%, Temperatures:(0.00, 17.77)\n",
      "Old & New Losses 2031.5065383911133 2031.1062335968018 Probab: tensor(1.0228, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 402, LR: 1.9531, Train Loss: 2.0266, Train Accuracy: 34.40%, Temperatures:(0.00, 17.59)\n",
      "Old & New Losses 2032.2809219360352 2041.5124893188477 Probab: tensor(0.5917, device='cuda:0')\n",
      "Epoch 403, LR: 1.9531, Train Loss: 2.0326, Train Accuracy: 35.00%, Temperatures:(0.00, 17.42)\n",
      "Old & New Losses 2030.827283859253 2056.746006011963 Probab: tensor(0.2258, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 404, LR: 1.9531, Train Loss: 2.0306, Train Accuracy: 33.50%, Temperatures:(0.00, 17.24)\n",
      "Old & New Losses 2042.6089763641357 2058.135747909546 Probab: tensor(0.4064, device='cuda:0')\n",
      "Epoch 405, LR: 1.9531, Train Loss: 2.0520, Train Accuracy: 28.70%, Temperatures:(0.00, 17.07)\n",
      "Old & New Losses 2042.0079231262207 2046.748399734497 Probab: tensor(0.7575, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 406, LR: 1.9531, Train Loss: 2.0521, Train Accuracy: 29.40%, Temperatures:(0.00, 16.90)\n",
      "Old & New Losses 2044.4414615631104 2057.7232837677 Probab: tensor(0.4557, device='cuda:0')\n",
      "Epoch 407, LR: 1.9531, Train Loss: 2.0455, Train Accuracy: 30.50%, Temperatures:(0.00, 16.73)\n",
      "Old & New Losses 2039.6842956542969 2050.037384033203 Probab: tensor(0.5386, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 408, LR: 1.9531, Train Loss: 2.0439, Train Accuracy: 31.30%, Temperatures:(0.00, 16.56)\n",
      "Old & New Losses 2041.245460510254 2054.516077041626 Probab: tensor(0.4488, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 409, LR: 1.9531, Train Loss: 2.0472, Train Accuracy: 30.30%, Temperatures:(0.00, 16.40)\n",
      "Old & New Losses 2042.2945022583008 2050.6515502929688 Probab: tensor(0.6007, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 410, LR: 1.9531, Train Loss: 2.0478, Train Accuracy: 31.40%, Temperatures:(0.00, 16.23)\n",
      "Old & New Losses 2054.68487739563 2055.4075241088867 Probab: tensor(0.9565, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 411, LR: 1.9531, Train Loss: 2.0505, Train Accuracy: 30.90%, Temperatures:(0.00, 16.07)\n",
      "Old & New Losses 2052.462577819824 2051.983594894409 Probab: tensor(1.0303, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 412, LR: 1.9531, Train Loss: 2.0566, Train Accuracy: 31.80%, Temperatures:(0.00, 15.91)\n",
      "Old & New Losses 2048.1514930725098 2058.6507320404053 Probab: tensor(0.5169, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 413, LR: 1.9531, Train Loss: 2.0458, Train Accuracy: 31.80%, Temperatures:(0.00, 15.75)\n",
      "Old & New Losses 2046.088457107544 2057.7688217163086 Probab: tensor(0.4764, device='cuda:0')\n",
      "Epoch 414, LR: 1.9531, Train Loss: 2.0525, Train Accuracy: 29.60%, Temperatures:(0.00, 15.59)\n",
      "Old & New Losses 2042.360782623291 2056.5476417541504 Probab: tensor(0.4026, device='cuda:0')\n",
      "Epoch 415, LR: 1.9531, Train Loss: 2.0499, Train Accuracy: 31.30%, Temperatures:(0.00, 15.44)\n",
      "Old & New Losses 2054.07977104187 2046.2899208068848 Probab: tensor(1.6563, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 416, LR: 1.9531, Train Loss: 2.0451, Train Accuracy: 31.50%, Temperatures:(0.00, 15.28)\n",
      "Old & New Losses 2053.5027980804443 2053.8880825042725 Probab: tensor(0.9751, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 417, LR: 1.9531, Train Loss: 2.0483, Train Accuracy: 31.20%, Temperatures:(0.00, 15.13)\n",
      "Old & New Losses 2050.708293914795 2053.173542022705 Probab: tensor(0.8497, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 418, LR: 1.9531, Train Loss: 2.0514, Train Accuracy: 32.40%, Temperatures:(0.00, 14.98)\n",
      "Old & New Losses 2047.931432723999 2044.8684692382812 Probab: tensor(1.2269, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 419, LR: 1.9531, Train Loss: 2.0596, Train Accuracy: 33.50%, Temperatures:(0.00, 14.83)\n",
      "Old & New Losses 2047.0287799835205 2048.8243103027344 Probab: tensor(0.8860, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 420, LR: 1.9531, Train Loss: 2.0448, Train Accuracy: 33.70%, Temperatures:(0.00, 14.68)\n",
      "Old & New Losses 2050.1773357391357 2054.056406021118 Probab: tensor(0.7678, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 421, LR: 1.9531, Train Loss: 2.0514, Train Accuracy: 33.10%, Temperatures:(0.00, 14.54)\n",
      "Old & New Losses 2047.6539134979248 2050.830125808716 Probab: tensor(0.8037, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 422, LR: 1.9531, Train Loss: 2.0539, Train Accuracy: 33.70%, Temperatures:(0.00, 14.39)\n",
      "Old & New Losses 2054.649829864502 2056.3535690307617 Probab: tensor(0.8883, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 423, LR: 1.9531, Train Loss: 2.0495, Train Accuracy: 33.70%, Temperatures:(0.00, 14.25)\n",
      "Old & New Losses 2050.522565841675 2051.2588024139404 Probab: tensor(0.9496, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 424, LR: 1.9531, Train Loss: 2.0531, Train Accuracy: 33.40%, Temperatures:(0.00, 14.10)\n",
      "Old & New Losses 2047.173261642456 2052.745819091797 Probab: tensor(0.6736, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 425, LR: 1.9531, Train Loss: 2.0571, Train Accuracy: 33.60%, Temperatures:(0.00, 13.96)\n",
      "Old & New Losses 2051.769495010376 2050.891637802124 Probab: tensor(1.0649, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 426, LR: 1.9531, Train Loss: 2.0572, Train Accuracy: 34.20%, Temperatures:(0.00, 13.82)\n",
      "Old & New Losses 2049.8757362365723 2073.9057064056396 Probab: tensor(0.1758, device='cuda:0')\n",
      "Epoch 427, LR: 1.9531, Train Loss: 2.0541, Train Accuracy: 33.40%, Temperatures:(0.00, 13.68)\n",
      "Old & New Losses 2053.124189376831 2056.7550659179688 Probab: tensor(0.7670, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 428, LR: 1.9531, Train Loss: 2.0499, Train Accuracy: 34.70%, Temperatures:(0.00, 13.55)\n",
      "Old & New Losses 2051.7337322235107 2044.6195602416992 Probab: tensor(1.6907, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 429, LR: 1.9531, Train Loss: 2.0576, Train Accuracy: 34.50%, Temperatures:(0.00, 13.41)\n",
      "Old & New Losses 2044.4250106811523 2049.680233001709 Probab: tensor(0.6758, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 430, LR: 1.9531, Train Loss: 2.0523, Train Accuracy: 34.20%, Temperatures:(0.00, 13.28)\n",
      "Old & New Losses 2049.5266914367676 2047.2180843353271 Probab: tensor(1.1899, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 431, LR: 1.9531, Train Loss: 2.0553, Train Accuracy: 34.60%, Temperatures:(0.00, 13.15)\n",
      "Old & New Losses 2040.1215553283691 2049.288511276245 Probab: tensor(0.4979, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 432, LR: 1.9531, Train Loss: 2.0473, Train Accuracy: 36.10%, Temperatures:(0.00, 13.01)\n",
      "Old & New Losses 2047.7125644683838 2042.940378189087 Probab: tensor(1.4430, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 433, LR: 1.9531, Train Loss: 2.0449, Train Accuracy: 34.80%, Temperatures:(0.00, 12.88)\n",
      "Old & New Losses 2042.736291885376 2044.6631908416748 Probab: tensor(0.8611, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 434, LR: 1.9531, Train Loss: 2.0510, Train Accuracy: 35.90%, Temperatures:(0.00, 12.75)\n",
      "Old & New Losses 2041.4998531341553 2041.6698455810547 Probab: tensor(0.9868, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 435, LR: 1.9531, Train Loss: 2.0450, Train Accuracy: 34.80%, Temperatures:(0.00, 12.63)\n",
      "Old & New Losses 2045.4206466674805 2042.8528785705566 Probab: tensor(1.2255, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 436, LR: 1.9531, Train Loss: 2.0451, Train Accuracy: 35.00%, Temperatures:(0.00, 12.50)\n",
      "Old & New Losses 2044.757604598999 2037.0211601257324 Probab: tensor(1.8568, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 437, LR: 1.9531, Train Loss: 2.0427, Train Accuracy: 34.60%, Temperatures:(0.00, 12.38)\n",
      "Old & New Losses 2043.9183712005615 2040.1806831359863 Probab: tensor(1.3526, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 438, LR: 1.9531, Train Loss: 2.0423, Train Accuracy: 34.70%, Temperatures:(0.00, 12.25)\n",
      "Old & New Losses 2038.6240482330322 2037.4548435211182 Probab: tensor(1.1001, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 439, LR: 1.9531, Train Loss: 2.0425, Train Accuracy: 34.60%, Temperatures:(0.00, 12.13)\n",
      "Old & New Losses 2039.1573905944824 2031.921148300171 Probab: tensor(1.8159, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 440, LR: 1.9531, Train Loss: 2.0396, Train Accuracy: 35.20%, Temperatures:(0.00, 12.01)\n",
      "Old & New Losses 2029.6783447265625 2036.8309020996094 Probab: tensor(0.5512, device='cuda:0')\n",
      "Epoch 441, LR: 1.9531, Train Loss: 2.0353, Train Accuracy: 34.80%, Temperatures:(0.00, 11.89)\n",
      "Old & New Losses 2035.8846187591553 2038.6929512023926 Probab: tensor(0.7896, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 442, LR: 1.9531, Train Loss: 2.0360, Train Accuracy: 36.40%, Temperatures:(0.00, 11.77)\n",
      "Old & New Losses 2031.0704708099365 2042.0842170715332 Probab: tensor(0.3923, device='cuda:0')\n",
      "Epoch 443, LR: 1.9531, Train Loss: 2.0361, Train Accuracy: 35.00%, Temperatures:(0.00, 11.65)\n",
      "Old & New Losses 2028.8429260253906 2038.3224487304688 Probab: tensor(0.4433, device='cuda:0')\n",
      "Epoch 444, LR: 1.9531, Train Loss: 2.0352, Train Accuracy: 33.80%, Temperatures:(0.00, 11.54)\n",
      "Old & New Losses 2034.2895984649658 2037.116289138794 Probab: tensor(0.7827, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 445, LR: 1.9531, Train Loss: 2.0371, Train Accuracy: 35.10%, Temperatures:(0.00, 11.42)\n",
      "Old & New Losses 2037.4445915222168 2028.5818576812744 Probab: tensor(2.1729, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 446, LR: 1.9531, Train Loss: 2.0280, Train Accuracy: 35.80%, Temperatures:(0.00, 11.31)\n",
      "Old & New Losses 2028.1190872192383 2025.3102779388428 Probab: tensor(1.2820, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 447, LR: 1.9531, Train Loss: 2.0286, Train Accuracy: 34.70%, Temperatures:(0.00, 11.19)\n",
      "Old & New Losses 2025.601863861084 2033.3728790283203 Probab: tensor(0.4994, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 448, LR: 1.9531, Train Loss: 2.0316, Train Accuracy: 34.00%, Temperatures:(0.00, 11.08)\n",
      "Old & New Losses 2028.2845497131348 2029.4616222381592 Probab: tensor(0.8992, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 449, LR: 1.9531, Train Loss: 2.0256, Train Accuracy: 35.90%, Temperatures:(0.00, 10.97)\n",
      "Old & New Losses 2031.0637950897217 2031.3129425048828 Probab: tensor(0.9775, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 450, LR: 1.9531, Train Loss: 2.0287, Train Accuracy: 34.30%, Temperatures:(0.00, 10.86)\n",
      "Old & New Losses 2033.8850021362305 2028.4149646759033 Probab: tensor(1.6548, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 451, LR: 1.9531, Train Loss: 2.0276, Train Accuracy: 34.90%, Temperatures:(0.00, 10.75)\n",
      "Old & New Losses 2029.9937725067139 2034.9717140197754 Probab: tensor(0.6294, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 452, LR: 0.9766, Train Loss: 2.0326, Train Accuracy: 36.70%, Temperatures:(0.00, 10.64)\n",
      "Old & New Losses 2029.0114879608154 2030.1918983459473 Probab: tensor(0.8950, device='cuda:0')\n",
      "Epoch 453, LR: 0.9766, Train Loss: 2.0346, Train Accuracy: 36.10%, Temperatures:(0.00, 10.54)\n",
      "Old & New Losses 2026.3309478759766 2032.8576564788818 Probab: tensor(0.5383, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 454, LR: 0.9766, Train Loss: 2.0321, Train Accuracy: 35.90%, Temperatures:(0.00, 10.43)\n",
      "Old & New Losses 2036.919355392456 2044.4433689117432 Probab: tensor(0.4862, device='cuda:0')\n",
      "Epoch 455, LR: 0.9766, Train Loss: 2.0301, Train Accuracy: 36.10%, Temperatures:(0.00, 10.33)\n",
      "Old & New Losses 2034.827709197998 2030.2979946136475 Probab: tensor(1.5505, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 456, LR: 0.9766, Train Loss: 2.0373, Train Accuracy: 36.00%, Temperatures:(0.00, 10.22)\n",
      "Old & New Losses 2034.7490310668945 2038.4397506713867 Probab: tensor(0.6970, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 457, LR: 0.9766, Train Loss: 2.0324, Train Accuracy: 36.50%, Temperatures:(0.00, 10.12)\n",
      "Old & New Losses 2034.2183113098145 2037.7006530761719 Probab: tensor(0.7089, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 458, LR: 0.9766, Train Loss: 2.0331, Train Accuracy: 34.90%, Temperatures:(0.00, 10.02)\n",
      "Old & New Losses 2032.4430465698242 2032.165288925171 Probab: tensor(1.0281, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 459, LR: 0.9766, Train Loss: 2.0375, Train Accuracy: 34.10%, Temperatures:(0.00, 9.92)\n",
      "Old & New Losses 2034.8896980285645 2031.0795307159424 Probab: tensor(1.4682, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 460, LR: 0.9766, Train Loss: 2.0323, Train Accuracy: 34.90%, Temperatures:(0.00, 9.82)\n",
      "Old & New Losses 2030.992031097412 2022.8066444396973 Probab: tensor(2.3011, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 461, LR: 0.9766, Train Loss: 2.0290, Train Accuracy: 35.20%, Temperatures:(0.00, 9.72)\n",
      "Old & New Losses 2029.3455123901367 2037.2190475463867 Probab: tensor(0.4450, device='cuda:0')\n",
      "Epoch 462, LR: 0.9766, Train Loss: 2.0418, Train Accuracy: 35.20%, Temperatures:(0.00, 9.63)\n",
      "Old & New Losses 2029.3290615081787 2033.6356163024902 Probab: tensor(0.6393, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 463, LR: 0.9766, Train Loss: 2.0319, Train Accuracy: 36.80%, Temperatures:(0.00, 9.53)\n",
      "Old & New Losses 2033.8938236236572 2036.9987487792969 Probab: tensor(0.7219, device='cuda:0')\n",
      "Epoch 464, LR: 0.9766, Train Loss: 2.0332, Train Accuracy: 36.40%, Temperatures:(0.00, 9.43)\n",
      "Old & New Losses 2036.0538959503174 2027.8351306915283 Probab: tensor(2.3896, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 465, LR: 0.9766, Train Loss: 2.0338, Train Accuracy: 37.40%, Temperatures:(0.00, 9.34)\n",
      "Old & New Losses 2026.5462398529053 2027.2095203399658 Probab: tensor(0.9315, device='cuda:0')\n",
      "Epoch 466, LR: 0.9766, Train Loss: 2.0291, Train Accuracy: 38.10%, Temperatures:(0.00, 9.25)\n",
      "Old & New Losses 2024.672269821167 2026.2160301208496 Probab: tensor(0.8462, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 467, LR: 0.9766, Train Loss: 2.0302, Train Accuracy: 37.30%, Temperatures:(0.00, 9.15)\n",
      "Old & New Losses 2025.7821083068848 2028.6359786987305 Probab: tensor(0.7322, device='cuda:0')\n",
      "Epoch 468, LR: 0.9766, Train Loss: 2.0256, Train Accuracy: 38.90%, Temperatures:(0.00, 9.06)\n",
      "Old & New Losses 2024.9121189117432 2029.8409461975098 Probab: tensor(0.5805, device='cuda:0')\n",
      "Epoch 469, LR: 0.9766, Train Loss: 2.0279, Train Accuracy: 37.90%, Temperatures:(0.00, 8.97)\n",
      "Old & New Losses 2025.087833404541 2025.4783630371094 Probab: tensor(0.9574, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 470, LR: 0.9766, Train Loss: 2.0263, Train Accuracy: 37.90%, Temperatures:(0.00, 8.88)\n",
      "Old & New Losses 2025.4151821136475 2017.2457695007324 Probab: tensor(2.5086, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 471, LR: 0.9766, Train Loss: 2.0203, Train Accuracy: 38.70%, Temperatures:(0.00, 8.79)\n",
      "Old & New Losses 2020.4565525054932 2021.0766792297363 Probab: tensor(0.9319, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 472, LR: 0.9766, Train Loss: 2.0273, Train Accuracy: 38.20%, Temperatures:(0.00, 8.71)\n",
      "Old & New Losses 2025.0260829925537 2018.3289051055908 Probab: tensor(2.1582, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 473, LR: 0.9766, Train Loss: 2.0236, Train Accuracy: 38.80%, Temperatures:(0.00, 8.62)\n",
      "Old & New Losses 2018.6262130737305 2026.289463043213 Probab: tensor(0.4110, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 474, LR: 0.9766, Train Loss: 2.0184, Train Accuracy: 38.90%, Temperatures:(0.00, 8.53)\n",
      "Old & New Losses 2026.0233879089355 2026.698112487793 Probab: tensor(0.9240, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 475, LR: 0.9766, Train Loss: 2.0248, Train Accuracy: 40.00%, Temperatures:(0.00, 8.45)\n",
      "Old & New Losses 2024.646520614624 2031.6970348358154 Probab: tensor(0.4340, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 476, LR: 0.9766, Train Loss: 2.0230, Train Accuracy: 38.40%, Temperatures:(0.00, 8.36)\n",
      "Old & New Losses 2024.0018367767334 2048.391580581665 Probab: tensor(0.0541, device='cuda:0')\n",
      "Epoch 477, LR: 0.9766, Train Loss: 2.0283, Train Accuracy: 38.50%, Temperatures:(0.00, 8.28)\n",
      "Old & New Losses 2026.778221130371 2034.447431564331 Probab: tensor(0.3960, device='cuda:0')\n",
      "Epoch 478, LR: 0.9766, Train Loss: 2.0341, Train Accuracy: 36.70%, Temperatures:(0.00, 8.20)\n",
      "Old & New Losses 2029.9670696258545 2030.240535736084 Probab: tensor(0.9672, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 479, LR: 0.9766, Train Loss: 2.0302, Train Accuracy: 39.70%, Temperatures:(0.00, 8.11)\n",
      "Old & New Losses 2027.6575088500977 2029.0098190307617 Probab: tensor(0.8465, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 480, LR: 0.9766, Train Loss: 2.0347, Train Accuracy: 37.40%, Temperatures:(0.00, 8.03)\n",
      "Old & New Losses 2039.7214889526367 2026.8828868865967 Probab: tensor(4.9440, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 481, LR: 0.9766, Train Loss: 2.0337, Train Accuracy: 37.60%, Temperatures:(0.00, 7.95)\n",
      "Old & New Losses 2026.4737606048584 2029.8280715942383 Probab: tensor(0.6559, device='cuda:0')\n",
      "Epoch 482, LR: 0.9766, Train Loss: 2.0310, Train Accuracy: 38.40%, Temperatures:(0.00, 7.87)\n",
      "Old & New Losses 2030.3783416748047 2025.3565311431885 Probab: tensor(1.8923, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 483, LR: 0.9766, Train Loss: 2.0267, Train Accuracy: 38.20%, Temperatures:(0.00, 7.79)\n",
      "Old & New Losses 2027.778148651123 2032.562017440796 Probab: tensor(0.5413, device='cuda:0')\n",
      "Epoch 484, LR: 0.9766, Train Loss: 2.0339, Train Accuracy: 38.50%, Temperatures:(0.00, 7.72)\n",
      "Old & New Losses 2032.0467948913574 2027.2579193115234 Probab: tensor(1.8600, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 485, LR: 0.9766, Train Loss: 2.0303, Train Accuracy: 38.70%, Temperatures:(0.00, 7.64)\n",
      "Old & New Losses 2023.8137245178223 2035.5372428894043 Probab: tensor(0.2155, device='cuda:0')\n",
      "Epoch 486, LR: 0.9766, Train Loss: 2.0312, Train Accuracy: 38.50%, Temperatures:(0.00, 7.56)\n",
      "Old & New Losses 2028.94926071167 2024.9056816101074 Probab: tensor(1.7068, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 487, LR: 0.9766, Train Loss: 2.0286, Train Accuracy: 37.80%, Temperatures:(0.00, 7.49)\n",
      "Old & New Losses 2027.3292064666748 2028.2974243164062 Probab: tensor(0.8787, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 488, LR: 0.9766, Train Loss: 2.0282, Train Accuracy: 38.70%, Temperatures:(0.00, 7.41)\n",
      "Old & New Losses 2028.132438659668 2026.7095565795898 Probab: tensor(1.2116, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 489, LR: 0.9766, Train Loss: 2.0323, Train Accuracy: 37.50%, Temperatures:(0.00, 7.34)\n",
      "Old & New Losses 2029.8373699188232 2028.0439853668213 Probab: tensor(1.2768, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 490, LR: 0.9766, Train Loss: 2.0285, Train Accuracy: 38.80%, Temperatures:(0.00, 7.27)\n",
      "Old & New Losses 2023.479700088501 2025.4693031311035 Probab: tensor(0.7604, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 491, LR: 0.9766, Train Loss: 2.0211, Train Accuracy: 38.80%, Temperatures:(0.00, 7.19)\n",
      "Old & New Losses 2022.4480628967285 2026.69358253479 Probab: tensor(0.5542, device='cuda:0')\n",
      "Epoch 492, LR: 0.9766, Train Loss: 2.0262, Train Accuracy: 38.50%, Temperatures:(0.00, 7.12)\n",
      "Old & New Losses 2028.221845626831 2023.611307144165 Probab: tensor(1.9107, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 493, LR: 0.9766, Train Loss: 2.0260, Train Accuracy: 38.00%, Temperatures:(0.00, 7.05)\n",
      "Old & New Losses 2024.9829292297363 2029.9046039581299 Probab: tensor(0.4975, device='cuda:0')\n",
      "Epoch 494, LR: 0.9766, Train Loss: 2.0251, Train Accuracy: 39.30%, Temperatures:(0.00, 6.98)\n",
      "Old & New Losses 2024.763822555542 2029.801607131958 Probab: tensor(0.4858, device='cuda:0')\n",
      "Epoch 495, LR: 0.9766, Train Loss: 2.0290, Train Accuracy: 38.40%, Temperatures:(0.00, 6.91)\n",
      "Old & New Losses 2030.4877758026123 2029.4876098632812 Probab: tensor(1.1558, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 496, LR: 0.9766, Train Loss: 2.0244, Train Accuracy: 39.50%, Temperatures:(0.00, 6.84)\n",
      "Old & New Losses 2024.543046951294 2022.0692157745361 Probab: tensor(1.4357, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 497, LR: 0.9766, Train Loss: 2.0264, Train Accuracy: 38.70%, Temperatures:(0.00, 6.77)\n",
      "Old & New Losses 2030.1532745361328 2024.9149799346924 Probab: tensor(2.1675, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 498, LR: 0.9766, Train Loss: 2.0293, Train Accuracy: 37.50%, Temperatures:(0.00, 6.70)\n",
      "Old & New Losses 2022.7253437042236 2025.7608890533447 Probab: tensor(0.6358, device='cuda:0')\n",
      "Epoch 499, LR: 0.9766, Train Loss: 2.0272, Train Accuracy: 38.50%, Temperatures:(0.00, 6.64)\n",
      "Old & New Losses 2026.066541671753 2022.921085357666 Probab: tensor(1.6063, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 500, LR: 0.9766, Train Loss: 2.0297, Train Accuracy: 37.40%, Temperatures:(0.00, 6.57)\n",
      "Old & New Losses 2022.4025249481201 2024.0235328674316 Probab: tensor(0.7814, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 501, LR: 0.9766, Train Loss: 2.0265, Train Accuracy: 38.10%, Temperatures:(0.00, 6.50)\n",
      "Old & New Losses 2015.0971412658691 2025.9904861450195 Probab: tensor(0.1874, device='cuda:0')\n",
      "Epoch 502, LR: 0.4883, Train Loss: 2.0208, Train Accuracy: 38.60%, Temperatures:(0.00, 6.44)\n",
      "Old & New Losses 2028.2602310180664 2026.714563369751 Probab: tensor(1.2713, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 503, LR: 0.4883, Train Loss: 2.0258, Train Accuracy: 37.20%, Temperatures:(0.00, 6.38)\n",
      "Old & New Losses 2025.7282257080078 2020.5442905426025 Probab: tensor(2.2549, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 504, LR: 0.4883, Train Loss: 2.0249, Train Accuracy: 37.70%, Temperatures:(0.00, 6.31)\n",
      "Old & New Losses 2019.7937488555908 2021.5122699737549 Probab: tensor(0.7616, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 505, LR: 0.4883, Train Loss: 2.0256, Train Accuracy: 36.50%, Temperatures:(0.00, 6.25)\n",
      "Old & New Losses 2022.4931240081787 2027.906894683838 Probab: tensor(0.4205, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 506, LR: 0.4883, Train Loss: 2.0258, Train Accuracy: 35.90%, Temperatures:(0.00, 6.19)\n",
      "Old & New Losses 2028.1586647033691 2026.0891914367676 Probab: tensor(1.3973, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 507, LR: 0.4883, Train Loss: 2.0228, Train Accuracy: 37.60%, Temperatures:(0.00, 6.12)\n",
      "Old & New Losses 2033.4701538085938 2026.9577503204346 Probab: tensor(2.8962, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 508, LR: 0.4883, Train Loss: 2.0266, Train Accuracy: 38.70%, Temperatures:(0.00, 6.06)\n",
      "Old & New Losses 2034.0008735656738 2028.8219451904297 Probab: tensor(2.3495, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 509, LR: 0.4883, Train Loss: 2.0232, Train Accuracy: 35.20%, Temperatures:(0.00, 6.00)\n",
      "Old & New Losses 2025.6974697113037 2047.9505062103271 Probab: tensor(0.0245, device='cuda:0')\n",
      "Epoch 510, LR: 0.4883, Train Loss: 2.0246, Train Accuracy: 38.50%, Temperatures:(0.00, 5.94)\n",
      "Old & New Losses 2030.4772853851318 2028.8541316986084 Probab: tensor(1.3141, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 511, LR: 0.4883, Train Loss: 2.0285, Train Accuracy: 37.70%, Temperatures:(0.00, 5.88)\n",
      "Old & New Losses 2037.412405014038 2028.6016464233398 Probab: tensor(4.4715, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 512, LR: 0.4883, Train Loss: 2.0295, Train Accuracy: 36.90%, Temperatures:(0.00, 5.82)\n",
      "Old & New Losses 2033.6360931396484 2044.2733764648438 Probab: tensor(0.1610, device='cuda:0')\n",
      "Epoch 513, LR: 0.4883, Train Loss: 2.0311, Train Accuracy: 36.10%, Temperatures:(0.00, 5.77)\n",
      "Old & New Losses 2030.1322937011719 2046.5428829193115 Probab: tensor(0.0581, device='cuda:0')\n",
      "Epoch 514, LR: 0.4883, Train Loss: 2.0299, Train Accuracy: 36.40%, Temperatures:(0.00, 5.71)\n",
      "Old & New Losses 2028.6757946014404 2024.4932174682617 Probab: tensor(2.0808, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 515, LR: 0.4883, Train Loss: 2.0335, Train Accuracy: 35.40%, Temperatures:(0.00, 5.65)\n",
      "Old & New Losses 2031.1861038208008 2036.3469123840332 Probab: tensor(0.4012, device='cuda:0')\n",
      "Epoch 516, LR: 0.4883, Train Loss: 2.0328, Train Accuracy: 36.30%, Temperatures:(0.00, 5.59)\n",
      "Old & New Losses 2024.6338844299316 2037.705421447754 Probab: tensor(0.0967, device='cuda:0')\n",
      "Epoch 517, LR: 0.4883, Train Loss: 2.0285, Train Accuracy: 35.60%, Temperatures:(0.00, 5.54)\n",
      "Old & New Losses 2024.3310928344727 2034.562349319458 Probab: tensor(0.1577, device='cuda:0')\n",
      "Epoch 518, LR: 0.4883, Train Loss: 2.0275, Train Accuracy: 36.30%, Temperatures:(0.00, 5.48)\n",
      "Old & New Losses 2035.128116607666 2039.7112369537354 Probab: tensor(0.4335, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 519, LR: 0.4883, Train Loss: 2.0253, Train Accuracy: 36.60%, Temperatures:(0.00, 5.43)\n",
      "Old & New Losses 2035.1848602294922 2033.9064598083496 Probab: tensor(1.2655, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 520, LR: 0.4883, Train Loss: 2.0300, Train Accuracy: 34.20%, Temperatures:(0.00, 5.37)\n",
      "Old & New Losses 2031.4013957977295 2038.5105609893799 Probab: tensor(0.2664, device='cuda:0')\n",
      "Epoch 521, LR: 0.4883, Train Loss: 2.0428, Train Accuracy: 34.50%, Temperatures:(0.00, 5.32)\n",
      "Old & New Losses 2035.6981754302979 2035.25972366333 Probab: tensor(1.0859, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 522, LR: 0.4883, Train Loss: 2.0345, Train Accuracy: 35.10%, Temperatures:(0.00, 5.27)\n",
      "Old & New Losses 2035.9790325164795 2041.8980121612549 Probab: tensor(0.3251, device='cuda:0')\n",
      "Epoch 523, LR: 0.4883, Train Loss: 2.0372, Train Accuracy: 35.10%, Temperatures:(0.00, 5.21)\n",
      "Old & New Losses 2030.4815769195557 2036.1289978027344 Probab: tensor(0.3386, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 524, LR: 0.4883, Train Loss: 2.0361, Train Accuracy: 35.70%, Temperatures:(0.00, 5.16)\n",
      "Old & New Losses 2031.6038131713867 2045.8054542541504 Probab: tensor(0.0639, device='cuda:0')\n",
      "Epoch 525, LR: 0.4883, Train Loss: 2.0384, Train Accuracy: 34.20%, Temperatures:(0.00, 5.11)\n",
      "Old & New Losses 2035.6273651123047 2041.3002967834473 Probab: tensor(0.3296, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 526, LR: 0.4883, Train Loss: 2.0358, Train Accuracy: 35.60%, Temperatures:(0.00, 5.06)\n",
      "Old & New Losses 2036.3175868988037 2040.5983924865723 Probab: tensor(0.4291, device='cuda:0')\n",
      "Epoch 527, LR: 0.4883, Train Loss: 2.0366, Train Accuracy: 35.00%, Temperatures:(0.00, 5.01)\n",
      "Old & New Losses 2035.3305339813232 2043.0281162261963 Probab: tensor(0.2151, device='cuda:0')\n",
      "Epoch 528, LR: 0.4883, Train Loss: 2.0359, Train Accuracy: 38.50%, Temperatures:(0.00, 4.96)\n",
      "Old & New Losses 2034.9483489990234 2045.8598136901855 Probab: tensor(0.1108, device='cuda:0')\n",
      "Epoch 529, LR: 0.4883, Train Loss: 2.0399, Train Accuracy: 37.50%, Temperatures:(0.00, 4.91)\n",
      "Old & New Losses 2032.3503017425537 2032.0069789886475 Probab: tensor(1.0724, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 530, LR: 0.4883, Train Loss: 2.0373, Train Accuracy: 36.90%, Temperatures:(0.00, 4.86)\n",
      "Old & New Losses 2027.8656482696533 2035.07661819458 Probab: tensor(0.2268, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 531, LR: 0.4883, Train Loss: 2.0303, Train Accuracy: 36.20%, Temperatures:(0.00, 4.81)\n",
      "Old & New Losses 2033.3325862884521 2040.3730869293213 Probab: tensor(0.2315, device='cuda:0')\n",
      "Epoch 532, LR: 0.4883, Train Loss: 2.0308, Train Accuracy: 34.40%, Temperatures:(0.00, 4.76)\n",
      "Old & New Losses 2026.538610458374 2030.5097103118896 Probab: tensor(0.4345, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 533, LR: 0.4883, Train Loss: 2.0287, Train Accuracy: 35.10%, Temperatures:(0.00, 4.72)\n",
      "Old & New Losses 2030.9813022613525 2041.1434173583984 Probab: tensor(0.1159, device='cuda:0')\n",
      "Epoch 534, LR: 0.4883, Train Loss: 2.0322, Train Accuracy: 36.00%, Temperatures:(0.00, 4.67)\n",
      "Old & New Losses 2033.4687232971191 2026.3607501983643 Probab: tensor(4.5836, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 535, LR: 0.4883, Train Loss: 2.0345, Train Accuracy: 35.80%, Temperatures:(0.00, 4.62)\n",
      "Old & New Losses 2033.0636501312256 2023.8888263702393 Probab: tensor(7.2793, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 536, LR: 0.4883, Train Loss: 2.0267, Train Accuracy: 35.50%, Temperatures:(0.00, 4.58)\n",
      "Old & New Losses 2032.665491104126 2026.0841846466064 Probab: tensor(4.2135, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 537, LR: 0.4883, Train Loss: 2.0252, Train Accuracy: 36.10%, Temperatures:(0.00, 4.53)\n",
      "Old & New Losses 2022.327184677124 2038.184642791748 Probab: tensor(0.0302, device='cuda:0')\n",
      "Epoch 538, LR: 0.4883, Train Loss: 2.0336, Train Accuracy: 34.30%, Temperatures:(0.00, 4.48)\n",
      "Old & New Losses 2018.9619064331055 2025.5331993103027 Probab: tensor(0.2310, device='cuda:0')\n",
      "Epoch 539, LR: 0.4883, Train Loss: 2.0261, Train Accuracy: 35.90%, Temperatures:(0.00, 4.44)\n",
      "Old & New Losses 2023.3895778656006 2026.071310043335 Probab: tensor(0.5466, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 540, LR: 0.4883, Train Loss: 2.0309, Train Accuracy: 36.10%, Temperatures:(0.00, 4.40)\n",
      "Old & New Losses 2025.418996810913 2029.4826030731201 Probab: tensor(0.3967, device='cuda:0')\n",
      "Epoch 541, LR: 0.4883, Train Loss: 2.0263, Train Accuracy: 36.10%, Temperatures:(0.00, 4.35)\n",
      "Old & New Losses 2020.9550857543945 2028.3219814300537 Probab: tensor(0.1840, device='cuda:0')\n",
      "Epoch 542, LR: 0.4883, Train Loss: 2.0267, Train Accuracy: 35.40%, Temperatures:(0.00, 4.31)\n",
      "Old & New Losses 2027.7557373046875 2024.0252017974854 Probab: tensor(2.3773, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 543, LR: 0.4883, Train Loss: 2.0222, Train Accuracy: 35.70%, Temperatures:(0.00, 4.26)\n",
      "Old & New Losses 2023.686408996582 2021.3279724121094 Probab: tensor(1.7384, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 544, LR: 0.4883, Train Loss: 2.0277, Train Accuracy: 35.90%, Temperatures:(0.00, 4.22)\n",
      "Old & New Losses 2027.5607109069824 2026.7691612243652 Probab: tensor(1.2062, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 545, LR: 0.4883, Train Loss: 2.0245, Train Accuracy: 36.30%, Temperatures:(0.00, 4.18)\n",
      "Old & New Losses 2024.3809223175049 2028.7132263183594 Probab: tensor(0.3547, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 546, LR: 0.4883, Train Loss: 2.0243, Train Accuracy: 37.30%, Temperatures:(0.00, 4.14)\n",
      "Old & New Losses 2029.8714637756348 2040.1883125305176 Probab: tensor(0.0827, device='cuda:0')\n",
      "Epoch 547, LR: 0.4883, Train Loss: 2.0307, Train Accuracy: 38.00%, Temperatures:(0.00, 4.10)\n",
      "Old & New Losses 2029.996633529663 2027.7986526489258 Probab: tensor(1.7100, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 548, LR: 0.4883, Train Loss: 2.0300, Train Accuracy: 36.70%, Temperatures:(0.00, 4.06)\n",
      "Old & New Losses 2029.249668121338 2027.7717113494873 Probab: tensor(1.4396, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 549, LR: 0.4883, Train Loss: 2.0278, Train Accuracy: 35.90%, Temperatures:(0.00, 4.02)\n",
      "Old & New Losses 2027.111291885376 2041.40043258667 Probab: tensor(0.0285, device='cuda:0')\n",
      "Epoch 550, LR: 0.4883, Train Loss: 2.0303, Train Accuracy: 36.20%, Temperatures:(0.00, 3.98)\n",
      "Old & New Losses 2033.0286026000977 2041.7706966400146 Probab: tensor(0.1109, device='cuda:0')\n",
      "Epoch 551, LR: 0.4883, Train Loss: 2.0282, Train Accuracy: 35.20%, Temperatures:(0.00, 3.94)\n",
      "Old & New Losses 2024.4266986846924 2030.9476852416992 Probab: tensor(0.1907, device='cuda:0')\n",
      "Epoch 552, LR: 0.2441, Train Loss: 2.0282, Train Accuracy: 36.40%, Temperatures:(0.00, 3.90)\n",
      "Old & New Losses 2026.2055397033691 2029.9580097198486 Probab: tensor(0.3817, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 553, LR: 0.2441, Train Loss: 2.0319, Train Accuracy: 37.30%, Temperatures:(0.00, 3.86)\n",
      "Old & New Losses 2027.7905464172363 2034.952163696289 Probab: tensor(0.1562, device='cuda:0')\n",
      "Epoch 554, LR: 0.2441, Train Loss: 2.0284, Train Accuracy: 36.30%, Temperatures:(0.00, 3.82)\n",
      "Old & New Losses 2037.302017211914 2027.6827812194824 Probab: tensor(12.4172, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 555, LR: 0.2441, Train Loss: 2.0320, Train Accuracy: 36.40%, Temperatures:(0.00, 3.78)\n",
      "Old & New Losses 2019.2029476165771 2024.3871212005615 Probab: tensor(0.2538, device='cuda:0')\n",
      "Epoch 556, LR: 0.2441, Train Loss: 2.0213, Train Accuracy: 35.40%, Temperatures:(0.00, 3.74)\n",
      "Old & New Losses 2018.5294151306152 2024.7471332550049 Probab: tensor(0.1899, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 557, LR: 0.2441, Train Loss: 2.0197, Train Accuracy: 35.60%, Temperatures:(0.00, 3.71)\n",
      "Old & New Losses 2022.322416305542 2029.4840335845947 Probab: tensor(0.1447, device='cuda:0')\n",
      "Epoch 558, LR: 0.2441, Train Loss: 2.0236, Train Accuracy: 37.20%, Temperatures:(0.00, 3.67)\n",
      "Old & New Losses 2017.310380935669 2024.1870880126953 Probab: tensor(0.1534, device='cuda:0')\n",
      "Epoch 559, LR: 0.2441, Train Loss: 2.0244, Train Accuracy: 33.70%, Temperatures:(0.00, 3.63)\n",
      "Old & New Losses 2018.27073097229 2024.9693393707275 Probab: tensor(0.1581, device='cuda:0')\n",
      "Epoch 560, LR: 0.2441, Train Loss: 2.0218, Train Accuracy: 34.80%, Temperatures:(0.00, 3.60)\n",
      "Old & New Losses 2025.984287261963 2023.1037139892578 Probab: tensor(2.2283, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 561, LR: 0.2441, Train Loss: 2.0212, Train Accuracy: 36.10%, Temperatures:(0.00, 3.56)\n",
      "Old & New Losses 2021.0247039794922 2024.137258529663 Probab: tensor(0.4171, device='cuda:0')\n",
      "Epoch 562, LR: 0.2441, Train Loss: 2.0229, Train Accuracy: 35.60%, Temperatures:(0.00, 3.52)\n",
      "Old & New Losses 2023.1661796569824 2022.6101875305176 Probab: tensor(1.1709, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 563, LR: 0.2441, Train Loss: 2.0183, Train Accuracy: 33.70%, Temperatures:(0.00, 3.49)\n",
      "Old & New Losses 2018.9440250396729 2026.7834663391113 Probab: tensor(0.1057, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 564, LR: 0.2441, Train Loss: 2.0261, Train Accuracy: 35.60%, Temperatures:(0.00, 3.45)\n",
      "Old & New Losses 2019.9015140533447 2028.7909507751465 Probab: tensor(0.0762, device='cuda:0')\n",
      "Epoch 565, LR: 0.2441, Train Loss: 2.0196, Train Accuracy: 36.50%, Temperatures:(0.00, 3.42)\n",
      "Old & New Losses 2020.5235481262207 2040.342092514038 Probab: tensor(0.0030, device='cuda:0')\n",
      "Epoch 566, LR: 0.2441, Train Loss: 2.0234, Train Accuracy: 35.50%, Temperatures:(0.00, 3.38)\n",
      "Old & New Losses 2021.3534832000732 2027.0581245422363 Probab: tensor(0.1854, device='cuda:0')\n",
      "Epoch 567, LR: 0.2441, Train Loss: 2.0256, Train Accuracy: 34.40%, Temperatures:(0.00, 3.35)\n",
      "Old & New Losses 2019.730806350708 2026.7233848571777 Probab: tensor(0.1241, device='cuda:0')\n",
      "Epoch 568, LR: 0.2441, Train Loss: 2.0205, Train Accuracy: 35.30%, Temperatures:(0.00, 3.32)\n",
      "Old & New Losses 2023.226022720337 2017.3194408416748 Probab: tensor(5.9329, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 569, LR: 0.2441, Train Loss: 2.0195, Train Accuracy: 36.30%, Temperatures:(0.00, 3.28)\n",
      "Old & New Losses 2020.6611156463623 2019.989013671875 Probab: tensor(1.2271, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 570, LR: 0.2441, Train Loss: 2.0196, Train Accuracy: 36.70%, Temperatures:(0.00, 3.25)\n",
      "Old & New Losses 2025.2962112426758 2036.1254215240479 Probab: tensor(0.0358, device='cuda:0')\n",
      "Epoch 571, LR: 0.2441, Train Loss: 2.0248, Train Accuracy: 35.90%, Temperatures:(0.00, 3.22)\n",
      "Old & New Losses 2023.6423015594482 2035.2768898010254 Probab: tensor(0.0269, device='cuda:0')\n",
      "Epoch 572, LR: 0.2441, Train Loss: 2.0223, Train Accuracy: 36.30%, Temperatures:(0.00, 3.19)\n",
      "Old & New Losses 2023.101568222046 2020.7335948944092 Probab: tensor(2.1024, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 573, LR: 0.2441, Train Loss: 2.0248, Train Accuracy: 33.30%, Temperatures:(0.00, 3.15)\n",
      "Old & New Losses 2028.3176898956299 2026.8487930297852 Probab: tensor(1.5930, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 574, LR: 0.2441, Train Loss: 2.0248, Train Accuracy: 34.10%, Temperatures:(0.00, 3.12)\n",
      "Old & New Losses 2022.862195968628 2030.0109386444092 Probab: tensor(0.1014, device='cuda:0')\n",
      "Epoch 575, LR: 0.2441, Train Loss: 2.0251, Train Accuracy: 34.50%, Temperatures:(0.00, 3.09)\n",
      "Old & New Losses 2021.6562747955322 2026.6587734222412 Probab: tensor(0.1983, device='cuda:0')\n",
      "Epoch 576, LR: 0.2441, Train Loss: 2.0256, Train Accuracy: 33.90%, Temperatures:(0.00, 3.06)\n",
      "Old & New Losses 2022.341251373291 2019.8259353637695 Probab: tensor(2.2744, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 577, LR: 0.2441, Train Loss: 2.0268, Train Accuracy: 34.80%, Temperatures:(0.00, 3.03)\n",
      "Old & New Losses 2020.2672481536865 2020.5214023590088 Probab: tensor(0.9196, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 578, LR: 0.2441, Train Loss: 2.0191, Train Accuracy: 36.30%, Temperatures:(0.00, 3.00)\n",
      "Old & New Losses 2020.7247734069824 2053.8084506988525 Probab: tensor(1.6251e-05, device='cuda:0')\n",
      "Epoch 579, LR: 0.2441, Train Loss: 2.0214, Train Accuracy: 35.50%, Temperatures:(0.00, 2.97)\n",
      "Old & New Losses 2017.9729461669922 2033.3120822906494 Probab: tensor(0.0057, device='cuda:0')\n",
      "Epoch 580, LR: 0.2441, Train Loss: 2.0195, Train Accuracy: 35.40%, Temperatures:(0.00, 2.94)\n",
      "Old & New Losses 2021.7444896697998 2023.59938621521 Probab: tensor(0.5322, device='cuda:0')\n",
      "Epoch 581, LR: 0.2441, Train Loss: 2.0195, Train Accuracy: 35.70%, Temperatures:(0.00, 2.91)\n",
      "Old & New Losses 2024.0044593811035 2024.7185230255127 Probab: tensor(0.7825, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 582, LR: 0.2441, Train Loss: 2.0215, Train Accuracy: 36.20%, Temperatures:(0.00, 2.88)\n",
      "Old & New Losses 2026.503562927246 2031.2633514404297 Probab: tensor(0.1917, device='cuda:0')\n",
      "Epoch 583, LR: 0.2441, Train Loss: 2.0328, Train Accuracy: 34.60%, Temperatures:(0.00, 2.85)\n",
      "Old & New Losses 2025.2397060394287 2025.0685214996338 Probab: tensor(1.0618, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 584, LR: 0.2441, Train Loss: 2.0275, Train Accuracy: 34.60%, Temperatures:(0.00, 2.82)\n",
      "Old & New Losses 2021.2244987487793 2025.9757041931152 Probab: tensor(0.1860, device='cuda:0')\n",
      "Epoch 585, LR: 0.2441, Train Loss: 2.0285, Train Accuracy: 35.00%, Temperatures:(0.00, 2.80)\n",
      "Old & New Losses 2021.5342044830322 2031.5616130828857 Probab: tensor(0.0277, device='cuda:0')\n",
      "Epoch 586, LR: 0.2441, Train Loss: 2.0252, Train Accuracy: 34.30%, Temperatures:(0.00, 2.77)\n",
      "Old & New Losses 2021.8274593353271 2025.432825088501 Probab: tensor(0.2719, device='cuda:0')\n",
      "Epoch 587, LR: 0.2441, Train Loss: 2.0228, Train Accuracy: 35.50%, Temperatures:(0.00, 2.74)\n",
      "Old & New Losses 2022.3147869110107 2019.9711322784424 Probab: tensor(2.3517, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 588, LR: 0.2441, Train Loss: 2.0271, Train Accuracy: 35.20%, Temperatures:(0.00, 2.71)\n",
      "Old & New Losses 2019.9964046478271 2017.0669555664062 Probab: tensor(2.9437, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 589, LR: 0.2441, Train Loss: 2.0216, Train Accuracy: 34.40%, Temperatures:(0.00, 2.69)\n",
      "Old & New Losses 2022.1614837646484 2020.7092761993408 Probab: tensor(1.7171, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 590, LR: 0.2441, Train Loss: 2.0262, Train Accuracy: 35.70%, Temperatures:(0.00, 2.66)\n",
      "Old & New Losses 2022.2969055175781 2020.2465057373047 Probab: tensor(2.1620, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 591, LR: 0.2441, Train Loss: 2.0266, Train Accuracy: 33.20%, Temperatures:(0.00, 2.63)\n",
      "Old & New Losses 2019.3886756896973 2026.8781185150146 Probab: tensor(0.0581, device='cuda:0')\n",
      "Epoch 592, LR: 0.2441, Train Loss: 2.0215, Train Accuracy: 33.80%, Temperatures:(0.00, 2.61)\n",
      "Old & New Losses 2020.3053951263428 2030.7583808898926 Probab: tensor(0.0181, device='cuda:0')\n",
      "Epoch 593, LR: 0.2441, Train Loss: 2.0221, Train Accuracy: 35.70%, Temperatures:(0.00, 2.58)\n",
      "Old & New Losses 2019.5114612579346 2030.9827327728271 Probab: tensor(0.0117, device='cuda:0')\n",
      "Epoch 594, LR: 0.2441, Train Loss: 2.0183, Train Accuracy: 34.80%, Temperatures:(0.00, 2.55)\n",
      "Old & New Losses 2022.843837738037 2028.075933456421 Probab: tensor(0.1290, device='cuda:0')\n",
      "Epoch 595, LR: 0.2441, Train Loss: 2.0244, Train Accuracy: 35.10%, Temperatures:(0.00, 2.53)\n",
      "Old & New Losses 2016.507625579834 2021.967887878418 Probab: tensor(0.1154, device='cuda:0')\n",
      "Epoch 596, LR: 0.2441, Train Loss: 2.0235, Train Accuracy: 34.90%, Temperatures:(0.00, 2.50)\n",
      "Old & New Losses 2019.4838047027588 2026.2365341186523 Probab: tensor(0.0674, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 597, LR: 0.2441, Train Loss: 2.0212, Train Accuracy: 35.00%, Temperatures:(0.00, 2.48)\n",
      "Old & New Losses 2030.43794631958 2036.0090732574463 Probab: tensor(0.1056, device='cuda:0')\n",
      "Epoch 598, LR: 0.2441, Train Loss: 2.0303, Train Accuracy: 34.60%, Temperatures:(0.00, 2.45)\n",
      "Old & New Losses 2029.2551517486572 2016.4000988006592 Probab: tensor(188.4341, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 599, LR: 0.2441, Train Loss: 2.0247, Train Accuracy: 35.10%, Temperatures:(0.00, 2.43)\n",
      "Old & New Losses 2024.9996185302734 2023.2324600219727 Probab: tensor(2.0698, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 600, LR: 0.2441, Train Loss: 2.0207, Train Accuracy: 34.90%, Temperatures:(0.00, 2.41)\n",
      "Old & New Losses 2023.7395763397217 2023.2162475585938 Probab: tensor(1.2431, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 601, LR: 0.2441, Train Loss: 2.0177, Train Accuracy: 34.90%, Temperatures:(0.00, 2.38)\n",
      "Old & New Losses 2021.7134952545166 2020.9155082702637 Probab: tensor(1.3982, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 602, LR: 0.1221, Train Loss: 2.0227, Train Accuracy: 34.70%, Temperatures:(0.00, 2.36)\n",
      "Old & New Losses 2022.782802581787 2026.8166065216064 Probab: tensor(0.1806, device='cuda:0')\n",
      "Epoch 603, LR: 0.1221, Train Loss: 2.0180, Train Accuracy: 35.30%, Temperatures:(0.00, 2.33)\n",
      "Old & New Losses 2022.8238105773926 2017.9040431976318 Probab: tensor(8.2338, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 604, LR: 0.1221, Train Loss: 2.0188, Train Accuracy: 35.60%, Temperatures:(0.00, 2.31)\n",
      "Old & New Losses 2013.308048248291 2034.7278118133545 Probab: tensor(9.4053e-05, device='cuda:0')\n",
      "Epoch 605, LR: 0.1221, Train Loss: 2.0237, Train Accuracy: 34.00%, Temperatures:(0.00, 2.29)\n",
      "Old & New Losses 2021.0542678833008 2025.163173675537 Probab: tensor(0.1659, device='cuda:0')\n",
      "Epoch 606, LR: 0.1221, Train Loss: 2.0224, Train Accuracy: 34.10%, Temperatures:(0.00, 2.26)\n",
      "Old & New Losses 2021.5740203857422 2026.9694328308105 Probab: tensor(0.0923, device='cuda:0')\n",
      "Epoch 607, LR: 0.1221, Train Loss: 2.0214, Train Accuracy: 35.10%, Temperatures:(0.00, 2.24)\n",
      "Old & New Losses 2018.7287330627441 2024.599313735962 Probab: tensor(0.0729, device='cuda:0')\n",
      "Epoch 608, LR: 0.1221, Train Loss: 2.0216, Train Accuracy: 34.70%, Temperatures:(0.00, 2.22)\n",
      "Old & New Losses 2012.5410556793213 2018.3353424072266 Probab: tensor(0.0735, device='cuda:0')\n",
      "Epoch 609, LR: 0.1221, Train Loss: 2.0155, Train Accuracy: 34.80%, Temperatures:(0.00, 2.20)\n",
      "Old & New Losses 2017.669439315796 2021.4834213256836 Probab: tensor(0.1762, device='cuda:0')\n",
      "Epoch 610, LR: 0.1221, Train Loss: 2.0185, Train Accuracy: 34.30%, Temperatures:(0.00, 2.18)\n",
      "Old & New Losses 2017.317295074463 2021.2478637695312 Probab: tensor(0.1641, device='cuda:0')\n",
      "Epoch 611, LR: 0.1221, Train Loss: 2.0161, Train Accuracy: 36.10%, Temperatures:(0.00, 2.15)\n",
      "Old & New Losses 2019.3967819213867 2021.9955444335938 Probab: tensor(0.2991, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 612, LR: 0.1221, Train Loss: 2.0185, Train Accuracy: 34.80%, Temperatures:(0.00, 2.13)\n",
      "Old & New Losses 2021.7771530151367 2021.0766792297363 Probab: tensor(1.3890, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 613, LR: 0.1221, Train Loss: 2.0228, Train Accuracy: 35.00%, Temperatures:(0.00, 2.11)\n",
      "Old & New Losses 2017.8265571594238 2027.9207229614258 Probab: tensor(0.0084, device='cuda:0')\n",
      "Epoch 614, LR: 0.1221, Train Loss: 2.0178, Train Accuracy: 33.30%, Temperatures:(0.00, 2.09)\n",
      "Old & New Losses 2013.4179592132568 2014.0140056610107 Probab: tensor(0.7518, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 615, LR: 0.1221, Train Loss: 2.0171, Train Accuracy: 34.00%, Temperatures:(0.00, 2.07)\n",
      "Old & New Losses 2017.6231861114502 2023.543119430542 Probab: tensor(0.0572, device='cuda:0')\n",
      "Epoch 616, LR: 0.1221, Train Loss: 2.0194, Train Accuracy: 34.20%, Temperatures:(0.00, 2.05)\n",
      "Old & New Losses 2016.8745517730713 2018.8090801239014 Probab: tensor(0.3888, device='cuda:0')\n",
      "Epoch 617, LR: 0.1221, Train Loss: 2.0215, Train Accuracy: 33.70%, Temperatures:(0.00, 2.03)\n",
      "Old & New Losses 2021.8689441680908 2018.629789352417 Probab: tensor(4.9420, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 618, LR: 0.1221, Train Loss: 2.0219, Train Accuracy: 35.00%, Temperatures:(0.00, 2.01)\n",
      "Old & New Losses 2027.1530151367188 2022.001028060913 Probab: tensor(13.0266, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 619, LR: 0.1221, Train Loss: 2.0243, Train Accuracy: 34.20%, Temperatures:(0.00, 1.99)\n",
      "Old & New Losses 2032.4947834014893 2024.8630046844482 Probab: tensor(46.5704, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 620, LR: 0.1221, Train Loss: 2.0233, Train Accuracy: 33.60%, Temperatures:(0.00, 1.97)\n",
      "Old & New Losses 2022.0773220062256 2026.902675628662 Probab: tensor(0.0860, device='cuda:0')\n",
      "Epoch 621, LR: 0.1221, Train Loss: 2.0318, Train Accuracy: 32.90%, Temperatures:(0.00, 1.95)\n",
      "Old & New Losses 2026.742935180664 2023.475170135498 Probab: tensor(5.3549, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 622, LR: 0.1221, Train Loss: 2.0295, Train Accuracy: 34.20%, Temperatures:(0.00, 1.93)\n",
      "Old & New Losses 2027.946949005127 2028.390884399414 Probab: tensor(0.7943, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 623, LR: 0.1221, Train Loss: 2.0308, Train Accuracy: 31.80%, Temperatures:(0.00, 1.91)\n",
      "Old & New Losses 2036.3497734069824 2039.574146270752 Probab: tensor(0.1846, device='cuda:0')\n",
      "Epoch 624, LR: 0.1221, Train Loss: 2.0312, Train Accuracy: 32.40%, Temperatures:(0.00, 1.89)\n",
      "Old & New Losses 2029.1154384613037 2027.5042057037354 Probab: tensor(2.3460, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 625, LR: 0.1221, Train Loss: 2.0316, Train Accuracy: 32.20%, Temperatures:(0.00, 1.87)\n",
      "Old & New Losses 2025.2723693847656 2024.6062278747559 Probab: tensor(1.4277, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 626, LR: 0.1221, Train Loss: 2.0338, Train Accuracy: 32.50%, Temperatures:(0.00, 1.85)\n",
      "Old & New Losses 2031.0592651367188 2027.6422500610352 Probab: tensor(6.3286, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 627, LR: 0.1221, Train Loss: 2.0258, Train Accuracy: 31.90%, Temperatures:(0.00, 1.83)\n",
      "Old & New Losses 2029.6661853790283 2021.6147899627686 Probab: tensor(80.7543, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 628, LR: 0.1221, Train Loss: 2.0279, Train Accuracy: 33.40%, Temperatures:(0.00, 1.82)\n",
      "Old & New Losses 2024.8816013336182 2032.9337120056152 Probab: tensor(0.0118, device='cuda:0')\n",
      "Epoch 629, LR: 0.1221, Train Loss: 2.0309, Train Accuracy: 32.50%, Temperatures:(0.00, 1.80)\n",
      "Old & New Losses 2027.1739959716797 2024.4483947753906 Probab: tensor(4.5576, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 630, LR: 0.1221, Train Loss: 2.0226, Train Accuracy: 31.80%, Temperatures:(0.00, 1.78)\n",
      "Old & New Losses 2020.531415939331 2024.2810249328613 Probab: tensor(0.1215, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 631, LR: 0.1221, Train Loss: 2.0202, Train Accuracy: 30.80%, Temperatures:(0.00, 1.76)\n",
      "Old & New Losses 2028.7044048309326 2033.3788394927979 Probab: tensor(0.0704, device='cuda:0')\n",
      "Epoch 632, LR: 0.1221, Train Loss: 2.0204, Train Accuracy: 31.80%, Temperatures:(0.00, 1.74)\n",
      "Old & New Losses 2023.7672328948975 2019.18363571167 Probab: tensor(13.8576, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 633, LR: 0.1221, Train Loss: 2.0251, Train Accuracy: 30.90%, Temperatures:(0.00, 1.73)\n",
      "Old & New Losses 2021.7480659484863 2023.2470035552979 Probab: tensor(0.4196, device='cuda:0')\n",
      "Epoch 634, LR: 0.1221, Train Loss: 2.0284, Train Accuracy: 30.20%, Temperatures:(0.00, 1.71)\n",
      "Old & New Losses 2028.108835220337 2017.8282260894775 Probab: tensor(409.9222, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 635, LR: 0.1221, Train Loss: 2.0209, Train Accuracy: 31.40%, Temperatures:(0.00, 1.69)\n",
      "Old & New Losses 2015.869379043579 2022.7081775665283 Probab: tensor(0.0176, device='cuda:0')\n",
      "Epoch 636, LR: 0.1221, Train Loss: 2.0210, Train Accuracy: 31.80%, Temperatures:(0.00, 1.67)\n",
      "Old & New Losses 2018.4662342071533 2027.7268886566162 Probab: tensor(0.0040, device='cuda:0')\n",
      "Epoch 637, LR: 0.1221, Train Loss: 2.0148, Train Accuracy: 32.60%, Temperatures:(0.00, 1.66)\n",
      "Old & New Losses 2017.089605331421 2019.63472366333 Probab: tensor(0.2155, device='cuda:0')\n",
      "Epoch 638, LR: 0.1221, Train Loss: 2.0192, Train Accuracy: 32.90%, Temperatures:(0.00, 1.64)\n",
      "Old & New Losses 2023.3309268951416 2028.3114910125732 Probab: tensor(0.0481, device='cuda:0')\n",
      "Epoch 639, LR: 0.1221, Train Loss: 2.0214, Train Accuracy: 31.00%, Temperatures:(0.00, 1.63)\n",
      "Old & New Losses 2020.0018882751465 2016.517162322998 Probab: tensor(8.5358, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 640, LR: 0.1221, Train Loss: 2.0183, Train Accuracy: 31.70%, Temperatures:(0.00, 1.61)\n",
      "Old & New Losses 2019.3204879760742 2018.1515216827393 Probab: tensor(2.0680, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 641, LR: 0.1221, Train Loss: 2.0152, Train Accuracy: 31.80%, Temperatures:(0.00, 1.59)\n",
      "Old & New Losses 2021.0278034210205 2018.876075744629 Probab: tensor(3.8610, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 642, LR: 0.1221, Train Loss: 2.0218, Train Accuracy: 32.40%, Temperatures:(0.00, 1.58)\n",
      "Old & New Losses 2020.287275314331 2023.2067108154297 Probab: tensor(0.1570, device='cuda:0')\n",
      "Epoch 643, LR: 0.1221, Train Loss: 2.0175, Train Accuracy: 31.30%, Temperatures:(0.00, 1.56)\n",
      "Old & New Losses 2021.446943283081 2023.1399536132812 Probab: tensor(0.3381, device='cuda:0')\n",
      "Epoch 644, LR: 0.1221, Train Loss: 2.0195, Train Accuracy: 31.30%, Temperatures:(0.00, 1.55)\n",
      "Old & New Losses 2018.0859565734863 2020.9548473358154 Probab: tensor(0.1562, device='cuda:0')\n",
      "Epoch 645, LR: 0.1221, Train Loss: 2.0129, Train Accuracy: 31.00%, Temperatures:(0.00, 1.53)\n",
      "Old & New Losses 2017.9262161254883 2023.8070487976074 Probab: tensor(0.0214, device='cuda:0')\n",
      "Epoch 646, LR: 0.1221, Train Loss: 2.0187, Train Accuracy: 31.70%, Temperatures:(0.00, 1.51)\n",
      "Old & New Losses 2018.4626579284668 2016.911268234253 Probab: tensor(2.7849, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 647, LR: 0.1221, Train Loss: 2.0132, Train Accuracy: 32.70%, Temperatures:(0.00, 1.50)\n",
      "Old & New Losses 2022.1104621887207 2014.8887634277344 Probab: tensor(123.4456, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 648, LR: 0.1221, Train Loss: 2.0217, Train Accuracy: 32.10%, Temperatures:(0.00, 1.48)\n",
      "Old & New Losses 2020.7829475402832 2026.0398387908936 Probab: tensor(0.0290, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 649, LR: 0.1221, Train Loss: 2.0175, Train Accuracy: 31.50%, Temperatures:(0.00, 1.47)\n",
      "Old & New Losses 2022.17698097229 2025.263786315918 Probab: tensor(0.1224, device='cuda:0')\n",
      "Epoch 650, LR: 0.1221, Train Loss: 2.0232, Train Accuracy: 30.20%, Temperatures:(0.00, 1.46)\n",
      "Old & New Losses 2020.93505859375 2041.3646697998047 Probab: tensor(7.9850e-07, device='cuda:0')\n",
      "Epoch 651, LR: 0.1221, Train Loss: 2.0219, Train Accuracy: 29.60%, Temperatures:(0.00, 1.44)\n",
      "Old & New Losses 2022.8221416473389 2023.5857963562012 Probab: tensor(0.5885, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 652, LR: 0.0610, Train Loss: 2.0214, Train Accuracy: 30.10%, Temperatures:(0.00, 1.43)\n",
      "Old & New Losses 2020.6108093261719 2029.4806957244873 Probab: tensor(0.0020, device='cuda:0')\n",
      "Epoch 653, LR: 0.0610, Train Loss: 2.0202, Train Accuracy: 31.20%, Temperatures:(0.00, 1.41)\n",
      "Old & New Losses 2025.4051685333252 2028.043270111084 Probab: tensor(0.1543, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 654, LR: 0.0610, Train Loss: 2.0221, Train Accuracy: 32.40%, Temperatures:(0.00, 1.40)\n",
      "Old & New Losses 2022.6714611053467 2026.9322395324707 Probab: tensor(0.0474, device='cuda:0')\n",
      "Epoch 655, LR: 0.0610, Train Loss: 2.0298, Train Accuracy: 30.60%, Temperatures:(0.00, 1.38)\n",
      "Old & New Losses 2024.7609615325928 2029.9415588378906 Probab: tensor(0.0237, device='cuda:0')\n",
      "Epoch 656, LR: 0.0610, Train Loss: 2.0226, Train Accuracy: 31.30%, Temperatures:(0.00, 1.37)\n",
      "Old & New Losses 2021.057367324829 2028.261661529541 Probab: tensor(0.0052, device='cuda:0')\n",
      "Epoch 657, LR: 0.0610, Train Loss: 2.0209, Train Accuracy: 29.70%, Temperatures:(0.00, 1.36)\n",
      "Old & New Losses 2026.40962600708 2030.9257507324219 Probab: tensor(0.0358, device='cuda:0')\n",
      "Epoch 658, LR: 0.0610, Train Loss: 2.0267, Train Accuracy: 30.30%, Temperatures:(0.00, 1.34)\n",
      "Old & New Losses 2019.852638244629 2032.411813735962 Probab: tensor(8.6608e-05, device='cuda:0')\n",
      "Epoch 659, LR: 0.0610, Train Loss: 2.0235, Train Accuracy: 29.60%, Temperatures:(0.00, 1.33)\n",
      "Old & New Losses 2027.303695678711 2024.219274520874 Probab: tensor(10.1807, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 660, LR: 0.0610, Train Loss: 2.0249, Train Accuracy: 30.30%, Temperatures:(0.00, 1.32)\n",
      "Old & New Losses 2025.2625942230225 2023.2253074645996 Probab: tensor(4.7029, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 661, LR: 0.0610, Train Loss: 2.0213, Train Accuracy: 31.20%, Temperatures:(0.00, 1.30)\n",
      "Old & New Losses 2025.0909328460693 2023.8194465637207 Probab: tensor(2.6538, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 662, LR: 0.0610, Train Loss: 2.0214, Train Accuracy: 30.70%, Temperatures:(0.00, 1.29)\n",
      "Old & New Losses 2018.9502239227295 2025.9993076324463 Probab: tensor(0.0042, device='cuda:0')\n",
      "Epoch 663, LR: 0.0610, Train Loss: 2.0214, Train Accuracy: 30.70%, Temperatures:(0.00, 1.28)\n",
      "Old & New Losses 2020.6105709075928 2022.0587253570557 Probab: tensor(0.3217, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 664, LR: 0.0610, Train Loss: 2.0217, Train Accuracy: 32.20%, Temperatures:(0.00, 1.26)\n",
      "Old & New Losses 2022.8338241577148 2027.5826454162598 Probab: tensor(0.0234, device='cuda:0')\n",
      "Epoch 665, LR: 0.0610, Train Loss: 2.0202, Train Accuracy: 31.60%, Temperatures:(0.00, 1.25)\n",
      "Old & New Losses 2020.503044128418 2031.189203262329 Probab: tensor(0.0002, device='cuda:0')\n",
      "Epoch 666, LR: 0.0610, Train Loss: 2.0233, Train Accuracy: 30.90%, Temperatures:(0.00, 1.24)\n",
      "Old & New Losses 2019.0653800964355 2024.8136520385742 Probab: tensor(0.0097, device='cuda:0')\n",
      "Epoch 667, LR: 0.0610, Train Loss: 2.0287, Train Accuracy: 31.30%, Temperatures:(0.00, 1.23)\n",
      "Old & New Losses 2022.007942199707 2036.3643169403076 Probab: tensor(8.2528e-06, device='cuda:0')\n",
      "Epoch 668, LR: 0.0610, Train Loss: 2.0212, Train Accuracy: 31.70%, Temperatures:(0.00, 1.21)\n",
      "Old & New Losses 2025.4299640655518 2021.1269855499268 Probab: tensor(34.5953, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 669, LR: 0.0610, Train Loss: 2.0195, Train Accuracy: 30.40%, Temperatures:(0.00, 1.20)\n",
      "Old & New Losses 2017.4977779388428 2027.451753616333 Probab: tensor(0.0003, device='cuda:0')\n",
      "Epoch 670, LR: 0.0610, Train Loss: 2.0203, Train Accuracy: 31.80%, Temperatures:(0.00, 1.19)\n",
      "Old & New Losses 2021.8987464904785 2015.0141716003418 Probab: tensor(325.3534, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 671, LR: 0.0610, Train Loss: 2.0218, Train Accuracy: 31.70%, Temperatures:(0.00, 1.18)\n",
      "Old & New Losses 2024.3453979492188 2021.1400985717773 Probab: tensor(15.1883, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 672, LR: 0.0610, Train Loss: 2.0162, Train Accuracy: 32.30%, Temperatures:(0.00, 1.17)\n",
      "Old & New Losses 2014.939785003662 2021.5847492218018 Probab: tensor(0.0034, device='cuda:0')\n",
      "Epoch 673, LR: 0.0610, Train Loss: 2.0207, Train Accuracy: 31.20%, Temperatures:(0.00, 1.15)\n",
      "Old & New Losses 2017.3149108886719 2012.4273300170898 Probab: tensor(68.8966, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 674, LR: 0.0610, Train Loss: 2.0170, Train Accuracy: 31.40%, Temperatures:(0.00, 1.14)\n",
      "Old & New Losses 2017.1887874603271 2015.4950618743896 Probab: tensor(4.3998, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 675, LR: 0.0610, Train Loss: 2.0214, Train Accuracy: 30.70%, Temperatures:(0.00, 1.13)\n",
      "Old & New Losses 2023.74267578125 2020.869493484497 Probab: tensor(12.6629, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 676, LR: 0.0610, Train Loss: 2.0257, Train Accuracy: 31.40%, Temperatures:(0.00, 1.12)\n",
      "Old & New Losses 2017.4055099487305 2021.8322277069092 Probab: tensor(0.0192, device='cuda:0')\n",
      "Epoch 677, LR: 0.0610, Train Loss: 2.0245, Train Accuracy: 31.00%, Temperatures:(0.00, 1.11)\n",
      "Old & New Losses 2022.0739841461182 2020.625352859497 Probab: tensor(3.6912, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 678, LR: 0.0610, Train Loss: 2.0199, Train Accuracy: 31.20%, Temperatures:(0.00, 1.10)\n",
      "Old & New Losses 2016.7863368988037 2019.1004276275635 Probab: tensor(0.1216, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 679, LR: 0.0610, Train Loss: 2.0248, Train Accuracy: 31.50%, Temperatures:(0.00, 1.09)\n",
      "Old & New Losses 2026.1497497558594 2018.4502601623535 Probab: tensor(1190.5192, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 680, LR: 0.0610, Train Loss: 2.0245, Train Accuracy: 32.50%, Temperatures:(0.00, 1.08)\n",
      "Old & New Losses 2023.2229232788086 2026.5707969665527 Probab: tensor(0.0446, device='cuda:0')\n",
      "Epoch 681, LR: 0.0610, Train Loss: 2.0271, Train Accuracy: 29.30%, Temperatures:(0.00, 1.07)\n",
      "Old & New Losses 2021.364450454712 2027.9619693756104 Probab: tensor(0.0020, device='cuda:0')\n",
      "Epoch 682, LR: 0.0610, Train Loss: 2.0250, Train Accuracy: 31.30%, Temperatures:(0.00, 1.05)\n",
      "Old & New Losses 2027.3735523223877 2019.9649333953857 Probab: tensor(1122.3634, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 683, LR: 0.0610, Train Loss: 2.0189, Train Accuracy: 31.20%, Temperatures:(0.00, 1.04)\n",
      "Old & New Losses 2013.7276649475098 2022.5114822387695 Probab: tensor(0.0002, device='cuda:0')\n",
      "Epoch 684, LR: 0.0610, Train Loss: 2.0202, Train Accuracy: 32.90%, Temperatures:(0.00, 1.03)\n",
      "Old & New Losses 2027.3194313049316 2020.7266807556152 Probab: tensor(587.9631, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 685, LR: 0.0610, Train Loss: 2.0170, Train Accuracy: 32.10%, Temperatures:(0.00, 1.02)\n",
      "Old & New Losses 2025.4790782928467 2021.0118293762207 Probab: tensor(78.6080, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 686, LR: 0.0610, Train Loss: 2.0142, Train Accuracy: 32.00%, Temperatures:(0.00, 1.01)\n",
      "Old & New Losses 2028.6695957183838 2046.2071895599365 Probab: tensor(3.0450e-08, device='cuda:0')\n",
      "Epoch 687, LR: 0.0610, Train Loss: 2.0293, Train Accuracy: 31.60%, Temperatures:(0.00, 1.00)\n",
      "Old & New Losses 2024.7917175292969 2025.0909328460693 Probab: tensor(0.7421, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 688, LR: 0.0610, Train Loss: 2.0212, Train Accuracy: 31.20%, Temperatures:(0.00, 0.99)\n",
      "Old & New Losses 2027.045488357544 2029.2243957519531 Probab: tensor(0.1115, device='cuda:0')\n",
      "Epoch 689, LR: 0.0610, Train Loss: 2.0265, Train Accuracy: 31.10%, Temperatures:(0.00, 0.98)\n",
      "Old & New Losses 2021.8732357025146 2026.1952877044678 Probab: tensor(0.0123, device='cuda:0')\n",
      "Epoch 690, LR: 0.0610, Train Loss: 2.0280, Train Accuracy: 31.40%, Temperatures:(0.00, 0.97)\n",
      "Old & New Losses 2028.8364887237549 2021.5437412261963 Probab: tensor(1793.9182, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 691, LR: 0.0610, Train Loss: 2.0277, Train Accuracy: 31.30%, Temperatures:(0.00, 0.96)\n",
      "Old & New Losses 2026.4017581939697 2020.6894874572754 Probab: tensor(375.3063, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 692, LR: 0.0610, Train Loss: 2.0183, Train Accuracy: 31.10%, Temperatures:(0.00, 0.95)\n",
      "Old & New Losses 2024.6305465698242 2013.9672756195068 Probab: tensor(71487.0078, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 693, LR: 0.0610, Train Loss: 2.0204, Train Accuracy: 31.70%, Temperatures:(0.00, 0.94)\n",
      "Old & New Losses 2007.8847408294678 2023.7493515014648 Probab: tensor(5.0702e-08, device='cuda:0')\n",
      "Epoch 694, LR: 0.0610, Train Loss: 2.0220, Train Accuracy: 31.90%, Temperatures:(0.00, 0.94)\n",
      "Old & New Losses 2017.8585052490234 2018.1078910827637 Probab: tensor(0.7659, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 695, LR: 0.0610, Train Loss: 2.0146, Train Accuracy: 32.10%, Temperatures:(0.00, 0.93)\n",
      "Old & New Losses 2012.578010559082 2020.707130432129 Probab: tensor(0.0002, device='cuda:0')\n",
      "Epoch 696, LR: 0.0610, Train Loss: 2.0183, Train Accuracy: 31.70%, Temperatures:(0.00, 0.92)\n",
      "Old & New Losses 2010.8866691589355 2013.9658451080322 Probab: tensor(0.0347, device='cuda:0')\n",
      "Epoch 697, LR: 0.0610, Train Loss: 2.0116, Train Accuracy: 32.20%, Temperatures:(0.00, 0.91)\n",
      "Old & New Losses 2016.1874294281006 2018.45383644104 Probab: tensor(0.0822, device='cuda:0')\n",
      "Epoch 698, LR: 0.0610, Train Loss: 2.0176, Train Accuracy: 31.30%, Temperatures:(0.00, 0.90)\n",
      "Old & New Losses 2017.1654224395752 2019.1774368286133 Probab: tensor(0.1064, device='cuda:0')\n",
      "Epoch 699, LR: 0.0610, Train Loss: 2.0181, Train Accuracy: 30.80%, Temperatures:(0.00, 0.89)\n",
      "Old & New Losses 2015.7005786895752 2017.1427726745605 Probab: tensor(0.1975, device='cuda:0')\n",
      "Epoch 700, LR: 0.0610, Train Loss: 2.0145, Train Accuracy: 32.30%, Temperatures:(0.00, 0.88)\n",
      "Old & New Losses 2020.8117961883545 2016.0722732543945 Probab: tensor(217.8742, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 701, LR: 0.0610, Train Loss: 2.0129, Train Accuracy: 31.70%, Temperatures:(0.00, 0.87)\n",
      "Old & New Losses 2017.4140930175781 2019.7694301605225 Probab: tensor(0.0670, device='cuda:0')\n",
      "Epoch 702, LR: 0.0305, Train Loss: 2.0189, Train Accuracy: 31.30%, Temperatures:(0.00, 0.86)\n",
      "Old & New Losses 2021.1756229400635 2022.200584411621 Probab: tensor(0.3048, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 703, LR: 0.0305, Train Loss: 2.0192, Train Accuracy: 32.30%, Temperatures:(0.00, 0.85)\n",
      "Old & New Losses 2013.9110088348389 2013.3342742919922 Probab: tensor(1.9644, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 704, LR: 0.0305, Train Loss: 2.0115, Train Accuracy: 32.80%, Temperatures:(0.00, 0.85)\n",
      "Old & New Losses 2017.2114372253418 2043.4212684631348 Probab: tensor(3.4608e-14, device='cuda:0')\n",
      "Epoch 705, LR: 0.0305, Train Loss: 2.0198, Train Accuracy: 32.10%, Temperatures:(0.00, 0.84)\n",
      "Old & New Losses 2020.3862190246582 2017.7054405212402 Probab: tensor(24.5866, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 706, LR: 0.0305, Train Loss: 2.0182, Train Accuracy: 32.00%, Temperatures:(0.00, 0.83)\n",
      "Old & New Losses 2021.127700805664 2020.9519863128662 Probab: tensor(1.2362, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 707, LR: 0.0305, Train Loss: 2.0213, Train Accuracy: 34.30%, Temperatures:(0.00, 0.82)\n",
      "Old & New Losses 2019.4470882415771 2020.6422805786133 Probab: tensor(0.2330, device='cuda:0')\n",
      "Epoch 708, LR: 0.0305, Train Loss: 2.0233, Train Accuracy: 32.80%, Temperatures:(0.00, 0.81)\n",
      "Old & New Losses 2017.2739028930664 2022.369146347046 Probab: tensor(0.0019, device='cuda:0')\n",
      "Epoch 709, LR: 0.0305, Train Loss: 2.0199, Train Accuracy: 32.80%, Temperatures:(0.00, 0.80)\n",
      "Old & New Losses 2022.7808952331543 2025.073528289795 Probab: tensor(0.0578, device='cuda:0')\n",
      "Epoch 710, LR: 0.0305, Train Loss: 2.0202, Train Accuracy: 31.90%, Temperatures:(0.00, 0.80)\n",
      "Old & New Losses 2018.85986328125 2024.4534015655518 Probab: tensor(0.0009, device='cuda:0')\n",
      "Epoch 711, LR: 0.0305, Train Loss: 2.0244, Train Accuracy: 31.10%, Temperatures:(0.00, 0.79)\n",
      "Old & New Losses 2018.179178237915 2021.5401649475098 Probab: tensor(0.0141, device='cuda:0')\n",
      "Epoch 712, LR: 0.0305, Train Loss: 2.0229, Train Accuracy: 33.80%, Temperatures:(0.00, 0.78)\n",
      "Old & New Losses 2020.60866355896 2024.1265296936035 Probab: tensor(0.0110, device='cuda:0')\n",
      "Epoch 713, LR: 0.0305, Train Loss: 2.0202, Train Accuracy: 34.20%, Temperatures:(0.00, 0.77)\n",
      "Old & New Losses 2020.5724239349365 2022.0203399658203 Probab: tensor(0.1535, device='cuda:0')\n",
      "Epoch 714, LR: 0.0305, Train Loss: 2.0218, Train Accuracy: 30.60%, Temperatures:(0.00, 0.76)\n",
      "Old & New Losses 2018.3615684509277 2015.8300399780273 Probab: tensor(27.3905, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 715, LR: 0.0305, Train Loss: 2.0173, Train Accuracy: 32.10%, Temperatures:(0.00, 0.76)\n",
      "Old & New Losses 2018.2571411132812 2013.8301849365234 Probab: tensor(346.2314, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 716, LR: 0.0305, Train Loss: 2.0167, Train Accuracy: 32.60%, Temperatures:(0.00, 0.75)\n",
      "Old & New Losses 2019.1333293914795 2018.9158916473389 Probab: tensor(1.3366, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 717, LR: 0.0305, Train Loss: 2.0172, Train Accuracy: 33.20%, Temperatures:(0.00, 0.74)\n",
      "Old & New Losses 2026.2374877929688 2023.1757164001465 Probab: tensor(61.9350, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 718, LR: 0.0305, Train Loss: 2.0196, Train Accuracy: 32.90%, Temperatures:(0.00, 0.73)\n",
      "Old & New Losses 2017.6324844360352 2027.3234844207764 Probab: tensor(1.8661e-06, device='cuda:0')\n",
      "Epoch 719, LR: 0.0305, Train Loss: 2.0230, Train Accuracy: 31.50%, Temperatures:(0.00, 0.73)\n",
      "Old & New Losses 2025.0434875488281 2027.5132656097412 Probab: tensor(0.0335, device='cuda:0')\n",
      "Epoch 720, LR: 0.0305, Train Loss: 2.0205, Train Accuracy: 34.70%, Temperatures:(0.00, 0.72)\n",
      "Old & New Losses 2022.4502086639404 2021.608829498291 Probab: tensor(3.2174, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 721, LR: 0.0305, Train Loss: 2.0228, Train Accuracy: 32.90%, Temperatures:(0.00, 0.71)\n",
      "Old & New Losses 2019.3097591400146 2043.5428619384766 Probab: tensor(1.7200e-15, device='cuda:0')\n",
      "Epoch 722, LR: 0.0305, Train Loss: 2.0242, Train Accuracy: 32.80%, Temperatures:(0.00, 0.71)\n",
      "Old & New Losses 2021.5389728546143 2020.9558010101318 Probab: tensor(2.2851, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 723, LR: 0.0305, Train Loss: 2.0246, Train Accuracy: 32.30%, Temperatures:(0.00, 0.70)\n",
      "Old & New Losses 2027.2252559661865 2027.8327465057373 Probab: tensor(0.4191, device='cuda:0')\n",
      "Epoch 724, LR: 0.0305, Train Loss: 2.0263, Train Accuracy: 32.90%, Temperatures:(0.00, 0.69)\n",
      "Old & New Losses 2020.1308727264404 2024.1947174072266 Probab: tensor(0.0028, device='cuda:0')\n",
      "Epoch 725, LR: 0.0305, Train Loss: 2.0244, Train Accuracy: 32.70%, Temperatures:(0.00, 0.68)\n",
      "Old & New Losses 2025.5842208862305 2030.7044982910156 Probab: tensor(0.0006, device='cuda:0')\n",
      "Epoch 726, LR: 0.0305, Train Loss: 2.0251, Train Accuracy: 31.80%, Temperatures:(0.00, 0.68)\n",
      "Old & New Losses 2023.2863426208496 2028.3334255218506 Probab: tensor(0.0006, device='cuda:0')\n",
      "Epoch 727, LR: 0.0305, Train Loss: 2.0221, Train Accuracy: 33.40%, Temperatures:(0.00, 0.67)\n",
      "Old & New Losses 2025.6757736206055 2023.4425067901611 Probab: tensor(27.8763, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 728, LR: 0.0305, Train Loss: 2.0220, Train Accuracy: 32.30%, Temperatures:(0.00, 0.66)\n",
      "Old & New Losses 2026.0190963745117 2026.129961013794 Probab: tensor(0.8463, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 729, LR: 0.0305, Train Loss: 2.0242, Train Accuracy: 30.90%, Temperatures:(0.00, 0.66)\n",
      "Old & New Losses 2023.528814315796 2029.1450023651123 Probab: tensor(0.0002, device='cuda:0')\n",
      "Epoch 730, LR: 0.0305, Train Loss: 2.0270, Train Accuracy: 32.50%, Temperatures:(0.00, 0.65)\n",
      "Old & New Losses 2026.1590480804443 2021.963357925415 Probab: tensor(628.5054, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 731, LR: 0.0305, Train Loss: 2.0301, Train Accuracy: 32.60%, Temperatures:(0.00, 0.64)\n",
      "Old & New Losses 2023.484230041504 2025.5115032196045 Probab: tensor(0.0431, device='cuda:0')\n",
      "Epoch 732, LR: 0.0305, Train Loss: 2.0215, Train Accuracy: 34.70%, Temperatures:(0.00, 0.64)\n",
      "Old & New Losses 2025.1305103302002 2024.7864723205566 Probab: tensor(1.7144, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 733, LR: 0.0305, Train Loss: 2.0257, Train Accuracy: 31.60%, Temperatures:(0.00, 0.63)\n",
      "Old & New Losses 2029.7174453735352 2030.395746231079 Probab: tensor(0.3418, device='cuda:0')\n",
      "Epoch 734, LR: 0.0305, Train Loss: 2.0214, Train Accuracy: 33.10%, Temperatures:(0.00, 0.63)\n",
      "Old & New Losses 2026.6551971435547 2032.1049690246582 Probab: tensor(0.0002, device='cuda:0')\n",
      "Epoch 735, LR: 0.0305, Train Loss: 2.0274, Train Accuracy: 32.60%, Temperatures:(0.00, 0.62)\n",
      "Old & New Losses 2028.259515762329 2035.379409790039 Probab: tensor(1.0155e-05, device='cuda:0')\n",
      "Epoch 736, LR: 0.0305, Train Loss: 2.0271, Train Accuracy: 32.20%, Temperatures:(0.00, 0.61)\n",
      "Old & New Losses 2027.0941257476807 2034.5981121063232 Probab: tensor(4.8322e-06, device='cuda:0')\n",
      "Epoch 737, LR: 0.0305, Train Loss: 2.0298, Train Accuracy: 31.30%, Temperatures:(0.00, 0.61)\n",
      "Old & New Losses 2024.94215965271 2024.3158340454102 Probab: tensor(2.8066, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 738, LR: 0.0305, Train Loss: 2.0238, Train Accuracy: 33.00%, Temperatures:(0.00, 0.60)\n",
      "Old & New Losses 2026.4654159545898 2033.165693283081 Probab: tensor(1.4359e-05, device='cuda:0')\n",
      "Epoch 739, LR: 0.0305, Train Loss: 2.0318, Train Accuracy: 32.40%, Temperatures:(0.00, 0.59)\n",
      "Old & New Losses 2026.900291442871 2027.921438217163 Probab: tensor(0.1797, device='cuda:0')\n",
      "Epoch 740, LR: 0.0305, Train Loss: 2.0323, Train Accuracy: 32.60%, Temperatures:(0.00, 0.59)\n",
      "Old & New Losses 2029.405117034912 2028.991937637329 Probab: tensor(2.0170, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 741, LR: 0.0305, Train Loss: 2.0274, Train Accuracy: 33.40%, Temperatures:(0.00, 0.58)\n",
      "Old & New Losses 2034.3477725982666 2031.442642211914 Probab: tensor(145.9040, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 742, LR: 0.0305, Train Loss: 2.0318, Train Accuracy: 32.30%, Temperatures:(0.00, 0.58)\n",
      "Old & New Losses 2032.6573848724365 2031.904697418213 Probab: tensor(3.6843, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 743, LR: 0.0305, Train Loss: 2.0308, Train Accuracy: 31.80%, Temperatures:(0.00, 0.57)\n",
      "Old & New Losses 2030.8430194854736 2029.7744274139404 Probab: tensor(6.4889, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 744, LR: 0.0305, Train Loss: 2.0300, Train Accuracy: 33.00%, Temperatures:(0.00, 0.57)\n",
      "Old & New Losses 2027.7206897735596 2033.4093570709229 Probab: tensor(4.2927e-05, device='cuda:0')\n",
      "Epoch 745, LR: 0.0305, Train Loss: 2.0330, Train Accuracy: 30.60%, Temperatures:(0.00, 0.56)\n",
      "Old & New Losses 2035.8245372772217 2034.3375205993652 Probab: tensor(14.2277, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 746, LR: 0.0305, Train Loss: 2.0253, Train Accuracy: 34.20%, Temperatures:(0.00, 0.55)\n",
      "Old & New Losses 2030.9202671051025 2036.698818206787 Probab: tensor(2.9761e-05, device='cuda:0')\n",
      "Epoch 747, LR: 0.0305, Train Loss: 2.0337, Train Accuracy: 31.80%, Temperatures:(0.00, 0.55)\n",
      "Old & New Losses 2037.0090007781982 2030.4038524627686 Probab: tensor(168300.5156, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 748, LR: 0.0305, Train Loss: 2.0336, Train Accuracy: 32.50%, Temperatures:(0.00, 0.54)\n",
      "Old & New Losses 2025.9490013122559 2031.9938659667969 Probab: tensor(1.4754e-05, device='cuda:0')\n",
      "Epoch 749, LR: 0.0305, Train Loss: 2.0261, Train Accuracy: 32.30%, Temperatures:(0.00, 0.54)\n",
      "Old & New Losses 2028.0585289001465 2035.9501838684082 Probab: tensor(4.2583e-07, device='cuda:0')\n",
      "Epoch 750, LR: 0.0305, Train Loss: 2.0287, Train Accuracy: 32.20%, Temperatures:(0.00, 0.53)\n",
      "Old & New Losses 2030.421495437622 2035.7849597930908 Probab: tensor(4.2311e-05, device='cuda:0')\n",
      "Epoch 751, LR: 0.0305, Train Loss: 2.0326, Train Accuracy: 33.30%, Temperatures:(0.00, 0.53)\n",
      "Old & New Losses 2022.7434635162354 2030.9531688690186 Probab: tensor(1.7295e-07, device='cuda:0')\n",
      "Epoch 752, LR: 0.0153, Train Loss: 2.0329, Train Accuracy: 32.90%, Temperatures:(0.00, 0.52)\n",
      "Old & New Losses 2031.8937301635742 2029.515266418457 Probab: tensor(95.2483, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 753, LR: 0.0153, Train Loss: 2.0284, Train Accuracy: 32.90%, Temperatures:(0.00, 0.52)\n",
      "Old & New Losses 2028.5120010375977 2033.7390899658203 Probab: tensor(4.0475e-05, device='cuda:0')\n",
      "Epoch 754, LR: 0.0153, Train Loss: 2.0299, Train Accuracy: 34.00%, Temperatures:(0.00, 0.51)\n",
      "Old & New Losses 2030.545711517334 2034.3687534332275 Probab: tensor(0.0006, device='cuda:0')\n",
      "Epoch 755, LR: 0.0153, Train Loss: 2.0301, Train Accuracy: 33.70%, Temperatures:(0.00, 0.51)\n",
      "Old & New Losses 2031.1496257781982 2032.1018695831299 Probab: tensor(0.1526, device='cuda:0')\n",
      "Epoch 756, LR: 0.0153, Train Loss: 2.0288, Train Accuracy: 33.00%, Temperatures:(0.00, 0.50)\n",
      "Old & New Losses 2026.7493724822998 2024.9788761138916 Probab: tensor(34.1554, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 757, LR: 0.0153, Train Loss: 2.0307, Train Accuracy: 34.20%, Temperatures:(0.00, 0.50)\n",
      "Old & New Losses 2028.6455154418945 2033.775806427002 Probab: tensor(3.2485e-05, device='cuda:0')\n",
      "Epoch 758, LR: 0.0153, Train Loss: 2.0308, Train Accuracy: 33.00%, Temperatures:(0.00, 0.49)\n",
      "Old & New Losses 2029.573678970337 2030.9371948242188 Probab: tensor(0.0624, device='cuda:0')\n",
      "Epoch 759, LR: 0.0153, Train Loss: 2.0275, Train Accuracy: 34.50%, Temperatures:(0.00, 0.49)\n",
      "Old & New Losses 2028.259515762329 2042.6552295684814 Probab: tensor(1.4124e-13, device='cuda:0')\n",
      "Epoch 760, LR: 0.0153, Train Loss: 2.0290, Train Accuracy: 32.10%, Temperatures:(0.00, 0.48)\n",
      "Old & New Losses 2027.9910564422607 2032.463788986206 Probab: tensor(9.2720e-05, device='cuda:0')\n",
      "Epoch 761, LR: 0.0153, Train Loss: 2.0265, Train Accuracy: 33.30%, Temperatures:(0.00, 0.48)\n",
      "Old & New Losses 2026.1139869689941 2035.8028411865234 Probab: tensor(1.4991e-09, device='cuda:0')\n",
      "Epoch 762, LR: 0.0153, Train Loss: 2.0262, Train Accuracy: 34.70%, Temperatures:(0.00, 0.47)\n",
      "Old & New Losses 2022.6194858551025 2029.4506549835205 Probab: tensor(5.1956e-07, device='cuda:0')\n",
      "Epoch 763, LR: 0.0153, Train Loss: 2.0279, Train Accuracy: 33.60%, Temperatures:(0.00, 0.47)\n",
      "Old & New Losses 2028.637409210205 2032.329797744751 Probab: tensor(0.0004, device='cuda:0')\n",
      "Epoch 764, LR: 0.0153, Train Loss: 2.0276, Train Accuracy: 32.70%, Temperatures:(0.00, 0.46)\n",
      "Old & New Losses 2027.4534225463867 2050.645589828491 Probab: tensor(1.7025e-22, device='cuda:0')\n",
      "Epoch 765, LR: 0.0153, Train Loss: 2.0354, Train Accuracy: 32.70%, Temperatures:(0.00, 0.46)\n",
      "Old & New Losses 2027.4510383605957 2038.8495922088623 Probab: tensor(1.5591e-11, device='cuda:0')\n",
      "Epoch 766, LR: 0.0153, Train Loss: 2.0269, Train Accuracy: 34.50%, Temperatures:(0.00, 0.45)\n",
      "Old & New Losses 2022.4437713623047 2028.1946659088135 Probab: tensor(3.1076e-06, device='cuda:0')\n",
      "Epoch 767, LR: 0.0153, Train Loss: 2.0301, Train Accuracy: 32.90%, Temperatures:(0.00, 0.45)\n",
      "Old & New Losses 2027.2963047027588 2042.754888534546 Probab: tensor(1.1115e-15, device='cuda:0')\n",
      "Epoch 768, LR: 0.0153, Train Loss: 2.0266, Train Accuracy: 33.80%, Temperatures:(0.00, 0.44)\n",
      "Old & New Losses 2026.902675628662 2025.9311199188232 Probab: tensor(8.8990, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 769, LR: 0.0153, Train Loss: 2.0278, Train Accuracy: 32.80%, Temperatures:(0.00, 0.44)\n",
      "Old & New Losses 2027.8446674346924 2028.487205505371 Probab: tensor(0.2322, device='cuda:0')\n",
      "Epoch 770, LR: 0.0153, Train Loss: 2.0266, Train Accuracy: 32.10%, Temperatures:(0.00, 0.44)\n",
      "Old & New Losses 2023.606777191162 2024.9955654144287 Probab: tensor(0.0412, device='cuda:0')\n",
      "Epoch 771, LR: 0.0153, Train Loss: 2.0317, Train Accuracy: 33.40%, Temperatures:(0.00, 0.43)\n",
      "Old & New Losses 2034.292459487915 2028.5868644714355 Probab: tensor(556924.3125, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 772, LR: 0.0153, Train Loss: 2.0251, Train Accuracy: 32.00%, Temperatures:(0.00, 0.43)\n",
      "Old & New Losses 2024.991512298584 2024.0998268127441 Probab: tensor(8.0731, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 773, LR: 0.0153, Train Loss: 2.0285, Train Accuracy: 33.50%, Temperatures:(0.00, 0.42)\n",
      "Old & New Losses 2025.1715183258057 2026.5982151031494 Probab: tensor(0.0342, device='cuda:0')\n",
      "Epoch 774, LR: 0.0153, Train Loss: 2.0310, Train Accuracy: 33.10%, Temperatures:(0.00, 0.42)\n",
      "Old & New Losses 2030.0359725952148 2025.5930423736572 Probab: tensor(40849.9727, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 775, LR: 0.0153, Train Loss: 2.0248, Train Accuracy: 33.90%, Temperatures:(0.00, 0.41)\n",
      "Old & New Losses 2022.6528644561768 2029.3843746185303 Probab: tensor(8.7697e-08, device='cuda:0')\n",
      "Epoch 776, LR: 0.0153, Train Loss: 2.0315, Train Accuracy: 32.30%, Temperatures:(0.00, 0.41)\n",
      "Old & New Losses 2029.6165943145752 2036.4060401916504 Probab: tensor(6.4618e-08, device='cuda:0')\n",
      "Epoch 777, LR: 0.0153, Train Loss: 2.0289, Train Accuracy: 33.00%, Temperatures:(0.00, 0.41)\n",
      "Old & New Losses 2027.0318984985352 2026.7665386199951 Probab: tensor(1.9224, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 778, LR: 0.0153, Train Loss: 2.0285, Train Accuracy: 31.00%, Temperatures:(0.00, 0.40)\n",
      "Old & New Losses 2035.9857082366943 2032.0534706115723 Probab: tensor(17724.3027, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 779, LR: 0.0153, Train Loss: 2.0336, Train Accuracy: 32.40%, Temperatures:(0.00, 0.40)\n",
      "Old & New Losses 2030.033826828003 2044.3792343139648 Probab: tensor(2.2080e-16, device='cuda:0')\n",
      "Epoch 780, LR: 0.0153, Train Loss: 2.0330, Train Accuracy: 32.60%, Temperatures:(0.00, 0.39)\n",
      "Old & New Losses 2032.1152210235596 2028.6054611206055 Probab: tensor(7397.7544, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 781, LR: 0.0153, Train Loss: 2.0284, Train Accuracy: 33.40%, Temperatures:(0.00, 0.39)\n",
      "Old & New Losses 2035.412073135376 2043.25532913208 Probab: tensor(1.8467e-09, device='cuda:0')\n",
      "Epoch 782, LR: 0.0153, Train Loss: 2.0317, Train Accuracy: 32.70%, Temperatures:(0.00, 0.39)\n",
      "Old & New Losses 2030.3025245666504 2037.3201370239258 Probab: tensor(1.2789e-08, device='cuda:0')\n",
      "Epoch 783, LR: 0.0153, Train Loss: 2.0336, Train Accuracy: 31.80%, Temperatures:(0.00, 0.38)\n",
      "Old & New Losses 2035.38179397583 2037.888526916504 Probab: tensor(0.0014, device='cuda:0')\n",
      "Epoch 784, LR: 0.0153, Train Loss: 2.0312, Train Accuracy: 33.40%, Temperatures:(0.00, 0.38)\n",
      "Old & New Losses 2037.4414920806885 2032.8896045684814 Probab: tensor(167402.0156, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 785, LR: 0.0153, Train Loss: 2.0304, Train Accuracy: 32.30%, Temperatures:(0.00, 0.37)\n",
      "Old & New Losses 2031.7354202270508 2031.5299034118652 Probab: tensor(1.7307, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 786, LR: 0.0153, Train Loss: 2.0338, Train Accuracy: 31.70%, Temperatures:(0.00, 0.37)\n",
      "Old & New Losses 2029.6082496643066 2034.649133682251 Probab: tensor(1.2520e-06, device='cuda:0')\n",
      "Epoch 787, LR: 0.0153, Train Loss: 2.0299, Train Accuracy: 32.60%, Temperatures:(0.00, 0.37)\n",
      "Old & New Losses 2031.5961837768555 2042.778730392456 Probab: tensor(5.9436e-14, device='cuda:0')\n",
      "Epoch 788, LR: 0.0153, Train Loss: 2.0332, Train Accuracy: 32.90%, Temperatures:(0.00, 0.36)\n",
      "Old & New Losses 2033.048391342163 2034.6746444702148 Probab: tensor(0.0114, device='cuda:0')\n",
      "Epoch 789, LR: 0.0153, Train Loss: 2.0336, Train Accuracy: 32.20%, Temperatures:(0.00, 0.36)\n",
      "Old & New Losses 2035.409688949585 2036.121129989624 Probab: tensor(0.1385, device='cuda:0')\n",
      "Epoch 790, LR: 0.0153, Train Loss: 2.0297, Train Accuracy: 32.70%, Temperatures:(0.00, 0.36)\n",
      "Old & New Losses 2029.1111469268799 2031.6393375396729 Probab: tensor(0.0008, device='cuda:0')\n",
      "Epoch 791, LR: 0.0153, Train Loss: 2.0327, Train Accuracy: 32.00%, Temperatures:(0.00, 0.35)\n",
      "Old & New Losses 2030.017375946045 2033.5917472839355 Probab: tensor(3.9725e-05, device='cuda:0')\n",
      "Epoch 792, LR: 0.0153, Train Loss: 2.0288, Train Accuracy: 33.90%, Temperatures:(0.00, 0.35)\n",
      "Old & New Losses 2035.8402729034424 2034.8215103149414 Probab: tensor(18.4935, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 793, LR: 0.0153, Train Loss: 2.0353, Train Accuracy: 32.80%, Temperatures:(0.00, 0.35)\n",
      "Old & New Losses 2030.4410457611084 2036.7023944854736 Probab: tensor(1.3621e-08, device='cuda:0')\n",
      "Epoch 794, LR: 0.0153, Train Loss: 2.0328, Train Accuracy: 33.90%, Temperatures:(0.00, 0.34)\n",
      "Old & New Losses 2033.6594581604004 2034.4698429107666 Probab: tensor(0.0937, device='cuda:0')\n",
      "Epoch 795, LR: 0.0153, Train Loss: 2.0316, Train Accuracy: 32.80%, Temperatures:(0.00, 0.34)\n",
      "Old & New Losses 2031.0425758361816 2042.7205562591553 Probab: tensor(1.0757e-15, device='cuda:0')\n",
      "Epoch 796, LR: 0.0153, Train Loss: 2.0338, Train Accuracy: 32.20%, Temperatures:(0.00, 0.34)\n",
      "Old & New Losses 2033.0071449279785 2027.268648147583 Probab: tensor(26892834., device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 797, LR: 0.0153, Train Loss: 2.0340, Train Accuracy: 33.50%, Temperatures:(0.00, 0.33)\n",
      "Old & New Losses 2030.705213546753 2045.2752113342285 Probab: tensor(8.8241e-20, device='cuda:0')\n",
      "Epoch 798, LR: 0.0153, Train Loss: 2.0318, Train Accuracy: 31.90%, Temperatures:(0.00, 0.33)\n",
      "Old & New Losses 2035.7294082641602 2039.4299030303955 Probab: tensor(1.2933e-05, device='cuda:0')\n",
      "Epoch 799, LR: 0.0153, Train Loss: 2.0306, Train Accuracy: 32.10%, Temperatures:(0.00, 0.33)\n",
      "Old & New Losses 2029.5276641845703 2030.496597290039 Probab: tensor(0.0509, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 800, LR: 0.0153, Train Loss: 2.0320, Train Accuracy: 31.90%, Temperatures:(0.00, 0.32)\n",
      "Old & New Losses 2034.5427989959717 2054.640293121338 Probab: tensor(8.1733e-28, device='cuda:0')\n",
      "Epoch 801, LR: 0.0153, Train Loss: 2.0352, Train Accuracy: 32.30%, Temperatures:(0.00, 0.32)\n",
      "Old & New Losses 2031.898021697998 2033.6332321166992 Probab: tensor(0.0043, device='cuda:0')\n",
      "Epoch 802, LR: 0.0076, Train Loss: 2.0307, Train Accuracy: 33.70%, Temperatures:(0.00, 0.32)\n",
      "Old & New Losses 2035.1166725158691 2033.1840515136719 Probab: tensor(454.6673, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 803, LR: 0.0076, Train Loss: 2.0339, Train Accuracy: 31.90%, Temperatures:(0.00, 0.31)\n",
      "Old & New Losses 2030.1172733306885 2036.5607738494873 Probab: tensor(1.1209e-09, device='cuda:0')\n",
      "Epoch 804, LR: 0.0076, Train Loss: 2.0337, Train Accuracy: 32.00%, Temperatures:(0.00, 0.31)\n",
      "Old & New Losses 2034.6524715423584 2040.6749248504639 Probab: tensor(3.5474e-09, device='cuda:0')\n",
      "Epoch 805, LR: 0.0076, Train Loss: 2.0386, Train Accuracy: 33.90%, Temperatures:(0.00, 0.31)\n",
      "Old & New Losses 2032.7458381652832 2029.024600982666 Probab: tensor(187931.0312, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 806, LR: 0.0076, Train Loss: 2.0392, Train Accuracy: 32.60%, Temperatures:(0.00, 0.30)\n",
      "Old & New Losses 2031.5892696380615 2029.214859008789 Probab: tensor(2507.1094, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 807, LR: 0.0076, Train Loss: 2.0355, Train Accuracy: 33.90%, Temperatures:(0.00, 0.30)\n",
      "Old & New Losses 2026.1662006378174 2031.8045616149902 Probab: tensor(7.0254e-09, device='cuda:0')\n",
      "Epoch 808, LR: 0.0076, Train Loss: 2.0283, Train Accuracy: 34.30%, Temperatures:(0.00, 0.30)\n",
      "Old & New Losses 2029.7114849090576 2028.8310050964355 Probab: tensor(19.3230, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 809, LR: 0.0076, Train Loss: 2.0280, Train Accuracy: 33.50%, Temperatures:(0.00, 0.29)\n",
      "Old & New Losses 2029.047966003418 2028.0451774597168 Probab: tensor(30.1663, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 810, LR: 0.0076, Train Loss: 2.0282, Train Accuracy: 33.20%, Temperatures:(0.00, 0.29)\n",
      "Old & New Losses 2029.388666152954 2026.3140201568604 Probab: tensor(38209.9336, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 811, LR: 0.0076, Train Loss: 2.0249, Train Accuracy: 33.90%, Temperatures:(0.00, 0.29)\n",
      "Old & New Losses 2030.137062072754 2030.9863090515137 Probab: tensor(0.0527, device='cuda:0')\n",
      "Epoch 812, LR: 0.0076, Train Loss: 2.0309, Train Accuracy: 32.80%, Temperatures:(0.00, 0.29)\n",
      "Old & New Losses 2028.099775314331 2035.1500511169434 Probab: tensor(1.9036e-11, device='cuda:0')\n",
      "Epoch 813, LR: 0.0076, Train Loss: 2.0324, Train Accuracy: 33.30%, Temperatures:(0.00, 0.28)\n",
      "Old & New Losses 2024.4956016540527 2031.0418605804443 Probab: tensor(8.8188e-11, device='cuda:0')\n",
      "Epoch 814, LR: 0.0076, Train Loss: 2.0304, Train Accuracy: 32.30%, Temperatures:(0.00, 0.28)\n",
      "Old & New Losses 2028.3353328704834 2027.3888111114502 Probab: tensor(29.4086, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 815, LR: 0.0076, Train Loss: 2.0241, Train Accuracy: 33.50%, Temperatures:(0.00, 0.28)\n",
      "Old & New Losses 2029.1249752044678 2025.5861282348633 Probab: tensor(351381.0938, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 816, LR: 0.0076, Train Loss: 2.0255, Train Accuracy: 32.80%, Temperatures:(0.00, 0.27)\n",
      "Old & New Losses 2031.3100814819336 2031.9163799285889 Probab: tensor(0.1097, device='cuda:0')\n",
      "Epoch 817, LR: 0.0076, Train Loss: 2.0289, Train Accuracy: 33.30%, Temperatures:(0.00, 0.27)\n",
      "Old & New Losses 2032.0706367492676 2032.7339172363281 Probab: tensor(0.0870, device='cuda:0')\n",
      "Epoch 818, LR: 0.0076, Train Loss: 2.0303, Train Accuracy: 32.80%, Temperatures:(0.00, 0.27)\n",
      "Old & New Losses 2026.4294147491455 2028.5511016845703 Probab: tensor(0.0004, device='cuda:0')\n",
      "Epoch 819, LR: 0.0076, Train Loss: 2.0270, Train Accuracy: 33.80%, Temperatures:(0.00, 0.27)\n",
      "Old & New Losses 2028.5844802856445 2028.2511711120605 Probab: tensor(3.4975, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 820, LR: 0.0076, Train Loss: 2.0289, Train Accuracy: 34.30%, Temperatures:(0.00, 0.26)\n",
      "Old & New Losses 2033.496379852295 2049.9138832092285 Probab: tensor(8.8303e-28, device='cuda:0')\n",
      "Epoch 821, LR: 0.0076, Train Loss: 2.0297, Train Accuracy: 33.00%, Temperatures:(0.00, 0.26)\n",
      "Old & New Losses 2024.8620510101318 2035.7661247253418 Probab: tensor(7.0783e-19, device='cuda:0')\n",
      "Epoch 822, LR: 0.0076, Train Loss: 2.0234, Train Accuracy: 33.10%, Temperatures:(0.00, 0.26)\n",
      "Old & New Losses 2029.982089996338 2028.0683040618896 Probab: tensor(1650.8817, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 823, LR: 0.0076, Train Loss: 2.0294, Train Accuracy: 31.40%, Temperatures:(0.00, 0.26)\n",
      "Old & New Losses 2029.6175479888916 2025.6931781768799 Probab: tensor(4622001., device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 824, LR: 0.0076, Train Loss: 2.0314, Train Accuracy: 33.30%, Temperatures:(0.00, 0.25)\n",
      "Old & New Losses 2029.5841693878174 2032.9108238220215 Probab: tensor(1.9643e-06, device='cuda:0')\n",
      "Epoch 825, LR: 0.0076, Train Loss: 2.0338, Train Accuracy: 30.70%, Temperatures:(0.00, 0.25)\n",
      "Old & New Losses 2028.8417339324951 2042.3665046691895 Probab: tensor(3.6666e-24, device='cuda:0')\n",
      "Epoch 826, LR: 0.0076, Train Loss: 2.0245, Train Accuracy: 31.50%, Temperatures:(0.00, 0.25)\n",
      "Old & New Losses 2026.2274742126465 2033.6577892303467 Probab: tensor(9.8784e-14, device='cuda:0')\n",
      "Epoch 827, LR: 0.0076, Train Loss: 2.0287, Train Accuracy: 30.90%, Temperatures:(0.00, 0.25)\n",
      "Old & New Losses 2032.5160026550293 2029.8497676849365 Probab: tensor(51744.1602, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 828, LR: 0.0076, Train Loss: 2.0298, Train Accuracy: 30.20%, Temperatures:(0.00, 0.24)\n",
      "Old & New Losses 2035.249948501587 2071.9192028045654 Probab: tensor(0., device='cuda:0')\n",
      "Epoch 829, LR: 0.0076, Train Loss: 2.0324, Train Accuracy: 32.60%, Temperatures:(0.00, 0.24)\n",
      "Old & New Losses 2032.4883460998535 2023.05006980896 Probab: tensor(1.0606e+17, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 830, LR: 0.0076, Train Loss: 2.0339, Train Accuracy: 30.80%, Temperatures:(0.00, 0.24)\n",
      "Old & New Losses 2020.6122398376465 2020.5824375152588 Probab: tensor(1.1332, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 831, LR: 0.0076, Train Loss: 2.0178, Train Accuracy: 32.80%, Temperatures:(0.00, 0.24)\n",
      "Old & New Losses 2021.8360424041748 2027.059555053711 Probab: tensor(2.4327e-10, device='cuda:0')\n",
      "Epoch 832, LR: 0.0076, Train Loss: 2.0230, Train Accuracy: 30.60%, Temperatures:(0.00, 0.23)\n",
      "Old & New Losses 2017.9133415222168 2023.5085487365723 Probab: tensor(3.9624e-11, device='cuda:0')\n",
      "Epoch 833, LR: 0.0076, Train Loss: 2.0191, Train Accuracy: 31.70%, Temperatures:(0.00, 0.23)\n",
      "Old & New Losses 2021.4319229125977 2029.1099548339844 Probab: tensor(3.8159e-15, device='cuda:0')\n",
      "Epoch 834, LR: 0.0076, Train Loss: 2.0237, Train Accuracy: 32.10%, Temperatures:(0.00, 0.23)\n",
      "Old & New Losses 2018.2099342346191 2020.3368663787842 Probab: tensor(9.2371e-05, device='cuda:0')\n",
      "Epoch 835, LR: 0.0076, Train Loss: 2.0206, Train Accuracy: 31.80%, Temperatures:(0.00, 0.23)\n",
      "Old & New Losses 2019.4926261901855 2018.1396007537842 Probab: tensor(391.2051, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 836, LR: 0.0076, Train Loss: 2.0144, Train Accuracy: 31.80%, Temperatures:(0.00, 0.22)\n",
      "Old & New Losses 2021.2938785552979 2020.4954147338867 Probab: tensor(35.1006, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 837, LR: 0.0076, Train Loss: 2.0236, Train Accuracy: 31.40%, Temperatures:(0.00, 0.22)\n",
      "Old & New Losses 2021.440029144287 2019.24467086792 Probab: tensor(19576.0254, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 838, LR: 0.0076, Train Loss: 2.0241, Train Accuracy: 32.60%, Temperatures:(0.00, 0.22)\n",
      "Old & New Losses 2020.2066898345947 2023.5254764556885 Probab: tensor(2.7960e-07, device='cuda:0')\n",
      "Epoch 839, LR: 0.0076, Train Loss: 2.0188, Train Accuracy: 31.30%, Temperatures:(0.00, 0.22)\n",
      "Old & New Losses 2024.1632461547852 2026.106834411621 Probab: tensor(0.0001, device='cuda:0')\n",
      "Epoch 840, LR: 0.0076, Train Loss: 2.0196, Train Accuracy: 31.40%, Temperatures:(0.00, 0.22)\n",
      "Old & New Losses 2021.8541622161865 2023.1943130493164 Probab: tensor(0.0020, device='cuda:0')\n",
      "Epoch 841, LR: 0.0076, Train Loss: 2.0203, Train Accuracy: 31.10%, Temperatures:(0.00, 0.21)\n",
      "Old & New Losses 2023.9427089691162 2030.5006504058838 Probab: tensor(4.5075e-14, device='cuda:0')\n",
      "Epoch 842, LR: 0.0076, Train Loss: 2.0208, Train Accuracy: 31.80%, Temperatures:(0.00, 0.21)\n",
      "Old & New Losses 2019.4551944732666 2016.371488571167 Probab: tensor(2182924.7500, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 843, LR: 0.0076, Train Loss: 2.0157, Train Accuracy: 30.10%, Temperatures:(0.00, 0.21)\n",
      "Old & New Losses 2024.0752696990967 2028.752088546753 Probab: tensor(1.9452e-10, device='cuda:0')\n",
      "Epoch 844, LR: 0.0076, Train Loss: 2.0184, Train Accuracy: 31.70%, Temperatures:(0.00, 0.21)\n",
      "Old & New Losses 2027.3017883300781 2020.6882953643799 Probab: tensor(7.4319e+13, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 845, LR: 0.0076, Train Loss: 2.0232, Train Accuracy: 31.20%, Temperatures:(0.00, 0.20)\n",
      "Old & New Losses 2020.477533340454 2028.8243293762207 Probab: tensor(2.0732e-18, device='cuda:0')\n",
      "Epoch 846, LR: 0.0076, Train Loss: 2.0278, Train Accuracy: 30.00%, Temperatures:(0.00, 0.20)\n",
      "Old & New Losses 2022.8769779205322 2028.2702445983887 Probab: tensor(2.8742e-12, device='cuda:0')\n",
      "Epoch 847, LR: 0.0076, Train Loss: 2.0212, Train Accuracy: 30.40%, Temperatures:(0.00, 0.20)\n",
      "Old & New Losses 2024.946689605713 2020.4665660858154 Probab: tensor(4.8330e+09, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 848, LR: 0.0076, Train Loss: 2.0193, Train Accuracy: 30.60%, Temperatures:(0.00, 0.20)\n",
      "Old & New Losses 2020.0040340423584 2023.3242511749268 Probab: tensor(5.6304e-08, device='cuda:0')\n",
      "Epoch 849, LR: 0.0076, Train Loss: 2.0218, Train Accuracy: 31.70%, Temperatures:(0.00, 0.20)\n",
      "Old & New Losses 2029.1366577148438 2025.545597076416 Probab: tensor(83182176., device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 850, LR: 0.0076, Train Loss: 2.0206, Train Accuracy: 31.40%, Temperatures:(0.00, 0.19)\n",
      "Old & New Losses 2025.6690979003906 2034.6240997314453 Probab: tensor(1.1230e-20, device='cuda:0')\n",
      "Epoch 851, LR: 0.0076, Train Loss: 2.0225, Train Accuracy: 30.20%, Temperatures:(0.00, 0.19)\n",
      "Old & New Losses 2027.850866317749 2025.8731842041016 Probab: tensor(28203.9180, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 852, LR: 0.0038, Train Loss: 2.0215, Train Accuracy: 30.40%, Temperatures:(0.00, 0.19)\n",
      "Old & New Losses 2026.824951171875 2026.7090797424316 Probab: tensor(1.8339, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 853, LR: 0.0038, Train Loss: 2.0281, Train Accuracy: 30.50%, Temperatures:(0.00, 0.19)\n",
      "Old & New Losses 2023.5185623168945 2050.2891540527344 Probab: tensor(0., device='cuda:0')\n",
      "Epoch 854, LR: 0.0038, Train Loss: 2.0262, Train Accuracy: 30.90%, Temperatures:(0.00, 0.19)\n",
      "Old & New Losses 2022.1531391143799 2027.477741241455 Probab: tensor(4.4820e-13, device='cuda:0')\n",
      "Epoch 855, LR: 0.0038, Train Loss: 2.0304, Train Accuracy: 31.40%, Temperatures:(0.00, 0.19)\n",
      "Old & New Losses 2031.0795307159424 2035.6526374816895 Probab: tensor(1.9372e-11, device='cuda:0')\n",
      "Epoch 856, LR: 0.0038, Train Loss: 2.0285, Train Accuracy: 29.60%, Temperatures:(0.00, 0.18)\n",
      "Old & New Losses 2026.7715454101562 2034.8260402679443 Probab: tensor(8.7333e-20, device='cuda:0')\n",
      "Epoch 857, LR: 0.0038, Train Loss: 2.0255, Train Accuracy: 31.10%, Temperatures:(0.00, 0.18)\n",
      "Old & New Losses 2029.242992401123 2028.6321640014648 Probab: tensor(28.8378, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 858, LR: 0.0038, Train Loss: 2.0292, Train Accuracy: 32.20%, Temperatures:(0.00, 0.18)\n",
      "Old & New Losses 2027.8418064117432 2047.2652912139893 Probab: tensor(0., device='cuda:0')\n",
      "Epoch 859, LR: 0.0038, Train Loss: 2.0289, Train Accuracy: 30.40%, Temperatures:(0.00, 0.18)\n",
      "Old & New Losses 2023.893117904663 2028.1195640563965 Probab: tensor(4.9330e-11, device='cuda:0')\n",
      "Epoch 860, LR: 0.0038, Train Loss: 2.0220, Train Accuracy: 30.20%, Temperatures:(0.00, 0.18)\n",
      "Old & New Losses 2026.5934467315674 2026.0241031646729 Probab: tensor(25.2617, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 861, LR: 0.0038, Train Loss: 2.0246, Train Accuracy: 30.20%, Temperatures:(0.00, 0.17)\n",
      "Old & New Losses 2028.1145572662354 2028.8565158843994 Probab: tensor(0.0143, device='cuda:0')\n",
      "Epoch 862, LR: 0.0038, Train Loss: 2.0214, Train Accuracy: 30.60%, Temperatures:(0.00, 0.17)\n",
      "Old & New Losses 2029.4244289398193 2027.9486179351807 Probab: tensor(5118.8774, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 863, LR: 0.0038, Train Loss: 2.0211, Train Accuracy: 32.00%, Temperatures:(0.00, 0.17)\n",
      "Old & New Losses 2030.0383567810059 2034.0263843536377 Probab: tensor(7.5092e-11, device='cuda:0')\n",
      "Epoch 864, LR: 0.0038, Train Loss: 2.0334, Train Accuracy: 30.70%, Temperatures:(0.00, 0.17)\n",
      "Old & New Losses 2029.576301574707 2031.16774559021 Probab: tensor(8.2983e-05, device='cuda:0')\n",
      "Epoch 865, LR: 0.0038, Train Loss: 2.0270, Train Accuracy: 31.60%, Temperatures:(0.00, 0.17)\n",
      "Old & New Losses 2028.315782546997 2042.1957969665527 Probab: tensor(1.1152e-36, device='cuda:0')\n",
      "Epoch 866, LR: 0.0038, Train Loss: 2.0227, Train Accuracy: 31.80%, Temperatures:(0.00, 0.17)\n",
      "Old & New Losses 2025.7539749145508 2031.1846733093262 Probab: tensor(6.1811e-15, device='cuda:0')\n",
      "Epoch 867, LR: 0.0038, Train Loss: 2.0334, Train Accuracy: 30.60%, Temperatures:(0.00, 0.16)\n",
      "Old & New Losses 2027.8029441833496 2037.5034809112549 Probab: tensor(2.3070e-26, device='cuda:0')\n",
      "Epoch 868, LR: 0.0038, Train Loss: 2.0278, Train Accuracy: 29.60%, Temperatures:(0.00, 0.16)\n",
      "Old & New Losses 2030.196189880371 2036.527156829834 Probab: tensor(1.2568e-17, device='cuda:0')\n",
      "Epoch 869, LR: 0.0038, Train Loss: 2.0313, Train Accuracy: 31.10%, Temperatures:(0.00, 0.16)\n",
      "Old & New Losses 2027.4207592010498 2027.143955230713 Probab: tensor(5.5770, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 870, LR: 0.0038, Train Loss: 2.0287, Train Accuracy: 31.50%, Temperatures:(0.00, 0.16)\n",
      "Old & New Losses 2026.965618133545 2032.7889919281006 Probab: tensor(1.3762e-16, device='cuda:0')\n",
      "Epoch 871, LR: 0.0038, Train Loss: 2.0296, Train Accuracy: 30.00%, Temperatures:(0.00, 0.16)\n",
      "Old & New Losses 2025.1779556274414 2036.2434387207031 Probab: tensor(3.5984e-31, device='cuda:0')\n",
      "Epoch 872, LR: 0.0038, Train Loss: 2.0246, Train Accuracy: 31.60%, Temperatures:(0.00, 0.16)\n",
      "Old & New Losses 2030.8995246887207 2038.1247997283936 Probab: tensor(8.3307e-21, device='cuda:0')\n",
      "Epoch 873, LR: 0.0038, Train Loss: 2.0282, Train Accuracy: 31.10%, Temperatures:(0.00, 0.15)\n",
      "Old & New Losses 2029.360055923462 2030.341625213623 Probab: tensor(0.0018, device='cuda:0')\n",
      "Epoch 874, LR: 0.0038, Train Loss: 2.0336, Train Accuracy: 32.40%, Temperatures:(0.00, 0.15)\n",
      "Old & New Losses 2028.9289951324463 2024.1212844848633 Probab: tensor(4.2864e+13, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 875, LR: 0.0038, Train Loss: 2.0283, Train Accuracy: 31.50%, Temperatures:(0.00, 0.15)\n",
      "Old & New Losses 2030.7178497314453 2027.303695678711 Probab: tensor(6.0048e+09, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 876, LR: 0.0038, Train Loss: 2.0270, Train Accuracy: 31.30%, Temperatures:(0.00, 0.15)\n",
      "Old & New Losses 2032.91654586792 2026.8933773040771 Probab: tensor(2.6622e+17, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 877, LR: 0.0038, Train Loss: 2.0310, Train Accuracy: 29.80%, Temperatures:(0.00, 0.15)\n",
      "Old & New Losses 2028.0165672302246 2032.1028232574463 Probab: tensor(1.1453e-12, device='cuda:0')\n",
      "Epoch 878, LR: 0.0038, Train Loss: 2.0290, Train Accuracy: 28.90%, Temperatures:(0.00, 0.15)\n",
      "Old & New Losses 2031.5325260162354 2030.1544666290283 Probab: tensor(11687.5410, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 879, LR: 0.0038, Train Loss: 2.0270, Train Accuracy: 30.90%, Temperatures:(0.00, 0.15)\n",
      "Old & New Losses 2032.785415649414 2029.8418998718262 Probab: tensor(5.9754e+08, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 880, LR: 0.0038, Train Loss: 2.0333, Train Accuracy: 29.40%, Temperatures:(0.00, 0.14)\n",
      "Old & New Losses 2030.5287837982178 2054.5074939727783 Probab: tensor(0., device='cuda:0')\n",
      "Epoch 881, LR: 0.0038, Train Loss: 2.0337, Train Accuracy: 29.00%, Temperatures:(0.00, 0.14)\n",
      "Old & New Losses 2032.2513580322266 2028.4209251403809 Probab: tensor(4.4945e+11, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 882, LR: 0.0038, Train Loss: 2.0282, Train Accuracy: 30.20%, Temperatures:(0.00, 0.14)\n",
      "Old & New Losses 2030.698299407959 2031.294822692871 Probab: tensor(0.0147, device='cuda:0')\n",
      "Epoch 883, LR: 0.0038, Train Loss: 2.0299, Train Accuracy: 30.30%, Temperatures:(0.00, 0.14)\n",
      "Old & New Losses 2031.1243534088135 2032.5336456298828 Probab: tensor(4.2238e-05, device='cuda:0')\n",
      "Epoch 884, LR: 0.0038, Train Loss: 2.0340, Train Accuracy: 28.90%, Temperatures:(0.00, 0.14)\n",
      "Old & New Losses 2033.649206161499 2041.0141944885254 Probab: tensor(8.1080e-24, device='cuda:0')\n",
      "Epoch 885, LR: 0.0038, Train Loss: 2.0301, Train Accuracy: 29.90%, Temperatures:(0.00, 0.14)\n",
      "Old & New Losses 2031.038761138916 2034.841537475586 Probab: tensor(9.0555e-13, device='cuda:0')\n",
      "Epoch 886, LR: 0.0038, Train Loss: 2.0280, Train Accuracy: 28.00%, Temperatures:(0.00, 0.14)\n",
      "Old & New Losses 2032.6414108276367 2030.874252319336 Probab: tensor(449758.5312, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 887, LR: 0.0038, Train Loss: 2.0371, Train Accuracy: 28.00%, Temperatures:(0.00, 0.13)\n",
      "Old & New Losses 2031.5501689910889 2029.0248394012451 Probab: tensor(1.4451e+08, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 888, LR: 0.0038, Train Loss: 2.0352, Train Accuracy: 29.40%, Temperatures:(0.00, 0.13)\n",
      "Old & New Losses 2031.618595123291 2033.216953277588 Probab: tensor(6.0699e-06, device='cuda:0')\n",
      "Epoch 889, LR: 0.0038, Train Loss: 2.0325, Train Accuracy: 28.40%, Temperatures:(0.00, 0.13)\n",
      "Old & New Losses 2024.3465900421143 2024.4605541229248 Probab: tensor(0.4210, device='cuda:0')\n",
      "Epoch 890, LR: 0.0038, Train Loss: 2.0335, Train Accuracy: 29.00%, Temperatures:(0.00, 0.13)\n",
      "Old & New Losses 2028.0964374542236 2029.5178890228271 Probab: tensor(1.8466e-05, device='cuda:0')\n",
      "Epoch 891, LR: 0.0038, Train Loss: 2.0286, Train Accuracy: 30.30%, Temperatures:(0.00, 0.13)\n",
      "Old & New Losses 2025.6946086883545 2038.949728012085 Probab: tensor(2.8026e-45, device='cuda:0')\n",
      "Epoch 892, LR: 0.0038, Train Loss: 2.0245, Train Accuracy: 28.70%, Temperatures:(0.00, 0.13)\n",
      "Old & New Losses 2035.7306003570557 2038.0406379699707 Probab: tensor(1.4161e-08, device='cuda:0')\n",
      "Epoch 893, LR: 0.0038, Train Loss: 2.0290, Train Accuracy: 29.40%, Temperatures:(0.00, 0.13)\n",
      "Old & New Losses 2030.8213233947754 2034.5439910888672 Probab: tensor(1.6733e-13, device='cuda:0')\n",
      "Epoch 894, LR: 0.0038, Train Loss: 2.0304, Train Accuracy: 30.80%, Temperatures:(0.00, 0.13)\n",
      "Old & New Losses 2026.1454582214355 2031.3045978546143 Probab: tensor(1.3021e-18, device='cuda:0')\n",
      "Epoch 895, LR: 0.0038, Train Loss: 2.0285, Train Accuracy: 27.00%, Temperatures:(0.00, 0.12)\n",
      "Old & New Losses 2027.5294780731201 2027.745246887207 Probab: tensor(0.1756, device='cuda:0')\n",
      "Epoch 896, LR: 0.0038, Train Loss: 2.0278, Train Accuracy: 29.40%, Temperatures:(0.00, 0.12)\n",
      "Old & New Losses 2027.7445316314697 2034.5983505249023 Probab: tensor(5.7176e-25, device='cuda:0')\n",
      "Epoch 897, LR: 0.0038, Train Loss: 2.0298, Train Accuracy: 29.20%, Temperatures:(0.00, 0.12)\n",
      "Old & New Losses 2025.1812934875488 2035.6361865997314 Probab: tensor(4.4283e-38, device='cuda:0')\n",
      "Epoch 898, LR: 0.0038, Train Loss: 2.0283, Train Accuracy: 29.30%, Temperatures:(0.00, 0.12)\n",
      "Old & New Losses 2028.2936096191406 2044.6200370788574 Probab: tensor(0., device='cuda:0')\n",
      "Epoch 899, LR: 0.0038, Train Loss: 2.0280, Train Accuracy: 30.20%, Temperatures:(0.00, 0.12)\n",
      "Old & New Losses 2027.634620666504 2030.7602882385254 Probab: tensor(4.0338e-12, device='cuda:0')\n",
      "Epoch 900, LR: 0.0038, Train Loss: 2.0289, Train Accuracy: 28.70%, Temperatures:(0.00, 0.12)\n",
      "Old & New Losses 2026.503562927246 2027.1282196044922 Probab: tensor(0.0050, device='cuda:0')\n",
      "Epoch 901, LR: 0.0038, Train Loss: 2.0304, Train Accuracy: 30.40%, Temperatures:(0.00, 0.12)\n",
      "Old & New Losses 2030.7722091674805 2029.7682285308838 Probab: tensor(5422.6924, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 902, LR: 0.0019, Train Loss: 2.0346, Train Accuracy: 29.00%, Temperatures:(0.00, 0.12)\n",
      "Old & New Losses 2021.5938091278076 2024.3635177612305 Probab: tensor(3.9288e-11, device='cuda:0')\n",
      "Epoch 903, LR: 0.0019, Train Loss: 2.0259, Train Accuracy: 30.30%, Temperatures:(0.00, 0.11)\n",
      "Old & New Losses 2020.1804637908936 2034.048318862915 Probab: tensor(0., device='cuda:0')\n",
      "Epoch 904, LR: 0.0019, Train Loss: 2.0236, Train Accuracy: 29.90%, Temperatures:(0.00, 0.11)\n",
      "Old & New Losses 2022.817850112915 2035.3198051452637 Probab: tensor(0., device='cuda:0')\n",
      "Epoch 905, LR: 0.0019, Train Loss: 2.0229, Train Accuracy: 28.70%, Temperatures:(0.00, 0.11)\n",
      "Old & New Losses 2023.2133865356445 2030.4529666900635 Probab: tensor(9.3000e-29, device='cuda:0')\n",
      "Epoch 906, LR: 0.0019, Train Loss: 2.0270, Train Accuracy: 29.90%, Temperatures:(0.00, 0.11)\n",
      "Old & New Losses 2024.369478225708 2031.8865776062012 Probab: tensor(3.9805e-30, device='cuda:0')\n",
      "Epoch 907, LR: 0.0019, Train Loss: 2.0238, Train Accuracy: 29.50%, Temperatures:(0.00, 0.11)\n",
      "Old & New Losses 2025.7630348205566 2034.8279476165771 Probab: tensor(1.5423e-36, device='cuda:0')\n",
      "Epoch 908, LR: 0.0019, Train Loss: 2.0252, Train Accuracy: 29.30%, Temperatures:(0.00, 0.11)\n",
      "Old & New Losses 2021.7835903167725 2025.9218215942383 Probab: tensor(3.0646e-17, device='cuda:0')\n",
      "Epoch 909, LR: 0.0019, Train Loss: 2.0270, Train Accuracy: 29.80%, Temperatures:(0.00, 0.11)\n",
      "Old & New Losses 2025.0530242919922 2026.9434452056885 Probab: tensor(2.3992e-08, device='cuda:0')\n",
      "Epoch 910, LR: 0.0019, Train Loss: 2.0249, Train Accuracy: 29.40%, Temperatures:(0.00, 0.11)\n",
      "Old & New Losses 2025.650978088379 2029.62064743042 Probab: tensor(6.8768e-17, device='cuda:0')\n",
      "Epoch 911, LR: 0.0019, Train Loss: 2.0294, Train Accuracy: 28.80%, Temperatures:(0.00, 0.11)\n",
      "Old & New Losses 2023.918628692627 2026.2057781219482 Probab: tensor(3.9242e-10, device='cuda:0')\n",
      "Epoch 912, LR: 0.0019, Train Loss: 2.0236, Train Accuracy: 31.00%, Temperatures:(0.00, 0.10)\n",
      "Old & New Losses 2022.8769779205322 2027.308464050293 Probab: tensor(3.8971e-19, device='cuda:0')\n",
      "Epoch 913, LR: 0.0019, Train Loss: 2.0246, Train Accuracy: 30.20%, Temperatures:(0.00, 0.10)\n",
      "Old & New Losses 2027.3854732513428 2022.6283073425293 Probab: tensor(9.1582e+19, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 914, LR: 0.0019, Train Loss: 2.0268, Train Accuracy: 29.40%, Temperatures:(0.00, 0.10)\n",
      "Old & New Losses 2029.0021896362305 2039.5803451538086 Probab: tensor(1.4013e-45, device='cuda:0')\n",
      "Epoch 915, LR: 0.0019, Train Loss: 2.0261, Train Accuracy: 30.00%, Temperatures:(0.00, 0.10)\n",
      "Old & New Losses 2028.2995700836182 2026.7620086669922 Probab: tensor(3826894.5000, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 916, LR: 0.0019, Train Loss: 2.0282, Train Accuracy: 29.70%, Temperatures:(0.00, 0.10)\n",
      "Old & New Losses 2030.0607681274414 2027.0450115203857 Probab: tensor(1.1014e+13, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 917, LR: 0.0019, Train Loss: 2.0278, Train Accuracy: 29.60%, Temperatures:(0.00, 0.10)\n",
      "Old & New Losses 2029.222011566162 2034.2199802398682 Probab: tensor(1.4705e-22, device='cuda:0')\n",
      "Epoch 918, LR: 0.0019, Train Loss: 2.0270, Train Accuracy: 30.70%, Temperatures:(0.00, 0.10)\n",
      "Old & New Losses 2023.4031677246094 2023.7889289855957 Probab: tensor(0.0199, device='cuda:0')\n",
      "Epoch 919, LR: 0.0019, Train Loss: 2.0290, Train Accuracy: 29.60%, Temperatures:(0.00, 0.10)\n",
      "Old & New Losses 2021.7247009277344 2025.6333351135254 Probab: tensor(3.7958e-18, device='cuda:0')\n",
      "Epoch 920, LR: 0.0019, Train Loss: 2.0277, Train Accuracy: 28.10%, Temperatures:(0.00, 0.10)\n",
      "Old & New Losses 2028.5143852233887 2026.8304347991943 Probab: tensor(38118968., device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 921, LR: 0.0019, Train Loss: 2.0328, Train Accuracy: 28.80%, Temperatures:(0.00, 0.10)\n",
      "Old & New Losses 2023.9760875701904 2042.6342487335205 Probab: tensor(0., device='cuda:0')\n",
      "Epoch 922, LR: 0.0019, Train Loss: 2.0281, Train Accuracy: 29.60%, Temperatures:(0.00, 0.09)\n",
      "Old & New Losses 2026.2248516082764 2027.3847579956055 Probab: tensor(4.6998e-06, device='cuda:0')\n",
      "Epoch 923, LR: 0.0019, Train Loss: 2.0324, Train Accuracy: 30.30%, Temperatures:(0.00, 0.09)\n",
      "Old & New Losses 2027.754306793213 2038.7976169586182 Probab: tensor(0., device='cuda:0')\n",
      "Epoch 924, LR: 0.0019, Train Loss: 2.0287, Train Accuracy: 30.60%, Temperatures:(0.00, 0.09)\n",
      "Old & New Losses 2028.1717777252197 2028.3558368682861 Probab: tensor(0.1372, device='cuda:0')\n",
      "Epoch 925, LR: 0.0019, Train Loss: 2.0275, Train Accuracy: 29.50%, Temperatures:(0.00, 0.09)\n",
      "Old & New Losses 2023.078441619873 2028.2981395721436 Probab: tensor(1.9493e-25, device='cuda:0')\n",
      "Epoch 926, LR: 0.0019, Train Loss: 2.0218, Train Accuracy: 30.40%, Temperatures:(0.00, 0.09)\n",
      "Old & New Losses 2021.8467712402344 2030.0862789154053 Probab: tensor(3.9818e-40, device='cuda:0')\n",
      "Epoch 927, LR: 0.0019, Train Loss: 2.0274, Train Accuracy: 29.20%, Temperatures:(0.00, 0.09)\n",
      "Old & New Losses 2032.4368476867676 2026.878833770752 Probab: tensor(7.0135e+26, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 928, LR: 0.0019, Train Loss: 2.0292, Train Accuracy: 29.60%, Temperatures:(0.00, 0.09)\n",
      "Old & New Losses 2030.2813053131104 2025.9454250335693 Probab: tensor(1.4270e+21, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 929, LR: 0.0019, Train Loss: 2.0260, Train Accuracy: 31.50%, Temperatures:(0.00, 0.09)\n",
      "Old & New Losses 2032.994031906128 2027.1596908569336 Probab: tensor(5.6599e+28, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 930, LR: 0.0019, Train Loss: 2.0312, Train Accuracy: 30.80%, Temperatures:(0.00, 0.09)\n",
      "Old & New Losses 2028.2180309295654 2034.7990989685059 Probab: tensor(1.7360e-33, device='cuda:0')\n",
      "Epoch 931, LR: 0.0019, Train Loss: 2.0296, Train Accuracy: 29.50%, Temperatures:(0.00, 0.09)\n",
      "Old & New Losses 2027.3823738098145 2031.3200950622559 Probab: tensor(1.5852e-20, device='cuda:0')\n",
      "Epoch 932, LR: 0.0019, Train Loss: 2.0292, Train Accuracy: 31.20%, Temperatures:(0.00, 0.09)\n",
      "Old & New Losses 2034.2340469360352 2027.3478031158447 Probab: tensor(9.4539e+34, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 933, LR: 0.0019, Train Loss: 2.0351, Train Accuracy: 30.40%, Temperatures:(0.00, 0.08)\n",
      "Old & New Losses 2034.3737602233887 2033.1969261169434 Probab: tensor(1090389.1250, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 934, LR: 0.0019, Train Loss: 2.0306, Train Accuracy: 28.20%, Temperatures:(0.00, 0.08)\n",
      "Old & New Losses 2035.062551498413 2039.1724109649658 Probab: tensor(5.0346e-22, device='cuda:0')\n",
      "Epoch 935, LR: 0.0019, Train Loss: 2.0308, Train Accuracy: 28.70%, Temperatures:(0.00, 0.08)\n",
      "Old & New Losses 2031.3827991485596 2031.2650203704834 Probab: tensor(4.1354, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 936, LR: 0.0019, Train Loss: 2.0354, Train Accuracy: 29.80%, Temperatures:(0.00, 0.08)\n",
      "Old & New Losses 2030.4996967315674 2029.344081878662 Probab: tensor(1288837.2500, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 937, LR: 0.0019, Train Loss: 2.0321, Train Accuracy: 29.50%, Temperatures:(0.00, 0.08)\n",
      "Old & New Losses 2034.513235092163 2040.5166149139404 Probab: tensor(8.6517e-33, device='cuda:0')\n",
      "Epoch 938, LR: 0.0019, Train Loss: 2.0323, Train Accuracy: 29.00%, Temperatures:(0.00, 0.08)\n",
      "Old & New Losses 2034.8188877105713 2040.3640270233154 Probab: tensor(1.2171e-30, device='cuda:0')\n",
      "Epoch 939, LR: 0.0019, Train Loss: 2.0347, Train Accuracy: 29.60%, Temperatures:(0.00, 0.08)\n",
      "Old & New Losses 2035.330057144165 2031.1760902404785 Probab: tensor(4.3251e+22, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 940, LR: 0.0019, Train Loss: 2.0351, Train Accuracy: 29.40%, Temperatures:(0.00, 0.08)\n",
      "Old & New Losses 2034.665584564209 2038.0005836486816 Probab: tensor(4.3974e-19, device='cuda:0')\n",
      "Epoch 941, LR: 0.0019, Train Loss: 2.0370, Train Accuracy: 29.30%, Temperatures:(0.00, 0.08)\n",
      "Old & New Losses 2026.3381004333496 2028.5453796386719 Probab: tensor(5.3428e-13, device='cuda:0')\n",
      "Epoch 942, LR: 0.0019, Train Loss: 2.0316, Train Accuracy: 30.00%, Temperatures:(0.00, 0.08)\n",
      "Old & New Losses 2031.3851833343506 2033.9829921722412 Probab: tensor(2.5738e-15, device='cuda:0')\n",
      "Epoch 943, LR: 0.0019, Train Loss: 2.0338, Train Accuracy: 29.80%, Temperatures:(0.00, 0.08)\n",
      "Old & New Losses 2033.2295894622803 2035.346269607544 Probab: tensor(9.8298e-13, device='cuda:0')\n",
      "Epoch 944, LR: 0.0019, Train Loss: 2.0345, Train Accuracy: 30.10%, Temperatures:(0.00, 0.08)\n",
      "Old & New Losses 2032.1097373962402 2036.130666732788 Probab: tensor(9.1150e-24, device='cuda:0')\n",
      "Epoch 945, LR: 0.0019, Train Loss: 2.0356, Train Accuracy: 28.00%, Temperatures:(0.00, 0.08)\n",
      "Old & New Losses 2028.0158519744873 2034.010648727417 Probab: tensor(2.0062e-35, device='cuda:0')\n",
      "Epoch 946, LR: 0.0019, Train Loss: 2.0315, Train Accuracy: 31.00%, Temperatures:(0.00, 0.07)\n",
      "Old & New Losses 2032.85551071167 2035.4528427124023 Probab: tensor(6.5298e-16, device='cuda:0')\n",
      "Epoch 947, LR: 0.0019, Train Loss: 2.0358, Train Accuracy: 31.00%, Temperatures:(0.00, 0.07)\n",
      "Old & New Losses 2030.6518077850342 2035.1626873016357 Probab: tensor(2.2967e-27, device='cuda:0')\n",
      "Epoch 948, LR: 0.0019, Train Loss: 2.0295, Train Accuracy: 29.60%, Temperatures:(0.00, 0.07)\n",
      "Old & New Losses 2035.142183303833 2049.668550491333 Probab: tensor(0., device='cuda:0')\n",
      "Epoch 949, LR: 0.0019, Train Loss: 2.0374, Train Accuracy: 30.20%, Temperatures:(0.00, 0.07)\n",
      "Old & New Losses 2035.7763767242432 2037.569522857666 Probab: tensor(1.5689e-11, device='cuda:0')\n",
      "Epoch 950, LR: 0.0019, Train Loss: 2.0373, Train Accuracy: 29.90%, Temperatures:(0.00, 0.07)\n",
      "Old & New Losses 2032.876968383789 2036.358118057251 Probab: tensor(6.4993e-22, device='cuda:0')\n",
      "Epoch 951, LR: 0.0019, Train Loss: 2.0375, Train Accuracy: 32.10%, Temperatures:(0.00, 0.07)\n",
      "Old & New Losses 2034.5323085784912 2033.6191654205322 Probab: tensor(410913.8438, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 952, LR: 0.0010, Train Loss: 2.0285, Train Accuracy: 30.10%, Temperatures:(0.00, 0.07)\n",
      "Old & New Losses 2039.1278266906738 2054.717540740967 Probab: tensor(0., device='cuda:0')\n",
      "Epoch 953, LR: 0.0010, Train Loss: 2.0393, Train Accuracy: 30.50%, Temperatures:(0.00, 0.07)\n",
      "Old & New Losses 2039.2951965332031 2039.8528575897217 Probab: tensor(0.0003, device='cuda:0')\n",
      "Epoch 954, LR: 0.0010, Train Loss: 2.0396, Train Accuracy: 29.90%, Temperatures:(0.00, 0.07)\n",
      "Old & New Losses 2036.5471839904785 2044.2156791687012 Probab: tensor(0., device='cuda:0')\n",
      "Epoch 955, LR: 0.0010, Train Loss: 2.0340, Train Accuracy: 30.30%, Temperatures:(0.00, 0.07)\n",
      "Old & New Losses 2036.4995002746582 2036.7214679718018 Probab: tensor(0.0380, device='cuda:0')\n",
      "Epoch 956, LR: 0.0010, Train Loss: 2.0343, Train Accuracy: 30.80%, Temperatures:(0.00, 0.07)\n",
      "Old & New Losses 2041.104793548584 2040.771722793579 Probab: tensor(142.2828, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 957, LR: 0.0010, Train Loss: 2.0431, Train Accuracy: 29.20%, Temperatures:(0.00, 0.07)\n",
      "Old & New Losses 2036.2355709075928 2059.377908706665 Probab: tensor(0., device='cuda:0')\n",
      "Epoch 958, LR: 0.0010, Train Loss: 2.0323, Train Accuracy: 30.50%, Temperatures:(0.00, 0.07)\n",
      "Old & New Losses 2037.8201007843018 2040.1127338409424 Probab: tensor(7.5551e-16, device='cuda:0')\n",
      "Epoch 959, LR: 0.0010, Train Loss: 2.0353, Train Accuracy: 29.30%, Temperatures:(0.00, 0.07)\n",
      "Old & New Losses 2043.6468124389648 2045.2601909637451 Probab: tensor(1.7823e-11, device='cuda:0')\n",
      "Epoch 960, LR: 0.0010, Train Loss: 2.0388, Train Accuracy: 30.30%, Temperatures:(0.00, 0.06)\n",
      "Old & New Losses 2039.3667221069336 2042.407751083374 Probab: tensor(3.4253e-21, device='cuda:0')\n",
      "Epoch 961, LR: 0.0010, Train Loss: 2.0415, Train Accuracy: 31.40%, Temperatures:(0.00, 0.06)\n",
      "Old & New Losses 2033.6813926696777 2042.3457622528076 Probab: tensor(0., device='cuda:0')\n",
      "Epoch 962, LR: 0.0010, Train Loss: 2.0439, Train Accuracy: 30.40%, Temperatures:(0.00, 0.06)\n",
      "Old & New Losses 2038.2394790649414 2039.1356945037842 Probab: tensor(7.0187e-07, device='cuda:0')\n",
      "Epoch 963, LR: 0.0010, Train Loss: 2.0358, Train Accuracy: 30.20%, Temperatures:(0.00, 0.06)\n",
      "Old & New Losses 2038.3884906768799 2038.663625717163 Probab: tensor(0.0124, device='cuda:0')\n",
      "Epoch 964, LR: 0.0010, Train Loss: 2.0362, Train Accuracy: 30.00%, Temperatures:(0.00, 0.06)\n",
      "Old & New Losses 2037.2607707977295 2043.6880588531494 Probab: tensor(1.4013e-45, device='cuda:0')\n",
      "Epoch 965, LR: 0.0010, Train Loss: 2.0392, Train Accuracy: 29.90%, Temperatures:(0.00, 0.06)\n",
      "Old & New Losses 2038.8784408569336 2036.3438129425049 Probab: tensor(8.6381e+17, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 966, LR: 0.0010, Train Loss: 2.0400, Train Accuracy: 29.60%, Temperatures:(0.00, 0.06)\n",
      "Old & New Losses 2036.6764068603516 2048.0427742004395 Probab: tensor(0., device='cuda:0')\n",
      "Epoch 967, LR: 0.0010, Train Loss: 2.0366, Train Accuracy: 30.50%, Temperatures:(0.00, 0.06)\n",
      "Old & New Losses 2035.2914333343506 2058.666944503784 Probab: tensor(0., device='cuda:0')\n",
      "Epoch 968, LR: 0.0010, Train Loss: 2.0413, Train Accuracy: 30.00%, Temperatures:(0.00, 0.06)\n",
      "Old & New Losses 2041.4597988128662 2040.3385162353516 Probab: tensor(1.5056e+08, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 969, LR: 0.0010, Train Loss: 2.0421, Train Accuracy: 29.70%, Temperatures:(0.00, 0.06)\n",
      "Old & New Losses 2048.013687133789 2046.4200973510742 Probab: tensor(5.4917e+11, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 970, LR: 0.0010, Train Loss: 2.0492, Train Accuracy: 30.40%, Temperatures:(0.00, 0.06)\n",
      "Old & New Losses 2046.6325283050537 2043.7564849853516 Probab: tensor(2.5197e+21, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 971, LR: 0.0010, Train Loss: 2.0441, Train Accuracy: 30.70%, Temperatures:(0.00, 0.06)\n",
      "Old & New Losses 2044.7139739990234 2052.81138420105 Probab: tensor(0., device='cuda:0')\n",
      "Epoch 972, LR: 0.0010, Train Loss: 2.0479, Train Accuracy: 29.70%, Temperatures:(0.00, 0.06)\n",
      "Old & New Losses 2047.9984283447266 2059.866428375244 Probab: tensor(0., device='cuda:0')\n",
      "Epoch 973, LR: 0.0010, Train Loss: 2.0464, Train Accuracy: 28.20%, Temperatures:(0.00, 0.06)\n",
      "Old & New Losses 2049.1294860839844 2054.3437004089355 Probab: tensor(1.0282e-40, device='cuda:0')\n",
      "Epoch 974, LR: 0.0010, Train Loss: 2.0491, Train Accuracy: 28.90%, Temperatures:(0.00, 0.06)\n",
      "Old & New Losses 2047.0695495605469 2050.3439903259277 Probab: tensor(4.3111e-26, device='cuda:0')\n",
      "Epoch 975, LR: 0.0010, Train Loss: 2.0475, Train Accuracy: 28.80%, Temperatures:(0.00, 0.06)\n",
      "Old & New Losses 2043.168067932129 2045.1416969299316 Probab: tensor(3.6048e-16, device='cuda:0')\n",
      "Epoch 976, LR: 0.0010, Train Loss: 2.0490, Train Accuracy: 28.50%, Temperatures:(0.00, 0.05)\n",
      "Old & New Losses 2046.5171337127686 2047.067642211914 Probab: tensor(4.4556e-05, device='cuda:0')\n",
      "Epoch 977, LR: 0.0010, Train Loss: 2.0421, Train Accuracy: 30.10%, Temperatures:(0.00, 0.05)\n",
      "Old & New Losses 2045.7265377044678 2051.6812801361084 Probab: tensor(0., device='cuda:0')\n",
      "Epoch 978, LR: 0.0010, Train Loss: 2.0492, Train Accuracy: 30.60%, Temperatures:(0.00, 0.05)\n",
      "Old & New Losses 2044.9934005737305 2047.4662780761719 Probab: tensor(1.1431e-20, device='cuda:0')\n",
      "Epoch 979, LR: 0.0010, Train Loss: 2.0518, Train Accuracy: 30.70%, Temperatures:(0.00, 0.05)\n",
      "Old & New Losses 2044.9044704437256 2046.9326972961426 Probab: tensor(3.0106e-17, device='cuda:0')\n",
      "Epoch 980, LR: 0.0010, Train Loss: 2.0472, Train Accuracy: 30.30%, Temperatures:(0.00, 0.05)\n",
      "Old & New Losses 2047.877311706543 2055.5715560913086 Probab: tensor(0., device='cuda:0')\n",
      "Epoch 981, LR: 0.0010, Train Loss: 2.0509, Train Accuracy: 30.80%, Temperatures:(0.00, 0.05)\n",
      "Old & New Losses 2045.8793640136719 2046.3671684265137 Probab: tensor(8.8269e-05, device='cuda:0')\n",
      "Epoch 982, LR: 0.0010, Train Loss: 2.0434, Train Accuracy: 29.90%, Temperatures:(0.00, 0.05)\n",
      "Old & New Losses 2044.9469089508057 2048.0737686157227 Probab: tensor(5.6216e-27, device='cuda:0')\n",
      "Epoch 983, LR: 0.0010, Train Loss: 2.0489, Train Accuracy: 29.50%, Temperatures:(0.00, 0.05)\n",
      "Old & New Losses 2048.478364944458 2053.464889526367 Probab: tensor(5.1848e-43, device='cuda:0')\n",
      "Epoch 984, LR: 0.0010, Train Loss: 2.0450, Train Accuracy: 30.40%, Temperatures:(0.00, 0.05)\n",
      "Old & New Losses 2046.5857982635498 2048.4790802001953 Probab: tensor(6.0686e-17, device='cuda:0')\n",
      "Epoch 985, LR: 0.0010, Train Loss: 2.0543, Train Accuracy: 30.30%, Temperatures:(0.00, 0.05)\n",
      "Old & New Losses 2047.9674339294434 2050.290107727051 Probab: tensor(8.0200e-21, device='cuda:0')\n",
      "Epoch 986, LR: 0.0010, Train Loss: 2.0487, Train Accuracy: 29.30%, Temperatures:(0.00, 0.05)\n",
      "Old & New Losses 2045.0873374938965 2050.564765930176 Probab: tensor(0., device='cuda:0')\n",
      "Epoch 987, LR: 0.0010, Train Loss: 2.0451, Train Accuracy: 29.10%, Temperatures:(0.00, 0.05)\n",
      "Old & New Losses 2050.7171154022217 2048.999547958374 Probab: tensor(1.4526e+15, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 988, LR: 0.0010, Train Loss: 2.0481, Train Accuracy: 30.60%, Temperatures:(0.00, 0.05)\n",
      "Old & New Losses 2053.408145904541 2050.7760047912598 Probab: tensor(2.9542e+23, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 989, LR: 0.0010, Train Loss: 2.0540, Train Accuracy: 27.30%, Temperatures:(0.00, 0.05)\n",
      "Old & New Losses 2057.015895843506 2054.1656017303467 Probab: tensor(4.7033e+25, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 990, LR: 0.0010, Train Loss: 2.0557, Train Accuracy: 28.40%, Temperatures:(0.00, 0.05)\n",
      "Old & New Losses 2054.9275875091553 2057.1529865264893 Probab: tensor(5.6692e-21, device='cuda:0')\n",
      "Epoch 991, LR: 0.0010, Train Loss: 2.0582, Train Accuracy: 28.30%, Temperatures:(0.00, 0.05)\n",
      "Old & New Losses 2055.9303760528564 2057.2168827056885 Probab: tensor(1.5040e-12, device='cuda:0')\n",
      "Epoch 992, LR: 0.0010, Train Loss: 2.0517, Train Accuracy: 29.40%, Temperatures:(0.00, 0.05)\n",
      "Old & New Losses 2053.527593612671 2052.797555923462 Probab: tensor(5979614., device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 993, LR: 0.0010, Train Loss: 2.0498, Train Accuracy: 30.10%, Temperatures:(0.00, 0.05)\n",
      "Old & New Losses 2056.9918155670166 2059.7736835479736 Probab: tensor(8.2432e-27, device='cuda:0')\n",
      "Epoch 994, LR: 0.0010, Train Loss: 2.0581, Train Accuracy: 27.50%, Temperatures:(0.00, 0.05)\n",
      "Old & New Losses 2054.579257965088 2053.2121658325195 Probab: tensor(8.8691e+12, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 995, LR: 0.0010, Train Loss: 2.0594, Train Accuracy: 29.40%, Temperatures:(0.00, 0.05)\n",
      "Old & New Losses 2054.999589920044 2055.25803565979 Probab: tensor(0.0034, device='cuda:0')\n",
      "Epoch 996, LR: 0.0010, Train Loss: 2.0492, Train Accuracy: 29.10%, Temperatures:(0.00, 0.04)\n",
      "Old & New Losses 2052.271842956543 2057.00421333313 Probab: tensor(0., device='cuda:0')\n",
      "Epoch 997, LR: 0.0010, Train Loss: 2.0596, Train Accuracy: 29.50%, Temperatures:(0.00, 0.04)\n",
      "Old & New Losses 2059.0052604675293 2051.41544342041 Probab: tensor(inf, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 998, LR: 0.0010, Train Loss: 2.0543, Train Accuracy: 28.30%, Temperatures:(0.00, 0.04)\n",
      "Old & New Losses 2050.3156185150146 2060.962677001953 Probab: tensor(0., device='cuda:0')\n",
      "Epoch 999, LR: 0.0010, Train Loss: 2.0541, Train Accuracy: 28.40%, Temperatures:(0.00, 0.04)\n",
      "Old & New Losses 2047.7943420410156 2067.971706390381 Probab: tensor(0., device='cuda:0')\n",
      "Epoch 1000, LR: 0.0010, Train Loss: 2.0520, Train Accuracy: 29.10%, Temperatures:(0.00, 0.04)\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "lr = params_RRAM[\"ext_lr\"] / 25  # Initial learning rate\n",
    "num_epochs = params_RRAM[\"epochs\"]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch == 1:\n",
    "        lr *= 5\n",
    "    elif epoch == 2:\n",
    "        lr *= 5\n",
    "\n",
    "    model_RRAM.train()\n",
    "    outputs = model_RRAM(train_in)\n",
    "    loss = criterion(outputs, train_lab)\n",
    "    loss.backward()\n",
    "    model_RRAM.backprop(lr)\n",
    "    model_RRAM.anneal(train_in, train_lab,0.99, 0.99)\n",
    "\n",
    "    _, train_preds = torch.max(outputs, dim=1)\n",
    "    train_accuracy = (train_preds == train_lab).float().mean().item() * 100\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, LR: {lr:.4f}, Train Loss: {loss.item():.4f}, \"\n",
    "          f\"Train Accuracy: {train_accuracy:.2f}%, Temperatures:({model_RRAM.temperature_1:.2f}, {model_RRAM.temperature_2:.2f})\")\n",
    "\n",
    "    if epoch % 50 == 0 and epoch != 0:\n",
    "        lr /= 2\n",
    "\n",
    "model_RRAM = SoftBinaryRecurrentForwardNetwork(**model_params).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282eeb54-9acf-4ce6-b5f0-13e5c60f8614",
   "metadata": {
    "id": "282eeb54-9acf-4ce6-b5f0-13e5c60f8614"
   },
   "source": [
    "### Loading Past Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0de620a-e0e4-49cf-863d-721e2680dd58",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0de620a-e0e4-49cf-863d-721e2680dd58",
    "outputId": "3fc15a9d-cc8e-4351-ff9c-d40906fc055f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best Validation Loss: 1.528846\n",
      "\n",
      " Parameters for Best Loss Model: {'scaling': 2, 'G_ON': 6e-05, 'G_OFF': 2.88e-06, 'V_INV': 1.65, 'R_INV': 3000.0, 'V_1': 0.1, 'V_0': -0.1, 'zeta': 10.0, 'initial_factor': 0.01, 'crossbar': (64, 64), 'input_size': 676, 'encoding_size': 4, 'output_size': 10, 'data_in': 52, 'bin_active': True, 'monitor_volts': False, 'monitor_grads': False, 'monitor_latents': False, 'dropout': 0.1, 'int_lr': 0.01, 'int_norm': True, 'ext_lr': 1000, 'epochs': 40}\n",
      "\n",
      " Best Validation Accuracy: 70.53\n",
      "\n",
      " Parameters for Best Accuracy Model: {'scaling': 5, 'G_ON': 6e-05, 'G_OFF': 2.88e-06, 'V_INV': 0.6, 'R_INV': 1000.0, 'V_1': 0.1, 'V_0': -0.1, 'zeta': 10.0, 'initial_factor': 0.01, 'crossbar': (64, 64), 'input_size': 676, 'encoding_size': 4, 'output_size': 10, 'data_in': 52, 'bin_active': True, 'monitor_volts': False, 'monitor_grads': False, 'monitor_latents': False, 'dropout': 0.1, 'int_lr': 0.01, 'int_norm': True, 'ext_lr': 500, 'epochs': 1000, 'temperature_1': 1, 'temperature_2': 5000, 'monitor_annealing': True}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load best validation loss model\n",
    "    with open(\"Best_Val_Loss.txt\", 'r') as f:\n",
    "        global_best_val_loss = float(f.read())\n",
    "    with open(\"Best_Params_Loss.txt\", 'r') as f:\n",
    "        params_best_loss = ast.literal_eval(f.read())\n",
    "\n",
    "    model_best_loss = SoftBinaryRecurrentForwardNetwork(**model_params).to(device)\n",
    "\n",
    "    print(\"\\n Best Validation Loss:\", global_best_val_loss)\n",
    "    print(\"\\n Parameters for Best Loss Model:\", params_best_loss)\n",
    "\n",
    "    checkpoint_loss = torch.load(\"Best_model_loss.pth\")\n",
    "    model_best_loss.load_state_dict(checkpoint_loss)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error loading best loss model:\", e)\n",
    "    global_best_val_loss = float('inf')\n",
    "    print(\"No Saved Model for Best Loss\")\n",
    "\n",
    "try:\n",
    "    # Load best validation accuracy model\n",
    "    with open(\"Best_Val_Acc.txt\", 'r') as f:\n",
    "        global_best_val_acc = float(f.read())\n",
    "    with open(\"Best_Params_Acc.txt\", 'r') as f:\n",
    "        params_best_acc = ast.literal_eval(f.read())\n",
    "\n",
    "    model_best_acc = SoftBinaryRecurrentForwardNetwork(**model_params).to(device)\n",
    "\n",
    "    print(\"\\n Best Validation Accuracy:\", global_best_val_acc)\n",
    "    print(\"\\n Parameters for Best Accuracy Model:\", params_best_acc)\n",
    "\n",
    "    checkpoint_acc = torch.load(\"Best_model_acc.pth\")\n",
    "    model_best_acc.load_state_dict(checkpoint_acc)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error loading best accuracy model:\", e)\n",
    "    global_best_val_acc = 0.0\n",
    "    print(\"No Saved Model for Best Accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "786757d6-3ef8-4327-900a-94924d5a1b57",
   "metadata": {
    "id": "786757d6-3ef8-4327-900a-94924d5a1b57"
   },
   "outputs": [],
   "source": [
    "history_RRAM = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_accuracy\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_accuracy\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc53685-ceb0-4b1c-a03f-04ca24416013",
   "metadata": {
    "id": "8dc53685-ceb0-4b1c-a03f-04ca24416013"
   },
   "source": [
    "### Complete Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ed1842-f618-4562-8e82-c5355ca39f90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9ed1842-f618-4562-8e82-c5355ca39f90",
    "outputId": "aba2264c-4110-4c7e-b283-98b16068904d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, LR: 20.0000, Train Loss: 2.0465, Train Acc: 39.14%, Val Loss: 1.8824, Val Acc: 56.19%, Temperatures: (0.20, 1000.00)\n",
      "Old & New Losses 18824.43904876709 18881.1993598938 Probab: tensor(0.9448, device='cuda:0')\n",
      "Annealed weights accepted\n",
      "Epoch 2, LR: 100.0000, Train Loss: 1.8586, Train Acc: 56.76%, Val Loss: 1.8335, Val Acc: 60.94%, Temperatures: (0.18, 900.00)\n",
      "Old & New Losses 18335.42823791504 18345.96276283264 Probab: tensor(0.9884, device='cuda:0')\n",
      "Annealed weights accepted\n"
     ]
    }
   ],
   "source": [
    "lr = params_RRAM[\"ext_lr\"] / 25\n",
    "num_epochs = params_RRAM[\"epochs\"]\n",
    "patience, wait, wait_2, wait_3 = 500, 0, 75, 15\n",
    "wait_lr = 0\n",
    "cur_best_val_loss, cur_best_val_acc = float('inf'), 0\n",
    "temp_boosted = False\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch == 0:\n",
    "        lr = lr\n",
    "    elif epoch <= 2:\n",
    "        lr *= 5\n",
    "\n",
    "    model_RRAM.train().to(device)\n",
    "    train_loss, train_correct, total_samples = 0, 0, 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model_RRAM(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        model_RRAM.backprop(lr)\n",
    "\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        train_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "        total_samples += inputs.size(0)\n",
    "\n",
    "        # model_RRAM.anneal(inputs, labels, 0.99, 0.999)\n",
    "\n",
    "    train_loss /= total_samples\n",
    "    train_accuracy = 100 * train_correct / total_samples\n",
    "\n",
    "    model_RRAM.eval()\n",
    "    val_loss, val_correct, total_test_samples = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model_RRAM(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            total_test_samples += inputs.size(0)\n",
    "\n",
    "    val_loss /= total_test_samples\n",
    "    val_accuracy = 100 * val_correct / total_test_samples\n",
    "\n",
    "    history_RRAM[\"train_loss\"].append(train_loss)\n",
    "    history_RRAM[\"train_accuracy\"].append(train_accuracy)\n",
    "    history_RRAM[\"val_loss\"].append(val_loss)\n",
    "    history_RRAM[\"val_accuracy\"].append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, LR: {lr:.4f}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%, Temperatures: ({model_RRAM.temperature_1:.2f}, {model_RRAM.temperature_2:.2f})\")\n",
    "\n",
    "    if val_loss < global_best_val_loss:\n",
    "        global_best_val_loss = val_loss\n",
    "        torch.save(model_RRAM.state_dict(), \"Best_model_loss.pth\")\n",
    "        with open(\"Best_Val_Loss.txt\", \"w\") as f:\n",
    "            f.write(f\"{val_loss:.6f}\")\n",
    "        with open(\"Best_Params_Loss.txt\", \"w\") as f:\n",
    "            f.write(f\"{params_RRAM}\")\n",
    "        print(f\"Model saved with best validation loss: {val_loss:.6f}\")\n",
    "\n",
    "    if val_accuracy > global_best_val_acc:\n",
    "        global_best_val_acc = val_accuracy\n",
    "        torch.save(model_RRAM.state_dict(), \"Best_model_acc.pth\")\n",
    "        with open(\"Best_Val_Acc.txt\", \"w\") as f:\n",
    "            f.write(f\"{val_accuracy:.6f}\")\n",
    "        with open(\"Best_Params_Acc.txt\", \"w\") as f:\n",
    "            f.write(f\"{params_RRAM}\")\n",
    "        print(f\"Model saved with best validation accuracy: {val_accuracy:.6f}\")\n",
    "\n",
    "    # Anneal Using Validation Set, Once Per Epoch\n",
    "    model_RRAM.anneal(inputs, labels, 0.9, 0.9)\n",
    "\n",
    "    if wait_lr >= wait_2 and epoch > 3:\n",
    "        lr /= 5\n",
    "        wait_lr = 0\n",
    "        print(f\"No improvement for {wait_2} epochs. Reducing LR to {lr:.4f}\")\n",
    "\n",
    "    if wait_lr >= wait_3 and not temp_boosted:\n",
    "        model_RRAM.temperature_2 *= 10\n",
    "        temp_boosted = True\n",
    "        print(f\"Training plateau detected. Temporarily increasing temperature_2 to {model_RRAM.temperature_2:.2f}\")\n",
    "\n",
    "    if val_loss < cur_best_val_loss - 0.001 or val_accuracy > cur_best_val_acc + 0.1:\n",
    "        if temp_boosted:\n",
    "            model_RRAM.temperature_2 /= 10\n",
    "            temp_boosted = False\n",
    "            print(f\"Training improved. Restoring temperature_2 to {model_RRAM.temperature_2:.2f}\")\n",
    "\n",
    "        cur_best_val_loss = min(cur_best_val_loss, val_loss)\n",
    "        cur_best_val_acc = max(cur_best_val_acc, val_accuracy)\n",
    "        wait = 0\n",
    "        wait_lr = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "        wait_lr += 1\n",
    "\n",
    "    if wait >= patience and epoch > 6:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070a2ef8-94c5-49ca-bd35-3ed75ae2327b",
   "metadata": {
    "id": "070a2ef8-94c5-49ca-bd35-3ed75ae2327b"
   },
   "outputs": [],
   "source": [
    "plot_history(history_RRAM, num_epochs, \"RRAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93e55bc-1bae-4483-a3cf-01cde188dc69",
   "metadata": {
    "id": "c93e55bc-1bae-4483-a3cf-01cde188dc69"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af52e53e-15ed-441f-8412-d94c5d6a0bdb",
   "metadata": {
    "id": "af52e53e-15ed-441f-8412-d94c5d6a0bdb"
   },
   "source": [
    "### Current Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51271c7c-25b1-4702-9db1-9ae4b805843c",
   "metadata": {
    "id": "51271c7c-25b1-4702-9db1-9ae4b805843c"
   },
   "outputs": [],
   "source": [
    "print(0 + (model_RRAM.w > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d477c9e-5678-4d07-a5ac-48f01480c916",
   "metadata": {
    "id": "9d477c9e-5678-4d07-a5ac-48f01480c916"
   },
   "outputs": [],
   "source": [
    "cm = test(model_RRAM, val_inputs, val_labels, class_names = [\"A\", \"T\", \"V\", \"X\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7941df-4678-4955-8a0f-f38b43f6b197",
   "metadata": {
    "id": "ed7941df-4678-4955-8a0f-f38b43f6b197"
   },
   "source": [
    "## Best Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867574d7-3627-49ff-a430-1d658fa4d655",
   "metadata": {
    "id": "867574d7-3627-49ff-a430-1d658fa4d655"
   },
   "outputs": [],
   "source": [
    "0+1*(model_best.w>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ca56c6-7023-416e-9675-93f1a3c8cc78",
   "metadata": {
    "id": "32ca56c6-7023-416e-9675-93f1a3c8cc78"
   },
   "outputs": [],
   "source": [
    "cm = test(model_best, val_inputs, val_labels, class_names = [\"A\", \"T\", \"V\", \"X\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad288f0b-0ec9-415d-96cc-a4ecec7aa5ce",
   "metadata": {
    "id": "ad288f0b-0ec9-415d-96cc-a4ecec7aa5ce"
   },
   "source": [
    "## PWL Generation\n",
    "\n",
    "Let's assume that we will program the two crossbars with seperate PWLs. That is, during programming, we will cut the Inverting Amplifier stages with a pass transistor and connect the programming lines with a pass transistor. First array has 16 Top PWLs and 8 Bottom PWLs. Second array has 8 Top PWLs and 4 Bottom PWLs. And then once the programming switch is toggled to inference mode, only the 16 Top PWLs are to be changed. Let's also generate a PWL for that too.\n",
    "\n",
    "In the code below, we will first maintain tuples for each PWL that holds what the voltage should be. And then we will write a function that will take there and space pulses of the given voltage that are 100us apart from other and have an ON duration of 100us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d5cebb-fc99-4a00-87a1-e183838c9a64",
   "metadata": {
    "id": "58d5cebb-fc99-4a00-87a1-e183838c9a64"
   },
   "outputs": [],
   "source": [
    "WL_FC1 = [list() for i in range(16)]\n",
    "BL_FC1 = [list() for i in range(8)]\n",
    "WL_FC2 = [list() for i in range(8)]\n",
    "BL_FC2 = [list() for i in range(4)]\n",
    "Mode = []\n",
    "Mode_B = []\n",
    "\n",
    "V_WRITE = 1.5\n",
    "V_READ = 0.1\n",
    "V_mode = 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c647afda-e1a7-47a7-b634-212ec5527be2",
   "metadata": {
    "id": "c647afda-e1a7-47a7-b634-212ec5527be2"
   },
   "source": [
    "#### Fully Connected Weights 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03334172-89f6-4d73-9461-6ac922bdb6ea",
   "metadata": {
    "id": "03334172-89f6-4d73-9461-6ac922bdb6ea"
   },
   "outputs": [],
   "source": [
    "target = (model_RRAM_best.w1>0).int()\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49287afe-8437-40aa-8825-93f2cffe3a66",
   "metadata": {
    "id": "49287afe-8437-40aa-8825-93f2cffe3a66"
   },
   "outputs": [],
   "source": [
    "for ind_i, i in enumerate(target):\n",
    "    for ind_j, j in enumerate(i):\n",
    "        if j==1: WL_FC1[ind_j].append(V_WRITE)\n",
    "        else: WL_FC1[ind_j].append(V_WRITE/3)\n",
    "    for ind_k in range(len(target)):\n",
    "        if ind_k==ind_i: BL_FC1[ind_i].append(0)\n",
    "        else: BL_FC1[ind_k].append(2*V_WRITE/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be77131c-005d-4f06-8a70-f40d267eb67e",
   "metadata": {
    "id": "be77131c-005d-4f06-8a70-f40d267eb67e"
   },
   "source": [
    "#### Fully Connected Weights 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adc2b24-ffb0-4470-8eae-3830221cb33d",
   "metadata": {
    "id": "9adc2b24-ffb0-4470-8eae-3830221cb33d"
   },
   "outputs": [],
   "source": [
    "target = (model_RRAM_best.w2>0).int()\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7ca037-0e2f-49a3-98b3-b2e0b93f8b79",
   "metadata": {
    "id": "3f7ca037-0e2f-49a3-98b3-b2e0b93f8b79"
   },
   "outputs": [],
   "source": [
    "for ind_i, i in enumerate(target):\n",
    "    for ind_j, j in enumerate(i):\n",
    "        if j==1: WL_FC2[ind_j].append(V_WRITE)\n",
    "        else: WL_FC2[ind_j].append(V_WRITE/3)\n",
    "    for ind_k in range(len(target)):\n",
    "        if ind_k==ind_i: BL_FC2[ind_i].append(0)\n",
    "        else: BL_FC2[ind_k].append(2*V_WRITE/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a602368f-07a2-4beb-90e3-d782ca717fea",
   "metadata": {
    "id": "a602368f-07a2-4beb-90e3-d782ca717fea"
   },
   "source": [
    "#### Filling Out Programming Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aa24d2-0d95-4ff4-8dfd-5c0800d9de11",
   "metadata": {
    "id": "c4aa24d2-0d95-4ff4-8dfd-5c0800d9de11"
   },
   "outputs": [],
   "source": [
    "WL_FC1 = [i + [0,0] for i in WL_FC1]\n",
    "BL_FC1 = [i + [0,0] for i in BL_FC1]\n",
    "while(len(WL_FC2[0]) < len(WL_FC1[0])):\n",
    "    WL_FC2 = [i + [0,] for i in WL_FC2]\n",
    "    BL_FC2 = [i + [0,] for i in BL_FC2]\n",
    "Mode.extend([V_mode]*(len(WL_FC1[0])-1) + [-V_mode])\n",
    "Mode_B.extend([-V_mode]*(len(WL_FC1[0])-1) + [V_mode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f953198a-a107-428c-9624-3c712086e922",
   "metadata": {
    "id": "f953198a-a107-428c-9624-3c712086e922"
   },
   "outputs": [],
   "source": [
    "print(WL_FC1[0])\n",
    "print(BL_FC1[0])\n",
    "print(WL_FC2[0])\n",
    "print(BL_FC2[0])\n",
    "print(Mode)\n",
    "print(Mode_B)\n",
    "print(len(WL_FC1[0]), len(BL_FC1[0]), len(WL_FC2[0]), len(BL_FC2[0]), len(Mode), len(Mode_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92339f84-e8d6-4280-92af-e5215aa129a0",
   "metadata": {
    "id": "92339f84-e8d6-4280-92af-e5215aa129a0"
   },
   "source": [
    "### Inference: Loading the Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97dacd2-42bb-49f0-8ca9-e0f02ca4cc58",
   "metadata": {
    "id": "b97dacd2-42bb-49f0-8ca9-e0f02ca4cc58"
   },
   "outputs": [],
   "source": [
    "val_inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2336c3ab-2181-4ee7-b1c9-0a97a3700248",
   "metadata": {
    "id": "2336c3ab-2181-4ee7-b1c9-0a97a3700248"
   },
   "outputs": [],
   "source": [
    "V_1 = 0.1\n",
    "V_0 = -0.1\n",
    "include_testing = True\n",
    "include_every = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1cfba2-3243-4062-9ef6-4cd309622c73",
   "metadata": {
    "id": "6a1cfba2-3243-4062-9ef6-4cd309622c73"
   },
   "outputs": [],
   "source": [
    "if include_testing:\n",
    "    for i in val_inputs[::include_every]:\n",
    "        i = i.flatten()\n",
    "        for ind, j in enumerate(i):\n",
    "            WL_FC1[ind].append(V_1 if j==1 else V_0)\n",
    "        BL_FC1 = [i + [0,] for i in BL_FC1]\n",
    "        WL_FC2 = [i + [0,] for i in WL_FC2]\n",
    "        BL_FC2 = [i + [0,] for i in BL_FC2]\n",
    "        Mode = Mode + [-V_mode,]\n",
    "        Mode_B = Mode_B + [V_mode,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dd97b8-fb1b-4634-9ae4-3a6fe05d0a2c",
   "metadata": {
    "id": "b9dd97b8-fb1b-4634-9ae4-3a6fe05d0a2c"
   },
   "source": [
    "### PWL Convertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98af5ff-8f34-4472-89fc-e308d98d4072",
   "metadata": {
    "id": "c98af5ff-8f34-4472-89fc-e308d98d4072"
   },
   "outputs": [],
   "source": [
    "def pwl(l):\n",
    "    t = 0\n",
    "    res = \"pwl(time, 0us, 0V\"\n",
    "    for i in l:\n",
    "        res += f\", {t+5}us, {i:.2f}V, {t+100}us, {i:.2f}V, {t+105}us, 0V, {t+200}us, 0V\"\n",
    "        t+=200\n",
    "    res += \")\"\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54850dbc-1317-44a2-9093-c5a4e048094c",
   "metadata": {
    "id": "54850dbc-1317-44a2-9093-c5a4e048094c"
   },
   "outputs": [],
   "source": [
    "pwl_data = []\n",
    "\n",
    "for ind, i in enumerate(WL_FC1):\n",
    "    pwl_data.append({\"Signal\": f\"WL_FC1_{ind}\", \"Index\": ind, \"PWL\": pwl(i)})\n",
    "for ind, i in enumerate(BL_FC1):\n",
    "    pwl_data.append({\"Signal\": f\"BL_FC1_{ind}\", \"Index\": ind, \"PWL\": pwl(i)})\n",
    "for ind, i in enumerate(WL_FC2):\n",
    "    pwl_data.append({\"Signal\": f\"WL_FC2_{ind}\", \"Index\": ind, \"PWL\": pwl(i)})\n",
    "for ind, i in enumerate(BL_FC2):\n",
    "    pwl_data.append({\"Signal\": f\"BL_FC2_{ind}\", \"Index\": ind, \"PWL\": pwl(i)})\n",
    "pwl_data.append({\"Signal\": \"Mode\", \"Index\": \"\", \"PWL\": pwl(Mode)})\n",
    "pwl_data.append({\"Signal\": \"Mode_b\", \"Index\": \"\", \"PWL\": pwl(Mode_B)})\n",
    "\n",
    "pwl_data = pd.DataFrame(pwl_data)\n",
    "pwl_data.to_csv(\"pwl_data.csv\", index=False)\n",
    "pwl_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e13866-8cd8-4b8e-a8d0-a5c69e951174",
   "metadata": {
    "id": "03e13866-8cd8-4b8e-a8d0-a5c69e951174"
   },
   "source": [
    "#### Testing Accuracy on 160 Images\n",
    "ADS isn't allowing PWLs longer than 160 Images, so let's check software accuracy for the same too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4e4e97-5ecd-4c41-9035-4a51015d684e",
   "metadata": {
    "id": "fd4e4e97-5ecd-4c41-9035-4a51015d684e"
   },
   "outputs": [],
   "source": [
    "test(model_RRAM_best, val_inputs[::4], val_labels[::4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf44b69-14ac-4fdd-a2b5-b9c2cc361e7f",
   "metadata": {
    "id": "7cf44b69-14ac-4fdd-a2b5-b9c2cc361e7f"
   },
   "outputs": [],
   "source": [
    "test(model_RRAM_best, train_inputs, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1079074f-8781-43af-b52e-fb3581ef8931",
   "metadata": {
    "id": "1079074f-8781-43af-b52e-fb3581ef8931"
   },
   "source": [
    "## Simulation Data from ADS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d523a2-f99f-46bb-93e0-fead2768da75",
   "metadata": {
    "id": "e0d523a2-f99f-46bb-93e0-fead2768da75"
   },
   "outputs": [],
   "source": [
    "simu = pd.read_csv(\"Testing_160_Images.csv\")\n",
    "simu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd96e845-0c19-470b-8910-6b45df78022c",
   "metadata": {
    "id": "fd96e845-0c19-470b-8910-6b45df78022c"
   },
   "outputs": [],
   "source": [
    "def remove_units(value):\n",
    "    return float(value.replace('E', 'e').split('V')[0].replace('sec', ''))\n",
    "\n",
    "simu['time'] = simu['time'].apply(remove_units)\n",
    "for col in ['A', 'X', 'V', 'T']:\n",
    "    simu[col] = simu[col].apply(remove_units)\n",
    "simu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223576d6-266c-49bd-aaca-ff3444617f09",
   "metadata": {
    "id": "223576d6-266c-49bd-aaca-ff3444617f09"
   },
   "source": [
    "We just need one sample every 0.1ms samples of these starting from 2.050ms to 33.850ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13d6e19-daa6-481f-8ad2-11a357dc20d3",
   "metadata": {
    "id": "a13d6e19-daa6-481f-8ad2-11a357dc20d3"
   },
   "outputs": [],
   "source": [
    "t_stamps = np.arange(2.05e-3, 33.9e-3, 0.2e-3)\n",
    "t_stamps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5548dd9-e376-4bd1-b9d2-484f4358f700",
   "metadata": {
    "id": "f5548dd9-e376-4bd1-b9d2-484f4358f700"
   },
   "outputs": [],
   "source": [
    "sampled = []\n",
    "window = 0.02e-3\n",
    "\n",
    "for t in t_stamps:\n",
    "    filtered = simu[(simu['time'] >= t - window) & (simu['time'] <= t + window)]\n",
    "\n",
    "    avg_A = filtered['A'].mean()\n",
    "    avg_X = filtered['X'].mean()\n",
    "    avg_V = filtered['V'].mean()\n",
    "    avg_T = filtered['T'].mean()\n",
    "\n",
    "    sampled.append({\n",
    "        'Image Index': t,\n",
    "        'A': avg_A,\n",
    "        'X': avg_X,\n",
    "        'V': avg_V,\n",
    "        'T': avg_T\n",
    "    })\n",
    "\n",
    "sampled = pd.DataFrame(sampled)\n",
    "sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096828ea-b6be-4839-8b7d-4e0f5a515a7e",
   "metadata": {
    "id": "096828ea-b6be-4839-8b7d-4e0f5a515a7e"
   },
   "outputs": [],
   "source": [
    "def get_max_column(row):\n",
    "    return row[['A', 'X', 'V', 'T']].idxmax()\n",
    "sampled['Predicted Class'] = sampled.apply(get_max_column, axis=1)\n",
    "sampled.to_csv(\"Sampled_Results.csv\", index=False)\n",
    "sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1143c06-5631-4c79-9965-0cb937929706",
   "metadata": {
    "id": "d1143c06-5631-4c79-9965-0cb937929706"
   },
   "outputs": [],
   "source": [
    "ground_truth = ['A']*40 + ['X']*40 + ['V']*40 + ['T']*40\n",
    "correct_predictions = sampled['Predicted Class'] == ground_truth\n",
    "accuracy = correct_predictions.sum() / len(ground_truth)\n",
    "print(accuracy*100,end=\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918b4552-b0d3-4768-bfe7-9b874959d42c",
   "metadata": {
    "id": "918b4552-b0d3-4768-bfe7-9b874959d42c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 3.5))\n",
    "\n",
    "plt.scatter(sampled.index, sampled['A'], color='red', label='A_pred', s=30, marker='o')  # Red dots for A\n",
    "plt.scatter(sampled.index, sampled['X'], color='blue', label='X_pred', s=30, marker='o')  # Blue dots for X\n",
    "plt.scatter(sampled.index, sampled['T'], color='green', label='T_pred', s=30, marker='o')  # Green dots for T\n",
    "plt.scatter(sampled.index, sampled['V'], color='orange', label='V_pred', s=30, marker='o')  # Orange dots for V\n",
    "\n",
    "plt.xlabel('Image Index')\n",
    "plt.ylabel('Predicted Voltages (V)')\n",
    "plt.legend()\n",
    "\n",
    "plt.axvline(x=40, color='gray', linestyle='--', linewidth=2)\n",
    "plt.axvline(x=80, color='gray', linestyle='--', linewidth=2)\n",
    "plt.axvline(x=120, color='gray', linestyle='--', linewidth=2)\n",
    "\n",
    "plt.text(20, plt.ylim()[1]*(-0.8), 'A', fontsize=15, color='black', ha='center')\n",
    "plt.text(60, plt.ylim()[1]*0.8, 'X', fontsize=15, color='black', ha='center')\n",
    "plt.text(100, plt.ylim()[1]*0.8, 'V', fontsize=15, color='black', ha='center')\n",
    "plt.text(140, plt.ylim()[1]*(-0.8), 'T', fontsize=15, color='black', ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f31ef-ad69-472f-9217-bdde6f8ab1e8",
   "metadata": {
    "id": "e86f31ef-ad69-472f-9217-bdde6f8ab1e8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
