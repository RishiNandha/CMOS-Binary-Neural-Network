{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "141370fc-54a4-4a6b-afcf-a7677dc6dc87",
      "metadata": {
        "id": "141370fc-54a4-4a6b-afcf-a7677dc6dc87"
      },
      "source": [
        "# Soft Binary Neural Network with Recurrent Crossbar Recycling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "508058d8-e23a-4c29-aad7-c2b233d621c9",
      "metadata": {
        "id": "508058d8-e23a-4c29-aad7-c2b233d621c9"
      },
      "source": [
        "## Imports and Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9a70e539-1dc9-4e36-9c9f-18fbdaeede1f",
      "metadata": {
        "id": "9a70e539-1dc9-4e36-9c9f-18fbdaeede1f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import ast\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d406d54c-db27-4536-a8c1-f46437f6fb71",
      "metadata": {
        "id": "d406d54c-db27-4536-a8c1-f46437f6fb71"
      },
      "outputs": [],
      "source": [
        "def plot_history(history, num_epochs, element):\n",
        "    epochs = range(len(history[list(history.keys())[0]]))\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    ax1.plot(epochs, history[\"train_loss\"], label=\"Train Loss\", color=\"blue\")\n",
        "    ax1.plot(epochs, history[\"val_loss\"], label=\"Validation Loss\", color=\"red\")\n",
        "    ax1.set_xlabel(\"Epochs\", fontsize=14)\n",
        "    ax1.set_ylabel(\"Loss\", fontsize=14, color=\"blue\")\n",
        "    ax1.tick_params(axis=\"y\", labelcolor=\"blue\")\n",
        "    ax1.legend(loc=\"upper left\")\n",
        "    ax1.grid(True)\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(epochs, history[\"train_accuracy\"], label=\"Train Accuracy\", color=\"green\")\n",
        "    ax2.plot(epochs, history[\"val_accuracy\"], label=\"Validation Accuracy\", color=\"orange\")\n",
        "    ax2.set_ylabel(\"Accuracy (%)\", fontsize=14, color=\"green\")\n",
        "    ax2.tick_params(axis=\"y\", labelcolor=\"green\")\n",
        "    ax2.legend(loc=\"upper right\")\n",
        "\n",
        "    plt.title(f\"Training and Validation Metrics for {element}\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7c658023-75df-4754-a617-a8ba6d08d068",
      "metadata": {
        "id": "7c658023-75df-4754-a617-a8ba6d08d068"
      },
      "outputs": [],
      "source": [
        "def test(model, test_loader, class_names=None):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    accuracy = (total_correct / total_samples) * 100\n",
        "\n",
        "    print(f\"Validation Loss: {avg_loss:.4f}\")\n",
        "    print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    return cm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be5e8ced-6bb5-445a-9b04-bc268eec917e",
      "metadata": {
        "id": "be5e8ced-6bb5-445a-9b04-bc268eec917e"
      },
      "source": [
        "### MNIST Handwritten Digits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d3bbad17-7067-4f06-8755-012646ca9567",
      "metadata": {
        "id": "d3bbad17-7067-4f06-8755-012646ca9567",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1798d9d-41cf-4c7c-810b-01a97006c6d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 111] Connection refused>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.48MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 111] Connection refused>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 57.8kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 111] Connection refused>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:06<00:00, 245kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 111] Connection refused>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 10.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "class BinarizeAndAddNoiseTransform:\n",
        "    def __init__(self, threshold, noise_std):\n",
        "        self.threshold = threshold\n",
        "        self.noise_std = noise_std\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img = transforms.ToTensor()(img).to(device)\n",
        "        img = (img > self.threshold).float()\n",
        "        img = img[:,1:-1, 1:-1]\n",
        "        noise = torch.randn(img.size(), device=device) * self.noise_std\n",
        "        noisy_img = img + noise\n",
        "        return noisy_img\n",
        "\n",
        "binary_noise_transform = transforms.Compose([\n",
        "    BinarizeAndAddNoiseTransform(threshold=0.48, noise_std=0.05)\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=binary_noise_transform)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=1000, shuffle=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=binary_noise_transform)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=10000, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "bc40653e-ebac-4013-9ecb-27dfe1370edd",
      "metadata": {
        "id": "bc40653e-ebac-4013-9ecb-27dfe1370edd"
      },
      "outputs": [],
      "source": [
        "# Get a subset of the dataset\n",
        "train_in, train_lab = next(iter(train_loader))\n",
        "val_in, val_lab = next(iter(test_loader))\n",
        "\n",
        "# Move data to the appropriate device\n",
        "train_in, train_lab = train_in.to(device), train_lab.to(device)\n",
        "val_in, val_lab = val_in.to(device), val_lab.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d76599e3-3d87-4d1d-bd5f-41ca0adab18f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "d76599e3-3d87-4d1d-bd5f-41ca0adab18f",
        "outputId": "1b72a9c5-d410-4fd7-d135-0495307455db"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x400 with 20 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1000, 1, 26, 26])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "fig, axes = plt.subplots(nrows=2, ncols=10, figsize=(15, 4))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(train_in[i].cpu().squeeze(), cmap='gray')\n",
        "    ax.set_title(f\"Label: {train_lab[i].item()}\", fontsize=10)\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "train_in.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1b44def-df85-406b-87e8-fbc5b4f7fe7a",
      "metadata": {
        "id": "d1b44def-df85-406b-87e8-fbc5b4f7fe7a"
      },
      "source": [
        "## Custom Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "391dfb3d-1a2b-42d7-9ff7-e3f0e831d50a",
      "metadata": {
        "id": "391dfb3d-1a2b-42d7-9ff7-e3f0e831d50a"
      },
      "outputs": [],
      "source": [
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def tensor_stats(tensor, name=\"Tensor\"):\n",
        "    tensor = tensor.to(device)\n",
        "    mean_magnitude = tensor.abs().mean().item()\n",
        "    print(f\"{name} - Mean Magnitude: {mean_magnitude:.2e}, Max: {tensor.max().item():.2e}, Min: {tensor.min().item():.2e}\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SoftBinaryRecurrentForwardNetwork(nn.Module):\n",
        "    def __init__(self, scaling, G_ON, G_OFF, V_INV, R_INV, V_1, V_0, zeta, initial_factor, crossbar=(64,64),\n",
        "                 input_size=676, encoding_size=4, output_size=10, data_in=52, bin_active=True,\n",
        "                 monitor_volts=False, monitor_grads=True, monitor_latents=False, dropout=0.01,\n",
        "                 int_lr=0.01, int_norm=True, temperature_1 = 500, temperature_2 = 10000,monitor_annealing=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.w = nn.Parameter(initial_factor * torch.empty(crossbar, device=device))\n",
        "        nn.init.xavier_uniform_(self.w)\n",
        "        self.w.data = (initial_factor*(self.w.data))\n",
        "\n",
        "        self.G_ON, self.G_OFF = torch.tensor(G_ON, device=device)*scaling, torch.tensor(G_OFF, device=device)*scaling\n",
        "        self.V_INV, self.R_INV = torch.tensor(V_INV, device=device), torch.tensor(R_INV, device=device)\n",
        "        self.V_1, self.V_0 = torch.tensor(V_1, device=device), torch.tensor(V_0, device=device)\n",
        "\n",
        "        self.crossbar_in, self.crossbar_out = crossbar\n",
        "        self.encoding, self.data_in, self.output_size = encoding_size, data_in, output_size\n",
        "        self.r_passes = input_size // data_in\n",
        "\n",
        "        self.first_bias = (crossbar[0] - data_in) % encoding_size\n",
        "        self.extra_final = crossbar[1] - self.encoding * self.r_passes - output_size\n",
        "        self.final_bias = (crossbar[0] - self.encoding * self.r_passes - self.extra_final) % (self.extra_final + self.encoding)\n",
        "\n",
        "        self.feed_repeats = (crossbar[0] - data_in)//encoding_size\n",
        "        self.final_repeats = (crossbar[0] - self.encoding * self.r_passes - self.extra_final)//(self.extra_final + self.encoding)\n",
        "\n",
        "        self.zeta, self.int_lr = torch.tensor(zeta, device=device), torch.tensor(int_lr, device=device)\n",
        "        self.bin_active, self.int_norm = bin_active, int_norm\n",
        "        self.monitor_volts, self.monitor_grads, self.monitor_latents = monitor_volts, monitor_grads, monitor_latents\n",
        "        self.monitor_annealing = monitor_annealing\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        self.temperature_1 = temperature_1\n",
        "        self.temperature_2 = temperature_2\n",
        "        self.device = device\n",
        "\n",
        "    def INV_AMP(self, x, R_INV):\n",
        "        return -self.V_INV * torch.tanh(R_INV * x / self.V_INV)\n",
        "\n",
        "    def SOFT_BIN(self, x):\n",
        "        if self.bin_active: return ((self.G_ON - self.G_OFF) * torch.sigmoid(x * self.zeta) + self.G_OFF)\n",
        "        else: return self.G_ON * x * self.zeta * 0.4\n",
        "\n",
        "    def PREPROCESS(self, img):\n",
        "        return (self.V_1 - self.V_0) * img.to(device) + self.V_0\n",
        "\n",
        "    def ANNEALER(self):\n",
        "        prob = torch.exp(torch.tensor(-1.0, device=self.device) / self.temperature_1)\n",
        "        prob = torch.clamp(prob, min=1e-3, max=1)\n",
        "\n",
        "        rand_vals = torch.rand((64, 64), device=self.device)\n",
        "        annealed_mask = torch.where(rand_vals < prob, -1, torch.where(rand_vals < 2 * prob, 0, 1))\n",
        "\n",
        "        return annealed_mask\n",
        "\n",
        "    def forward(self, img):\n",
        "        # Preprocessing: Two States of input (V_ON and V_OFF)\n",
        "        img = self.PREPROCESS(img.view(img.size(0), -1))\n",
        "        bias = self.PREPROCESS(((-1) ** torch.arange(self.first_bias, device=device)).repeat(img.shape[0], 1))\n",
        "        bias2 = self.PREPROCESS(((-1) ** torch.arange(self.final_bias, device=device)).repeat(img.shape[0], 1))\n",
        "\n",
        "        # RRAM Soft Binarization\n",
        "        g = self.SOFT_BIN(self.w)\n",
        "        if self.monitor_latents: tensor_stats(self.w, \"Latent Weights:\")\n",
        "\n",
        "        # Recurrent Encoding Layer\n",
        "        out1size = self.crossbar_out - self.output_size\n",
        "        feedback = torch.zeros((img.shape[0], self.encoding*self.feed_repeats), device=device)\n",
        "        out1 = torch.zeros((img.shape[0], out1size), device = device)\n",
        "\n",
        "        for r_pass in range(self.r_passes - 1):\n",
        "            ind_s, ind_f = self.crossbar_out - (r_pass+1)*self.encoding, self.crossbar_out - (r_pass)*self.encoding\n",
        "            ind_a, ind_b = out1size - (r_pass+1)*self.encoding, out1size - (r_pass)*self.encoding\n",
        "\n",
        "            x = torch.cat((feedback, bias, img[:, r_pass * self.data_in:(r_pass + 1) * self.data_in]), dim=1)\n",
        "            x = F.linear(x, g[ind_s:ind_f, : ], bias=None)\n",
        "\n",
        "            out1[:, ind_a:ind_b] = self.INV_AMP(x, self.R_INV)\n",
        "            if self.monitor_volts: tensor_stats(feedback, f\"Voltages in Recurrent Stage after pass {r_pass}\")\n",
        "\n",
        "            feedback = out1[:, ind_a:ind_b].repeat(1,self.feed_repeats)\n",
        "\n",
        "        else:\n",
        "            r_pass += 1\n",
        "            ind_s, ind_f = self.crossbar_out - (r_pass+1)*self.encoding - self.extra_final, self.crossbar_out - (r_pass)*self.encoding\n",
        "            ind_a, ind_b = out1size - (r_pass+1)*self.encoding - self.extra_final, out1size - (r_pass)*self.encoding\n",
        "\n",
        "            x = torch.cat((feedback, bias, img[:, r_pass * self.data_in:(r_pass + 1) * self.data_in]), dim=1)\n",
        "            x = F.linear(x, g[-(r_pass+1)*self.encoding - self.extra_final:-r_pass*self.encoding, : ], bias=None)\n",
        "\n",
        "            out1[:, ind_a:ind_b] = self.INV_AMP(x, self.R_INV)\n",
        "            if self.monitor_volts: tensor_stats(out1, f\"All Voltages in Recurrent Stage\")\n",
        "\n",
        "            feedback = out1[:, ind_a:ind_b].repeat(1,self.final_repeats)\n",
        "\n",
        "        x = torch.cat((feedback, bias2, out1), dim = 1)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Feature Extraction Layer\n",
        "        x = F.linear(x, g[:self.output_size, : ], bias=None)\n",
        "        x = self.INV_AMP(x, self.R_INV)\n",
        "        if self.monitor_volts: tensor_stats(x, f\"Voltages after h_layer {h_pass}\")\n",
        "\n",
        "        return x\n",
        "\n",
        "    def backprop(self, ext_lr):\n",
        "        with torch.no_grad():\n",
        "            if self.w.grad is not None:\n",
        "                grad = self.w.grad.to(device)\n",
        "                for i in range(grad.shape[0]):\n",
        "                    if self.int_norm:\n",
        "                        grad[i] = self.int_lr * grad[i] / (torch.norm(grad[i]) + 1e-20)\n",
        "                    grad[i] = ext_lr * grad[i]\n",
        "                if self.monitor_grads: tensor_stats(grad, \"Gradients\")\n",
        "                self.w -= grad\n",
        "                self.w.grad.zero_()\n",
        "\n",
        "    def anneal(self, inputs, labels, decay1, decay2):\n",
        "        with torch.no_grad():\n",
        "            outputs = self.forward(inputs)\n",
        "            old_loss = criterion(outputs, labels).item() * inputs.size(0)\n",
        "            old_w = self.w.data.clone()\n",
        "\n",
        "            self.w.data = self.w.data * self.ANNEALER()\n",
        "            outputs = self.forward(inputs)\n",
        "            new_loss = criterion(outputs, labels).item() * inputs.size(0)\n",
        "\n",
        "            acceptance_prob = torch.exp(torch.tensor(-(new_loss - old_loss) / self.temperature_2, device=self.device))\n",
        "            if self.monitor_annealing: print(\"Old & New Losses\", old_loss, new_loss,\"Probab:\", acceptance_prob)\n",
        "            if new_loss < old_loss or torch.rand(1, device=self.device) < acceptance_prob:\n",
        "                if self.monitor_annealing: print(\"Annealed weights accepted\")\n",
        "            else:\n",
        "                self.w.data = old_w\n",
        "\n",
        "            self.temperature_1 *= decay1\n",
        "            self.temperature_2 *= decay2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "845d51da-8368-4c97-89a2-9fc1374f408b",
      "metadata": {
        "id": "845d51da-8368-4c97-89a2-9fc1374f408b"
      },
      "source": [
        "## Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "d332b5ba-a9e0-4e8a-bff9-3176267bef00",
      "metadata": {
        "id": "d332b5ba-a9e0-4e8a-bff9-3176267bef00"
      },
      "outputs": [],
      "source": [
        "params_RRAM = {\n",
        "    \"scaling\": 5,\n",
        "    \"G_ON\": 6e-5,\n",
        "    \"G_OFF\": 2.88e-6,\n",
        "    \"V_INV\": 0.6,\n",
        "    \"R_INV\": 1000.0,\n",
        "    \"V_1\": 0.1,\n",
        "    \"V_0\": -0.1,\n",
        "    \"zeta\": 10.0,\n",
        "    \"initial_factor\": 0.01,\n",
        "    \"crossbar\": (64, 64),\n",
        "    \"input_size\": 676,\n",
        "    \"encoding_size\": 4,\n",
        "    \"output_size\": 10,\n",
        "    \"data_in\": 52,\n",
        "    \"bin_active\": True,\n",
        "    \"monitor_volts\": False,\n",
        "    \"monitor_grads\": False,\n",
        "    \"monitor_latents\": False,\n",
        "    \"dropout\": 0.1,\n",
        "    \"int_lr\": 0.01,\n",
        "    \"int_norm\": True,\n",
        "    \"ext_lr\": 500,\n",
        "    \"epochs\": 1000,\n",
        "    \"temperature_1\": 1,\n",
        "    \"temperature_2\": 5000,\n",
        "    \"monitor_annealing\": True\n",
        "}\n",
        "\n",
        "\n",
        "model_params = {k: v for k, v in params_RRAM.items() if k not in [\"noise_std\", \"batch_size\", \"lr\", \"epochs\",\"ext_lr\"]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "83a6271e-f7f2-4be1-8a71-b162ad2055e9",
      "metadata": {
        "scrolled": true,
        "id": "83a6271e-f7f2-4be1-8a71-b162ad2055e9"
      },
      "outputs": [],
      "source": [
        "model_RRAM = SoftBinaryRecurrentForwardNetwork(**model_params).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "241d1155-be9e-4a7a-9a74-a1d608188350",
      "metadata": {
        "id": "241d1155-be9e-4a7a-9a74-a1d608188350"
      },
      "source": [
        "## Training:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25c0b7d2-7c19-4b3e-8856-76d2e9f32792",
      "metadata": {
        "id": "25c0b7d2-7c19-4b3e-8856-76d2e9f32792"
      },
      "source": [
        "### Training to a subset of Dataset First\n",
        "\n",
        "This is just to see if the model is backpropagating before putting in into the full training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "efd13def-9751-4e0e-89bf-666a98444d02",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efd13def-9751-4e0e-89bf-666a98444d02",
        "outputId": "76bd894d-87a5-429b-bb31-d646e8921856",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Old & New Losses 2307.6751232147217 2302.8006553649902 Probab: tensor(1.0010, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 1, LR: 20.0000, Train Loss: 2.3026, Train Accuracy: 8.30%, Temperatures:(0.99, 4950.00)\n",
            "Old & New Losses 2421.16117477417 2304.0411472320557 Probab: tensor(1.0239, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 2, LR: 100.0000, Train Loss: 2.3028, Train Accuracy: 8.10%, Temperatures:(0.98, 4900.50)\n",
            "Old & New Losses 2425.926685333252 2325.026750564575 Probab: tensor(1.0208, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 3, LR: 500.0000, Train Loss: 2.3046, Train Accuracy: 12.40%, Temperatures:(0.97, 4851.49)\n",
            "Old & New Losses 2400.8755683898926 2305.0570487976074 Probab: tensor(1.0199, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 4, LR: 500.0000, Train Loss: 2.3227, Train Accuracy: 13.10%, Temperatures:(0.96, 4802.98)\n",
            "Old & New Losses 2416.4750576019287 2303.57027053833 Probab: tensor(1.0238, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 5, LR: 500.0000, Train Loss: 2.3044, Train Accuracy: 10.90%, Temperatures:(0.95, 4754.95)\n",
            "Old & New Losses 2300.70424079895 2305.2234649658203 Probab: tensor(0.9991, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 6, LR: 500.0000, Train Loss: 2.3043, Train Accuracy: 9.50%, Temperatures:(0.94, 4707.40)\n",
            "Old & New Losses 2330.681562423706 2304.2237758636475 Probab: tensor(1.0056, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 7, LR: 500.0000, Train Loss: 2.3052, Train Accuracy: 7.60%, Temperatures:(0.93, 4660.33)\n",
            "Old & New Losses 2293.8625812530518 2302.3035526275635 Probab: tensor(0.9982, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 8, LR: 500.0000, Train Loss: 2.3039, Train Accuracy: 9.00%, Temperatures:(0.92, 4613.72)\n",
            "Old & New Losses 2304.579019546509 2299.4565963745117 Probab: tensor(1.0011, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 9, LR: 500.0000, Train Loss: 2.3031, Train Accuracy: 11.60%, Temperatures:(0.91, 4567.59)\n",
            "Old & New Losses 2338.90438079834 2310.427188873291 Probab: tensor(1.0063, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 10, LR: 500.0000, Train Loss: 2.3016, Train Accuracy: 11.60%, Temperatures:(0.90, 4521.91)\n",
            "Old & New Losses 2344.70272064209 2303.4520149230957 Probab: tensor(1.0092, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 11, LR: 500.0000, Train Loss: 2.3104, Train Accuracy: 11.90%, Temperatures:(0.90, 4476.69)\n",
            "Old & New Losses 2382.5578689575195 2305.277109146118 Probab: tensor(1.0174, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 12, LR: 500.0000, Train Loss: 2.3038, Train Accuracy: 10.70%, Temperatures:(0.89, 4431.92)\n",
            "Old & New Losses 2300.1739978790283 2302.4046421051025 Probab: tensor(0.9995, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 13, LR: 500.0000, Train Loss: 2.3053, Train Accuracy: 10.90%, Temperatures:(0.88, 4387.61)\n",
            "Old & New Losses 2353.149175643921 2303.997278213501 Probab: tensor(1.0113, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 14, LR: 500.0000, Train Loss: 2.3013, Train Accuracy: 10.10%, Temperatures:(0.87, 4343.73)\n",
            "Old & New Losses 2319.4026947021484 2301.757335662842 Probab: tensor(1.0041, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 15, LR: 500.0000, Train Loss: 2.3033, Train Accuracy: 9.30%, Temperatures:(0.86, 4300.29)\n",
            "Old & New Losses 2297.36590385437 2304.009437561035 Probab: tensor(0.9985, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 16, LR: 500.0000, Train Loss: 2.3021, Train Accuracy: 8.60%, Temperatures:(0.85, 4257.29)\n",
            "Old & New Losses 2358.2427501678467 2300.4560470581055 Probab: tensor(1.0137, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 17, LR: 500.0000, Train Loss: 2.3043, Train Accuracy: 10.20%, Temperatures:(0.84, 4214.72)\n",
            "Old & New Losses 2363.938808441162 2300.109624862671 Probab: tensor(1.0153, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 18, LR: 500.0000, Train Loss: 2.3007, Train Accuracy: 7.30%, Temperatures:(0.83, 4172.57)\n",
            "Old & New Losses 2266.4437294006348 2300.7538318634033 Probab: tensor(0.9918, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 19, LR: 500.0000, Train Loss: 2.3012, Train Accuracy: 11.10%, Temperatures:(0.83, 4130.84)\n",
            "Old & New Losses 2342.9408073425293 2299.8762130737305 Probab: tensor(1.0105, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 20, LR: 500.0000, Train Loss: 2.3013, Train Accuracy: 9.60%, Temperatures:(0.82, 4089.53)\n",
            "Old & New Losses 2303.2402992248535 2302.0267486572266 Probab: tensor(1.0003, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 21, LR: 500.0000, Train Loss: 2.2980, Train Accuracy: 11.50%, Temperatures:(0.81, 4048.64)\n",
            "Old & New Losses 2316.5054321289062 2301.5127182006836 Probab: tensor(1.0037, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 22, LR: 500.0000, Train Loss: 2.3012, Train Accuracy: 11.50%, Temperatures:(0.80, 4008.15)\n",
            "Old & New Losses 2291.3944721221924 2304.3699264526367 Probab: tensor(0.9968, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 23, LR: 500.0000, Train Loss: 2.3017, Train Accuracy: 9.50%, Temperatures:(0.79, 3968.07)\n",
            "Old & New Losses 2332.5321674346924 2303.3411502838135 Probab: tensor(1.0074, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 24, LR: 500.0000, Train Loss: 2.3048, Train Accuracy: 9.80%, Temperatures:(0.79, 3928.39)\n",
            "Old & New Losses 2288.405656814575 2296.2725162506104 Probab: tensor(0.9980, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 25, LR: 500.0000, Train Loss: 2.3029, Train Accuracy: 10.70%, Temperatures:(0.78, 3889.11)\n",
            "Old & New Losses 2314.642906188965 2299.295663833618 Probab: tensor(1.0040, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 26, LR: 500.0000, Train Loss: 2.2990, Train Accuracy: 9.40%, Temperatures:(0.77, 3850.22)\n",
            "Old & New Losses 2409.2636108398438 2303.332805633545 Probab: tensor(1.0279, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 27, LR: 500.0000, Train Loss: 2.3000, Train Accuracy: 10.10%, Temperatures:(0.76, 3811.71)\n",
            "Old & New Losses 2384.219169616699 2305.337429046631 Probab: tensor(1.0209, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 28, LR: 500.0000, Train Loss: 2.3030, Train Accuracy: 10.00%, Temperatures:(0.75, 3773.60)\n",
            "Old & New Losses 2303.248882293701 2305.6890964508057 Probab: tensor(0.9994, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 29, LR: 500.0000, Train Loss: 2.3052, Train Accuracy: 12.30%, Temperatures:(0.75, 3735.86)\n",
            "Old & New Losses 2294.433355331421 2299.987316131592 Probab: tensor(0.9985, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 30, LR: 500.0000, Train Loss: 2.3045, Train Accuracy: 8.50%, Temperatures:(0.74, 3698.50)\n",
            "Old & New Losses 2284.074306488037 2300.0710010528564 Probab: tensor(0.9957, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 31, LR: 500.0000, Train Loss: 2.3017, Train Accuracy: 8.30%, Temperatures:(0.73, 3661.52)\n",
            "Old & New Losses 2357.19633102417 2303.0006885528564 Probab: tensor(1.0149, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 32, LR: 500.0000, Train Loss: 2.3005, Train Accuracy: 10.40%, Temperatures:(0.72, 3624.90)\n",
            "Old & New Losses 2291.311740875244 2302.635908126831 Probab: tensor(0.9969, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 33, LR: 500.0000, Train Loss: 2.3022, Train Accuracy: 9.00%, Temperatures:(0.72, 3588.65)\n",
            "Old & New Losses 2284.8761081695557 2302.978515625 Probab: tensor(0.9950, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 34, LR: 500.0000, Train Loss: 2.3026, Train Accuracy: 10.30%, Temperatures:(0.71, 3552.77)\n",
            "Old & New Losses 2309.3254566192627 2300.636053085327 Probab: tensor(1.0024, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 35, LR: 500.0000, Train Loss: 2.3024, Train Accuracy: 10.20%, Temperatures:(0.70, 3517.24)\n",
            "Old & New Losses 2323.3344554901123 2294.9039936065674 Probab: tensor(1.0081, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 36, LR: 500.0000, Train Loss: 2.3008, Train Accuracy: 13.10%, Temperatures:(0.70, 3482.07)\n",
            "Old & New Losses 2306.0691356658936 2291.484594345093 Probab: tensor(1.0042, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 37, LR: 500.0000, Train Loss: 2.2988, Train Accuracy: 12.00%, Temperatures:(0.69, 3447.25)\n",
            "Old & New Losses 2299.257278442383 2300.694227218628 Probab: tensor(0.9996, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 38, LR: 500.0000, Train Loss: 2.2913, Train Accuracy: 10.10%, Temperatures:(0.68, 3412.77)\n",
            "Old & New Losses 2364.051342010498 2297.6036071777344 Probab: tensor(1.0197, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 39, LR: 500.0000, Train Loss: 2.3004, Train Accuracy: 12.30%, Temperatures:(0.68, 3378.65)\n",
            "Old & New Losses 2323.012113571167 2302.0434379577637 Probab: tensor(1.0062, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 40, LR: 500.0000, Train Loss: 2.2992, Train Accuracy: 10.40%, Temperatures:(0.67, 3344.86)\n",
            "Old & New Losses 2288.7210845947266 2299.609661102295 Probab: tensor(0.9967, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 41, LR: 500.0000, Train Loss: 2.3009, Train Accuracy: 9.60%, Temperatures:(0.66, 3311.41)\n",
            "Old & New Losses 2291.5685176849365 2300.2216815948486 Probab: tensor(0.9974, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 42, LR: 500.0000, Train Loss: 2.2996, Train Accuracy: 9.10%, Temperatures:(0.66, 3278.30)\n",
            "Old & New Losses 2262.409448623657 2286.29994392395 Probab: tensor(0.9927, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 43, LR: 500.0000, Train Loss: 2.2985, Train Accuracy: 9.60%, Temperatures:(0.65, 3245.51)\n",
            "Old & New Losses 2328.382730484009 2288.264751434326 Probab: tensor(1.0124, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 44, LR: 500.0000, Train Loss: 2.2896, Train Accuracy: 12.30%, Temperatures:(0.64, 3213.06)\n",
            "Old & New Losses 2301.231861114502 2294.182777404785 Probab: tensor(1.0022, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 45, LR: 500.0000, Train Loss: 2.2870, Train Accuracy: 10.10%, Temperatures:(0.64, 3180.93)\n",
            "Old & New Losses 2296.767234802246 2300.0621795654297 Probab: tensor(0.9990, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 46, LR: 500.0000, Train Loss: 2.2955, Train Accuracy: 13.20%, Temperatures:(0.63, 3149.12)\n",
            "Old & New Losses 2309.018135070801 2295.6974506378174 Probab: tensor(1.0042, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 47, LR: 500.0000, Train Loss: 2.3005, Train Accuracy: 11.30%, Temperatures:(0.62, 3117.63)\n",
            "Old & New Losses 2264.8255825042725 2288.484573364258 Probab: tensor(0.9924, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 48, LR: 500.0000, Train Loss: 2.2971, Train Accuracy: 8.40%, Temperatures:(0.62, 3086.45)\n",
            "Old & New Losses 2358.076810836792 2301.3391494750977 Probab: tensor(1.0186, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 49, LR: 500.0000, Train Loss: 2.2888, Train Accuracy: 10.40%, Temperatures:(0.61, 3055.59)\n",
            "Old & New Losses 2342.151880264282 2301.7563819885254 Probab: tensor(1.0133, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 50, LR: 500.0000, Train Loss: 2.3004, Train Accuracy: 10.20%, Temperatures:(0.61, 3025.03)\n",
            "Old & New Losses 2276.8468856811523 2301.8622398376465 Probab: tensor(0.9918, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 51, LR: 500.0000, Train Loss: 2.3000, Train Accuracy: 11.40%, Temperatures:(0.60, 2994.78)\n",
            "Old & New Losses 2276.6711711883545 2300.2281188964844 Probab: tensor(0.9922, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 52, LR: 250.0000, Train Loss: 2.3030, Train Accuracy: 10.50%, Temperatures:(0.59, 2964.83)\n",
            "Old & New Losses 2277.4064540863037 2295.6600189208984 Probab: tensor(0.9939, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 53, LR: 250.0000, Train Loss: 2.2999, Train Accuracy: 9.20%, Temperatures:(0.59, 2935.18)\n",
            "Old & New Losses 2312.1023178100586 2295.4177856445312 Probab: tensor(1.0057, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 54, LR: 250.0000, Train Loss: 2.2972, Train Accuracy: 9.00%, Temperatures:(0.58, 2905.83)\n",
            "Old & New Losses 2298.2187271118164 2300.374984741211 Probab: tensor(0.9993, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 55, LR: 250.0000, Train Loss: 2.2969, Train Accuracy: 11.00%, Temperatures:(0.58, 2876.77)\n",
            "Old & New Losses 2406.1522483825684 2297.7468967437744 Probab: tensor(1.0384, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 56, LR: 250.0000, Train Loss: 2.3013, Train Accuracy: 10.20%, Temperatures:(0.57, 2848.01)\n",
            "Old & New Losses 2298.867702484131 2296.975612640381 Probab: tensor(1.0007, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 57, LR: 250.0000, Train Loss: 2.2988, Train Accuracy: 9.20%, Temperatures:(0.56, 2819.53)\n",
            "Old & New Losses 2269.0656185150146 2298.9988327026367 Probab: tensor(0.9894, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 58, LR: 250.0000, Train Loss: 2.2979, Train Accuracy: 10.40%, Temperatures:(0.56, 2791.33)\n",
            "Old & New Losses 2283.9341163635254 2299.4558811187744 Probab: tensor(0.9945, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 59, LR: 250.0000, Train Loss: 2.2995, Train Accuracy: 7.80%, Temperatures:(0.55, 2763.42)\n",
            "Old & New Losses 2284.5067977905273 2298.2606887817383 Probab: tensor(0.9950, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 60, LR: 250.0000, Train Loss: 2.3026, Train Accuracy: 8.30%, Temperatures:(0.55, 2735.78)\n",
            "Old & New Losses 2360.330581665039 2296.7913150787354 Probab: tensor(1.0235, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 61, LR: 250.0000, Train Loss: 2.2982, Train Accuracy: 9.80%, Temperatures:(0.54, 2708.43)\n",
            "Old & New Losses 2333.728075027466 2327.6915550231934 Probab: tensor(1.0022, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 62, LR: 250.0000, Train Loss: 2.2988, Train Accuracy: 11.70%, Temperatures:(0.54, 2681.34)\n",
            "Old & New Losses 2290.383815765381 2299.1604804992676 Probab: tensor(0.9967, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 63, LR: 250.0000, Train Loss: 2.3232, Train Accuracy: 10.30%, Temperatures:(0.53, 2654.53)\n",
            "Old & New Losses 2247.906446456909 2300.4305362701416 Probab: tensor(0.9804, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 64, LR: 250.0000, Train Loss: 2.2976, Train Accuracy: 10.80%, Temperatures:(0.53, 2627.98)\n",
            "Old & New Losses 2307.1157932281494 2295.463800430298 Probab: tensor(1.0044, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 65, LR: 250.0000, Train Loss: 2.3007, Train Accuracy: 10.70%, Temperatures:(0.52, 2601.70)\n",
            "Old & New Losses 2397.7646827697754 2291.966199874878 Probab: tensor(1.0415, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 66, LR: 250.0000, Train Loss: 2.2946, Train Accuracy: 13.00%, Temperatures:(0.52, 2575.69)\n",
            "Old & New Losses 2395.4405784606934 2290.318012237549 Probab: tensor(1.0417, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 67, LR: 250.0000, Train Loss: 2.2864, Train Accuracy: 11.80%, Temperatures:(0.51, 2549.93)\n",
            "Old & New Losses 2287.5313758850098 2302.3440837860107 Probab: tensor(0.9942, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 68, LR: 250.0000, Train Loss: 2.2946, Train Accuracy: 11.30%, Temperatures:(0.50, 2524.43)\n",
            "Old & New Losses 2313.1330013275146 2298.1083393096924 Probab: tensor(1.0060, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 69, LR: 250.0000, Train Loss: 2.3028, Train Accuracy: 7.80%, Temperatures:(0.50, 2499.19)\n",
            "Old & New Losses 2416.4459705352783 2350.368022918701 Probab: tensor(1.0268, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 70, LR: 250.0000, Train Loss: 2.2969, Train Accuracy: 11.60%, Temperatures:(0.49, 2474.19)\n",
            "Old & New Losses 2308.143138885498 2300.370216369629 Probab: tensor(1.0031, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 71, LR: 250.0000, Train Loss: 2.3469, Train Accuracy: 10.70%, Temperatures:(0.49, 2449.45)\n",
            "Old & New Losses 2323.9238262176514 2299.72767829895 Probab: tensor(1.0099, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 72, LR: 250.0000, Train Loss: 2.3005, Train Accuracy: 10.20%, Temperatures:(0.48, 2424.96)\n",
            "Old & New Losses 2277.2703170776367 2298.060417175293 Probab: tensor(0.9915, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 73, LR: 250.0000, Train Loss: 2.3000, Train Accuracy: 10.00%, Temperatures:(0.48, 2400.71)\n",
            "Old & New Losses 2297.226905822754 2305.356979370117 Probab: tensor(0.9966, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 74, LR: 250.0000, Train Loss: 2.2976, Train Accuracy: 13.70%, Temperatures:(0.48, 2376.70)\n",
            "Old & New Losses 2266.681671142578 2293.4494018554688 Probab: tensor(0.9888, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 75, LR: 250.0000, Train Loss: 2.3044, Train Accuracy: 10.30%, Temperatures:(0.47, 2352.93)\n",
            "Old & New Losses 2258.513927459717 2298.492193222046 Probab: tensor(0.9832, device='cuda:0')\n",
            "Epoch 76, LR: 250.0000, Train Loss: 2.2923, Train Accuracy: 9.60%, Temperatures:(0.47, 2329.40)\n",
            "Old & New Losses 2250.2880096435547 2279.7412872314453 Probab: tensor(0.9874, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 77, LR: 250.0000, Train Loss: 2.2602, Train Accuracy: 14.20%, Temperatures:(0.46, 2306.11)\n",
            "Old & New Losses 2300.313711166382 2293.3061122894287 Probab: tensor(1.0030, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 78, LR: 250.0000, Train Loss: 2.2803, Train Accuracy: 9.70%, Temperatures:(0.46, 2283.05)\n",
            "Old & New Losses 2279.6761989593506 2296.8227863311768 Probab: tensor(0.9925, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 79, LR: 250.0000, Train Loss: 2.2940, Train Accuracy: 10.70%, Temperatures:(0.45, 2260.22)\n",
            "Old & New Losses 2292.2110557556152 2292.041778564453 Probab: tensor(1.0001, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 80, LR: 250.0000, Train Loss: 2.2970, Train Accuracy: 9.60%, Temperatures:(0.45, 2237.62)\n",
            "Old & New Losses 2275.4273414611816 2289.4606590270996 Probab: tensor(0.9937, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 81, LR: 250.0000, Train Loss: 2.2952, Train Accuracy: 10.60%, Temperatures:(0.44, 2215.24)\n",
            "Old & New Losses 2297.011375427246 2290.1668548583984 Probab: tensor(1.0031, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 82, LR: 250.0000, Train Loss: 2.2901, Train Accuracy: 11.90%, Temperatures:(0.44, 2193.09)\n",
            "Old & New Losses 2272.1569538116455 2287.548065185547 Probab: tensor(0.9930, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 83, LR: 250.0000, Train Loss: 2.2886, Train Accuracy: 14.30%, Temperatures:(0.43, 2171.16)\n",
            "Old & New Losses 2306.6511154174805 2283.517360687256 Probab: tensor(1.0107, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 84, LR: 250.0000, Train Loss: 2.2874, Train Accuracy: 12.10%, Temperatures:(0.43, 2149.45)\n",
            "Old & New Losses 2378.3226013183594 2294.106960296631 Probab: tensor(1.0400, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 85, LR: 250.0000, Train Loss: 2.2852, Train Accuracy: 12.10%, Temperatures:(0.43, 2127.95)\n",
            "Old & New Losses 2277.2819995880127 2290.44508934021 Probab: tensor(0.9938, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 86, LR: 250.0000, Train Loss: 2.2908, Train Accuracy: 12.70%, Temperatures:(0.42, 2106.67)\n",
            "Old & New Losses 2259.894371032715 2279.681444168091 Probab: tensor(0.9907, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 87, LR: 250.0000, Train Loss: 2.2928, Train Accuracy: 12.20%, Temperatures:(0.42, 2085.60)\n",
            "Old & New Losses 2264.9569511413574 2282.217025756836 Probab: tensor(0.9918, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 88, LR: 250.0000, Train Loss: 2.2839, Train Accuracy: 10.70%, Temperatures:(0.41, 2064.75)\n",
            "Old & New Losses 2240.400791168213 2291.576862335205 Probab: tensor(0.9755, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 89, LR: 250.0000, Train Loss: 2.2842, Train Accuracy: 9.50%, Temperatures:(0.41, 2044.10)\n",
            "Old & New Losses 2260.6520652770996 2297.6162433624268 Probab: tensor(0.9821, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 90, LR: 250.0000, Train Loss: 2.2870, Train Accuracy: 10.60%, Temperatures:(0.40, 2023.66)\n",
            "Old & New Losses 2272.150993347168 2293.4703826904297 Probab: tensor(0.9895, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 91, LR: 250.0000, Train Loss: 2.2977, Train Accuracy: 12.60%, Temperatures:(0.40, 2003.42)\n",
            "Old & New Losses 2262.453556060791 2286.034345626831 Probab: tensor(0.9883, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 92, LR: 250.0000, Train Loss: 2.2924, Train Accuracy: 10.20%, Temperatures:(0.40, 1983.39)\n",
            "Old & New Losses 2255.6958198547363 2283.283472061157 Probab: tensor(0.9862, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 93, LR: 250.0000, Train Loss: 2.2898, Train Accuracy: 10.00%, Temperatures:(0.39, 1963.56)\n",
            "Old & New Losses 2226.4578342437744 2274.1353511810303 Probab: tensor(0.9760, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 94, LR: 250.0000, Train Loss: 2.2827, Train Accuracy: 12.50%, Temperatures:(0.39, 1943.92)\n",
            "Old & New Losses 2216.5491580963135 2267.7042484283447 Probab: tensor(0.9740, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 95, LR: 250.0000, Train Loss: 2.2774, Train Accuracy: 12.10%, Temperatures:(0.38, 1924.48)\n",
            "Old & New Losses 2245.500087738037 2275.2397060394287 Probab: tensor(0.9847, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 96, LR: 250.0000, Train Loss: 2.2645, Train Accuracy: 16.20%, Temperatures:(0.38, 1905.24)\n",
            "Old & New Losses 2273.5328674316406 2303.5027980804443 Probab: tensor(0.9844, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 97, LR: 250.0000, Train Loss: 2.2704, Train Accuracy: 13.70%, Temperatures:(0.38, 1886.18)\n",
            "Old & New Losses 2255.67889213562 2281.9032669067383 Probab: tensor(0.9862, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 98, LR: 250.0000, Train Loss: 2.3066, Train Accuracy: 10.80%, Temperatures:(0.37, 1867.32)\n",
            "Old & New Losses 2255.441188812256 2272.3443508148193 Probab: tensor(0.9910, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 99, LR: 250.0000, Train Loss: 2.2853, Train Accuracy: 12.40%, Temperatures:(0.37, 1848.65)\n",
            "Old & New Losses 2235.776424407959 2269.8891162872314 Probab: tensor(0.9817, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 100, LR: 250.0000, Train Loss: 2.2706, Train Accuracy: 13.70%, Temperatures:(0.37, 1830.16)\n",
            "Old & New Losses 2232.4960231781006 2276.069164276123 Probab: tensor(0.9765, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 101, LR: 250.0000, Train Loss: 2.2671, Train Accuracy: 12.90%, Temperatures:(0.36, 1811.86)\n",
            "Old & New Losses 2242.6798343658447 2291.66316986084 Probab: tensor(0.9733, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 102, LR: 125.0000, Train Loss: 2.2744, Train Accuracy: 16.00%, Temperatures:(0.36, 1793.74)\n",
            "Old & New Losses 2254.1468143463135 2281.742811203003 Probab: tensor(0.9847, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 103, LR: 125.0000, Train Loss: 2.2913, Train Accuracy: 10.60%, Temperatures:(0.36, 1775.80)\n",
            "Old & New Losses 2257.187604904175 2275.959014892578 Probab: tensor(0.9895, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 104, LR: 125.0000, Train Loss: 2.2826, Train Accuracy: 13.00%, Temperatures:(0.35, 1758.05)\n",
            "Old & New Losses 2266.7908668518066 2272.3875045776367 Probab: tensor(0.9968, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 105, LR: 125.0000, Train Loss: 2.2785, Train Accuracy: 11.30%, Temperatures:(0.35, 1740.47)\n",
            "Old & New Losses 2306.9751262664795 2285.949468612671 Probab: tensor(1.0122, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 106, LR: 125.0000, Train Loss: 2.2719, Train Accuracy: 16.40%, Temperatures:(0.34, 1723.06)\n",
            "Old & New Losses 2272.8545665740967 2284.900665283203 Probab: tensor(0.9930, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 107, LR: 125.0000, Train Loss: 2.2843, Train Accuracy: 14.40%, Temperatures:(0.34, 1705.83)\n",
            "Old & New Losses 2256.4778327941895 2269.747734069824 Probab: tensor(0.9923, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 108, LR: 125.0000, Train Loss: 2.2870, Train Accuracy: 13.70%, Temperatures:(0.34, 1688.77)\n",
            "Old & New Losses 2348.937749862671 2336.0085487365723 Probab: tensor(1.0077, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 109, LR: 125.0000, Train Loss: 2.2746, Train Accuracy: 12.90%, Temperatures:(0.33, 1671.88)\n",
            "Old & New Losses 2249.448776245117 2276.662826538086 Probab: tensor(0.9839, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 110, LR: 125.0000, Train Loss: 2.3332, Train Accuracy: 11.10%, Temperatures:(0.33, 1655.17)\n",
            "Old & New Losses 2288.8169288635254 2272.4828720092773 Probab: tensor(1.0099, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 111, LR: 125.0000, Train Loss: 2.2770, Train Accuracy: 14.90%, Temperatures:(0.33, 1638.61)\n",
            "Old & New Losses 2266.1187648773193 2278.0585289001465 Probab: tensor(0.9927, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 112, LR: 125.0000, Train Loss: 2.2712, Train Accuracy: 14.40%, Temperatures:(0.32, 1622.23)\n",
            "Old & New Losses 2264.957904815674 2275.193452835083 Probab: tensor(0.9937, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 113, LR: 125.0000, Train Loss: 2.2773, Train Accuracy: 12.20%, Temperatures:(0.32, 1606.01)\n",
            "Old & New Losses 2268.7017917633057 2275.236129760742 Probab: tensor(0.9959, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 114, LR: 125.0000, Train Loss: 2.2720, Train Accuracy: 13.40%, Temperatures:(0.32, 1589.95)\n",
            "Old & New Losses 2238.4467124938965 2264.737606048584 Probab: tensor(0.9836, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 115, LR: 125.0000, Train Loss: 2.2780, Train Accuracy: 9.90%, Temperatures:(0.31, 1574.05)\n",
            "Old & New Losses 2253.1728744506836 2262.369394302368 Probab: tensor(0.9942, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 116, LR: 125.0000, Train Loss: 2.2628, Train Accuracy: 11.50%, Temperatures:(0.31, 1558.31)\n",
            "Old & New Losses 2234.494209289551 2273.144245147705 Probab: tensor(0.9755, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 117, LR: 125.0000, Train Loss: 2.2588, Train Accuracy: 11.40%, Temperatures:(0.31, 1542.72)\n",
            "Old & New Losses 2235.3603839874268 2254.046678543091 Probab: tensor(0.9880, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 118, LR: 125.0000, Train Loss: 2.2705, Train Accuracy: 13.90%, Temperatures:(0.31, 1527.30)\n",
            "Old & New Losses 2281.6340923309326 2283.165216445923 Probab: tensor(0.9990, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 119, LR: 125.0000, Train Loss: 2.2572, Train Accuracy: 12.60%, Temperatures:(0.30, 1512.02)\n",
            "Old & New Losses 2241.77622795105 2264.819622039795 Probab: tensor(0.9849, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 120, LR: 125.0000, Train Loss: 2.2810, Train Accuracy: 9.30%, Temperatures:(0.30, 1496.90)\n",
            "Old & New Losses 2263.5128498077393 2253.7379264831543 Probab: tensor(1.0066, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 121, LR: 125.0000, Train Loss: 2.2640, Train Accuracy: 10.90%, Temperatures:(0.30, 1481.93)\n",
            "Old & New Losses 2251.4138221740723 2252.6581287384033 Probab: tensor(0.9992, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 122, LR: 125.0000, Train Loss: 2.2499, Train Accuracy: 14.80%, Temperatures:(0.29, 1467.11)\n",
            "Old & New Losses 2212.3608589172363 2249.9306201934814 Probab: tensor(0.9747, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 123, LR: 125.0000, Train Loss: 2.2531, Train Accuracy: 12.70%, Temperatures:(0.29, 1452.44)\n",
            "Old & New Losses 2236.4537715911865 2251.3768672943115 Probab: tensor(0.9898, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 124, LR: 125.0000, Train Loss: 2.2530, Train Accuracy: 11.30%, Temperatures:(0.29, 1437.92)\n",
            "Old & New Losses 2262.897491455078 2230.888843536377 Probab: tensor(1.0225, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 125, LR: 125.0000, Train Loss: 2.2518, Train Accuracy: 10.50%, Temperatures:(0.28, 1423.54)\n",
            "Old & New Losses 2252.368450164795 2221.7962741851807 Probab: tensor(1.0217, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 126, LR: 125.0000, Train Loss: 2.2275, Train Accuracy: 12.60%, Temperatures:(0.28, 1409.30)\n",
            "Old & New Losses 2287.809371948242 2323.133945465088 Probab: tensor(0.9752, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 127, LR: 125.0000, Train Loss: 2.2204, Train Accuracy: 14.30%, Temperatures:(0.28, 1395.21)\n",
            "Old & New Losses 2228.2493114471436 2232.841968536377 Probab: tensor(0.9967, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 128, LR: 125.0000, Train Loss: 2.3221, Train Accuracy: 13.20%, Temperatures:(0.28, 1381.26)\n",
            "Old & New Losses 2257.734775543213 2259.706974029541 Probab: tensor(0.9986, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 129, LR: 125.0000, Train Loss: 2.2386, Train Accuracy: 11.00%, Temperatures:(0.27, 1367.45)\n",
            "Old & New Losses 2236.0730171203613 2229.412317276001 Probab: tensor(1.0049, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 130, LR: 125.0000, Train Loss: 2.2575, Train Accuracy: 9.90%, Temperatures:(0.27, 1353.77)\n",
            "Old & New Losses 2222.9862213134766 2233.8788509368896 Probab: tensor(0.9920, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 131, LR: 125.0000, Train Loss: 2.2377, Train Accuracy: 10.00%, Temperatures:(0.27, 1340.23)\n",
            "Old & New Losses 2213.2246494293213 2261.807441711426 Probab: tensor(0.9644, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 132, LR: 125.0000, Train Loss: 2.2376, Train Accuracy: 12.60%, Temperatures:(0.27, 1326.83)\n",
            "Old & New Losses 2213.989496231079 2219.663143157959 Probab: tensor(0.9957, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 133, LR: 125.0000, Train Loss: 2.2573, Train Accuracy: 13.90%, Temperatures:(0.26, 1313.56)\n",
            "Old & New Losses 2218.3003425598145 2218.211889266968 Probab: tensor(1.0001, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 134, LR: 125.0000, Train Loss: 2.2195, Train Accuracy: 14.10%, Temperatures:(0.26, 1300.43)\n",
            "Old & New Losses 2221.123218536377 2212.7654552459717 Probab: tensor(1.0064, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 135, LR: 125.0000, Train Loss: 2.2206, Train Accuracy: 13.50%, Temperatures:(0.26, 1287.42)\n",
            "Old & New Losses 2227.6628017425537 2226.1781692504883 Probab: tensor(1.0012, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 136, LR: 125.0000, Train Loss: 2.2182, Train Accuracy: 16.40%, Temperatures:(0.25, 1274.55)\n",
            "Old & New Losses 2186.4607334136963 2207.8211307525635 Probab: tensor(0.9834, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 137, LR: 125.0000, Train Loss: 2.2220, Train Accuracy: 13.50%, Temperatures:(0.25, 1261.80)\n",
            "Old & New Losses 2245.304822921753 2223.743438720703 Probab: tensor(1.0172, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 138, LR: 125.0000, Train Loss: 2.2091, Train Accuracy: 14.00%, Temperatures:(0.25, 1249.19)\n",
            "Old & New Losses 2199.678659439087 2192.89231300354 Probab: tensor(1.0054, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 139, LR: 125.0000, Train Loss: 2.2220, Train Accuracy: 21.10%, Temperatures:(0.25, 1236.69)\n",
            "Old & New Losses 2201.3683319091797 2193.4399604797363 Probab: tensor(1.0064, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 140, LR: 125.0000, Train Loss: 2.1913, Train Accuracy: 17.40%, Temperatures:(0.24, 1224.33)\n",
            "Old & New Losses 2214.8022651672363 2223.088264465332 Probab: tensor(0.9933, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 141, LR: 125.0000, Train Loss: 2.1891, Train Accuracy: 21.40%, Temperatures:(0.24, 1212.08)\n",
            "Old & New Losses 2166.217803955078 2182.2333335876465 Probab: tensor(0.9869, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 142, LR: 125.0000, Train Loss: 2.2321, Train Accuracy: 16.50%, Temperatures:(0.24, 1199.96)\n",
            "Old & New Losses 2184.3390464782715 2198.889970779419 Probab: tensor(0.9879, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 143, LR: 125.0000, Train Loss: 2.1844, Train Accuracy: 19.10%, Temperatures:(0.24, 1187.96)\n",
            "Old & New Losses 2206.455945968628 2216.1922454833984 Probab: tensor(0.9918, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 144, LR: 125.0000, Train Loss: 2.1933, Train Accuracy: 15.30%, Temperatures:(0.24, 1176.08)\n",
            "Old & New Losses 2167.1924591064453 2180.0761222839355 Probab: tensor(0.9891, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 145, LR: 125.0000, Train Loss: 2.2141, Train Accuracy: 19.00%, Temperatures:(0.23, 1164.32)\n",
            "Old & New Losses 2169.0332889556885 2176.6278743743896 Probab: tensor(0.9935, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 146, LR: 125.0000, Train Loss: 2.1813, Train Accuracy: 17.60%, Temperatures:(0.23, 1152.68)\n",
            "Old & New Losses 2172.943115234375 2174.4165420532227 Probab: tensor(0.9987, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 147, LR: 125.0000, Train Loss: 2.1755, Train Accuracy: 17.70%, Temperatures:(0.23, 1141.15)\n",
            "Old & New Losses 2174.610376358032 2184.4637393951416 Probab: tensor(0.9914, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 148, LR: 125.0000, Train Loss: 2.1773, Train Accuracy: 17.90%, Temperatures:(0.23, 1129.74)\n",
            "Old & New Losses 2157.357931137085 2177.2305965423584 Probab: tensor(0.9826, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 149, LR: 125.0000, Train Loss: 2.1776, Train Accuracy: 13.70%, Temperatures:(0.22, 1118.44)\n",
            "Old & New Losses 2174.5667457580566 2178.8487434387207 Probab: tensor(0.9962, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 150, LR: 125.0000, Train Loss: 2.1750, Train Accuracy: 18.60%, Temperatures:(0.22, 1107.26)\n",
            "Old & New Losses 2144.9520587921143 2156.737804412842 Probab: tensor(0.9894, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 151, LR: 125.0000, Train Loss: 2.1824, Train Accuracy: 16.20%, Temperatures:(0.22, 1096.19)\n",
            "Old & New Losses 2166.877031326294 2187.7777576446533 Probab: tensor(0.9811, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 152, LR: 62.5000, Train Loss: 2.1625, Train Accuracy: 15.20%, Temperatures:(0.22, 1085.22)\n",
            "Old & New Losses 2157.949447631836 2160.6452465057373 Probab: tensor(0.9975, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 153, LR: 62.5000, Train Loss: 2.1926, Train Accuracy: 14.30%, Temperatures:(0.21, 1074.37)\n",
            "Old & New Losses 2168.278694152832 2158.931016921997 Probab: tensor(1.0087, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 154, LR: 62.5000, Train Loss: 2.1586, Train Accuracy: 15.70%, Temperatures:(0.21, 1063.63)\n",
            "Old & New Losses 2169.045925140381 2177.6437759399414 Probab: tensor(0.9919, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 155, LR: 62.5000, Train Loss: 2.1606, Train Accuracy: 16.30%, Temperatures:(0.21, 1052.99)\n",
            "Old & New Losses 2151.364803314209 2159.2857837677 Probab: tensor(0.9925, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 156, LR: 62.5000, Train Loss: 2.1740, Train Accuracy: 14.30%, Temperatures:(0.21, 1042.46)\n",
            "Old & New Losses 2157.754898071289 2162.236213684082 Probab: tensor(0.9957, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 157, LR: 62.5000, Train Loss: 2.1614, Train Accuracy: 17.30%, Temperatures:(0.21, 1032.04)\n",
            "Old & New Losses 2160.511255264282 2172.9772090911865 Probab: tensor(0.9880, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 158, LR: 62.5000, Train Loss: 2.1626, Train Accuracy: 16.20%, Temperatures:(0.20, 1021.72)\n",
            "Old & New Losses 2157.122850418091 2161.8294715881348 Probab: tensor(0.9954, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 159, LR: 62.5000, Train Loss: 2.1764, Train Accuracy: 17.30%, Temperatures:(0.20, 1011.50)\n",
            "Old & New Losses 2150.5861282348633 2150.1519680023193 Probab: tensor(1.0004, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 160, LR: 62.5000, Train Loss: 2.1650, Train Accuracy: 16.10%, Temperatures:(0.20, 1001.39)\n",
            "Old & New Losses 2181.1704635620117 2164.382219314575 Probab: tensor(1.0169, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 161, LR: 62.5000, Train Loss: 2.1513, Train Accuracy: 17.20%, Temperatures:(0.20, 991.37)\n",
            "Old & New Losses 2148.7648487091064 2162.429094314575 Probab: tensor(0.9863, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 162, LR: 62.5000, Train Loss: 2.1645, Train Accuracy: 17.00%, Temperatures:(0.20, 981.46)\n",
            "Old & New Losses 2151.45206451416 2151.6683101654053 Probab: tensor(0.9998, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 163, LR: 62.5000, Train Loss: 2.1642, Train Accuracy: 15.00%, Temperatures:(0.19, 971.64)\n",
            "Old & New Losses 2140.8891677856445 2160.0358486175537 Probab: tensor(0.9805, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 164, LR: 62.5000, Train Loss: 2.1502, Train Accuracy: 17.30%, Temperatures:(0.19, 961.93)\n",
            "Old & New Losses 2150.129556655884 2186.9943141937256 Probab: tensor(0.9624, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 165, LR: 62.5000, Train Loss: 2.1606, Train Accuracy: 17.00%, Temperatures:(0.19, 952.31)\n",
            "Old & New Losses 2147.9196548461914 2150.2864360809326 Probab: tensor(0.9975, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 166, LR: 62.5000, Train Loss: 2.1841, Train Accuracy: 18.10%, Temperatures:(0.19, 942.78)\n",
            "Old & New Losses 2149.4359970092773 2155.972719192505 Probab: tensor(0.9931, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 167, LR: 62.5000, Train Loss: 2.1500, Train Accuracy: 18.20%, Temperatures:(0.19, 933.36)\n",
            "Old & New Losses 2140.927791595459 2151.191234588623 Probab: tensor(0.9891, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 168, LR: 62.5000, Train Loss: 2.1523, Train Accuracy: 15.70%, Temperatures:(0.18, 924.02)\n",
            "Old & New Losses 2135.6770992279053 2138.3872032165527 Probab: tensor(0.9971, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 169, LR: 62.5000, Train Loss: 2.1505, Train Accuracy: 17.10%, Temperatures:(0.18, 914.78)\n",
            "Old & New Losses 2140.298366546631 2177.919387817383 Probab: tensor(0.9597, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 170, LR: 62.5000, Train Loss: 2.1371, Train Accuracy: 17.50%, Temperatures:(0.18, 905.63)\n",
            "Old & New Losses 2136.1794471740723 2146.4669704437256 Probab: tensor(0.9887, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 171, LR: 62.5000, Train Loss: 2.1743, Train Accuracy: 16.50%, Temperatures:(0.18, 896.58)\n",
            "Old & New Losses 2132.6422691345215 2138.615369796753 Probab: tensor(0.9934, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 172, LR: 62.5000, Train Loss: 2.1503, Train Accuracy: 15.30%, Temperatures:(0.18, 887.61)\n",
            "Old & New Losses 2138.237953186035 2134.383201599121 Probab: tensor(1.0044, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 173, LR: 62.5000, Train Loss: 2.1388, Train Accuracy: 22.40%, Temperatures:(0.18, 878.74)\n",
            "Old & New Losses 2135.204076766968 2133.6300373077393 Probab: tensor(1.0018, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 174, LR: 62.5000, Train Loss: 2.1335, Train Accuracy: 23.20%, Temperatures:(0.17, 869.95)\n",
            "Old & New Losses 2128.51881980896 2133.8069438934326 Probab: tensor(0.9939, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 175, LR: 62.5000, Train Loss: 2.1374, Train Accuracy: 20.90%, Temperatures:(0.17, 861.25)\n",
            "Old & New Losses 2130.2926540374756 2130.0437450408936 Probab: tensor(1.0003, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 176, LR: 62.5000, Train Loss: 2.1331, Train Accuracy: 26.00%, Temperatures:(0.17, 852.64)\n",
            "Old & New Losses 2125.2543926239014 2121.3746070861816 Probab: tensor(1.0046, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 177, LR: 62.5000, Train Loss: 2.1309, Train Accuracy: 24.50%, Temperatures:(0.17, 844.11)\n",
            "Old & New Losses 2120.823860168457 2113.36088180542 Probab: tensor(1.0089, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 178, LR: 62.5000, Train Loss: 2.1285, Train Accuracy: 26.70%, Temperatures:(0.17, 835.67)\n",
            "Old & New Losses 2119.136095046997 2113.942861557007 Probab: tensor(1.0062, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 179, LR: 62.5000, Train Loss: 2.1163, Train Accuracy: 27.20%, Temperatures:(0.17, 827.31)\n",
            "Old & New Losses 2121.4184761047363 2119.3854808807373 Probab: tensor(1.0025, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 180, LR: 62.5000, Train Loss: 2.1167, Train Accuracy: 28.50%, Temperatures:(0.16, 819.04)\n",
            "Old & New Losses 2109.506368637085 2112.887144088745 Probab: tensor(0.9959, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 181, LR: 62.5000, Train Loss: 2.1171, Train Accuracy: 27.70%, Temperatures:(0.16, 810.85)\n",
            "Old & New Losses 2191.526174545288 2203.4316062927246 Probab: tensor(0.9854, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 182, LR: 62.5000, Train Loss: 2.1175, Train Accuracy: 27.20%, Temperatures:(0.16, 802.74)\n",
            "Old & New Losses 2111.4227771759033 2115.7522201538086 Probab: tensor(0.9946, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 183, LR: 62.5000, Train Loss: 2.2007, Train Accuracy: 22.90%, Temperatures:(0.16, 794.71)\n",
            "Old & New Losses 2106.9540977478027 2108.13570022583 Probab: tensor(0.9985, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 184, LR: 62.5000, Train Loss: 2.1127, Train Accuracy: 27.70%, Temperatures:(0.16, 786.77)\n",
            "Old & New Losses 2099.9765396118164 2097.37491607666 Probab: tensor(1.0033, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 185, LR: 62.5000, Train Loss: 2.1058, Train Accuracy: 28.00%, Temperatures:(0.16, 778.90)\n",
            "Old & New Losses 2095.106601715088 2101.0143756866455 Probab: tensor(0.9924, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 186, LR: 62.5000, Train Loss: 2.0991, Train Accuracy: 26.90%, Temperatures:(0.15, 771.11)\n",
            "Old & New Losses 2097.0206260681152 2101.5965938568115 Probab: tensor(0.9941, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 187, LR: 62.5000, Train Loss: 2.0992, Train Accuracy: 28.00%, Temperatures:(0.15, 763.40)\n",
            "Old & New Losses 2097.1195697784424 2099.4856357574463 Probab: tensor(0.9969, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 188, LR: 62.5000, Train Loss: 2.0962, Train Accuracy: 26.60%, Temperatures:(0.15, 755.76)\n",
            "Old & New Losses 2092.481851577759 2091.0210609436035 Probab: tensor(1.0019, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 189, LR: 62.5000, Train Loss: 2.0972, Train Accuracy: 26.50%, Temperatures:(0.15, 748.21)\n",
            "Old & New Losses 2090.2302265167236 2089.7576808929443 Probab: tensor(1.0006, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 190, LR: 62.5000, Train Loss: 2.0863, Train Accuracy: 28.50%, Temperatures:(0.15, 740.72)\n",
            "Old & New Losses 2083.111047744751 2080.7433128356934 Probab: tensor(1.0032, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 191, LR: 62.5000, Train Loss: 2.0879, Train Accuracy: 28.40%, Temperatures:(0.15, 733.32)\n",
            "Old & New Losses 2080.1291465759277 2086.714267730713 Probab: tensor(0.9911, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 192, LR: 62.5000, Train Loss: 2.0789, Train Accuracy: 30.10%, Temperatures:(0.15, 725.98)\n",
            "Old & New Losses 2082.7510356903076 2078.559398651123 Probab: tensor(1.0058, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 193, LR: 62.5000, Train Loss: 2.0805, Train Accuracy: 28.90%, Temperatures:(0.14, 718.72)\n",
            "Old & New Losses 2084.096908569336 2077.1546363830566 Probab: tensor(1.0097, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 194, LR: 62.5000, Train Loss: 2.0766, Train Accuracy: 28.10%, Temperatures:(0.14, 711.54)\n",
            "Old & New Losses 2070.345640182495 2076.0269165039062 Probab: tensor(0.9920, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 195, LR: 62.5000, Train Loss: 2.0809, Train Accuracy: 29.60%, Temperatures:(0.14, 704.42)\n",
            "Old & New Losses 2090.7304286956787 2089.1566276550293 Probab: tensor(1.0022, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 196, LR: 62.5000, Train Loss: 2.0714, Train Accuracy: 28.30%, Temperatures:(0.14, 697.38)\n",
            "Old & New Losses 2065.3433799743652 2068.3727264404297 Probab: tensor(0.9957, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 197, LR: 62.5000, Train Loss: 2.0901, Train Accuracy: 27.00%, Temperatures:(0.14, 690.40)\n",
            "Old & New Losses 2064.106225967407 2062.6745223999023 Probab: tensor(1.0021, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 198, LR: 62.5000, Train Loss: 2.0675, Train Accuracy: 27.40%, Temperatures:(0.14, 683.50)\n",
            "Old & New Losses 2061.7306232452393 2060.7311725616455 Probab: tensor(1.0015, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 199, LR: 62.5000, Train Loss: 2.0653, Train Accuracy: 27.20%, Temperatures:(0.14, 676.67)\n",
            "Old & New Losses 2057.689905166626 2058.2292079925537 Probab: tensor(0.9992, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 200, LR: 62.5000, Train Loss: 2.0578, Train Accuracy: 28.10%, Temperatures:(0.13, 669.90)\n",
            "Old & New Losses 2048.619270324707 2056.1742782592773 Probab: tensor(0.9888, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 201, LR: 62.5000, Train Loss: 2.0585, Train Accuracy: 28.00%, Temperatures:(0.13, 663.20)\n",
            "Old & New Losses 2052.2730350494385 2051.1741638183594 Probab: tensor(1.0017, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 202, LR: 31.2500, Train Loss: 2.0569, Train Accuracy: 27.70%, Temperatures:(0.13, 656.57)\n",
            "Old & New Losses 2054.5763969421387 2047.3053455352783 Probab: tensor(1.0111, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 203, LR: 31.2500, Train Loss: 2.0547, Train Accuracy: 28.50%, Temperatures:(0.13, 650.00)\n",
            "Old & New Losses 2045.7873344421387 2049.6647357940674 Probab: tensor(0.9941, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 204, LR: 31.2500, Train Loss: 2.0528, Train Accuracy: 27.10%, Temperatures:(0.13, 643.50)\n",
            "Old & New Losses 2044.8698997497559 2045.6359386444092 Probab: tensor(0.9988, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 205, LR: 31.2500, Train Loss: 2.0474, Train Accuracy: 28.40%, Temperatures:(0.13, 637.07)\n",
            "Old & New Losses 2042.5009727478027 2045.567512512207 Probab: tensor(0.9952, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 206, LR: 31.2500, Train Loss: 2.0429, Train Accuracy: 27.70%, Temperatures:(0.13, 630.70)\n",
            "Old & New Losses 2041.4791107177734 2046.3109016418457 Probab: tensor(0.9924, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 207, LR: 31.2500, Train Loss: 2.0442, Train Accuracy: 26.60%, Temperatures:(0.12, 624.39)\n",
            "Old & New Losses 2040.3435230255127 2041.050672531128 Probab: tensor(0.9989, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 208, LR: 31.2500, Train Loss: 2.0457, Train Accuracy: 26.50%, Temperatures:(0.12, 618.15)\n",
            "Old & New Losses 2048.4533309936523 2043.6115264892578 Probab: tensor(1.0079, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 209, LR: 31.2500, Train Loss: 2.0432, Train Accuracy: 27.70%, Temperatures:(0.12, 611.96)\n",
            "Old & New Losses 2041.2249565124512 2038.8517379760742 Probab: tensor(1.0039, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 210, LR: 31.2500, Train Loss: 2.0461, Train Accuracy: 28.10%, Temperatures:(0.12, 605.84)\n",
            "Old & New Losses 2043.154001235962 2043.182134628296 Probab: tensor(1.0000, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 211, LR: 31.2500, Train Loss: 2.0446, Train Accuracy: 27.40%, Temperatures:(0.12, 599.79)\n",
            "Old & New Losses 2034.3847274780273 2041.70560836792 Probab: tensor(0.9879, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 212, LR: 31.2500, Train Loss: 2.0397, Train Accuracy: 28.00%, Temperatures:(0.12, 593.79)\n",
            "Old & New Losses 2041.036605834961 2065.037250518799 Probab: tensor(0.9604, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 213, LR: 31.2500, Train Loss: 2.0443, Train Accuracy: 25.40%, Temperatures:(0.12, 587.85)\n",
            "Old & New Losses 2049.4298934936523 2055.7353496551514 Probab: tensor(0.9893, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 214, LR: 31.2500, Train Loss: 2.0642, Train Accuracy: 25.00%, Temperatures:(0.12, 581.97)\n",
            "Old & New Losses 2043.0645942687988 2049.7403144836426 Probab: tensor(0.9886, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 215, LR: 31.2500, Train Loss: 2.0596, Train Accuracy: 26.80%, Temperatures:(0.12, 576.15)\n",
            "Old & New Losses 2041.123867034912 2043.3595180511475 Probab: tensor(0.9961, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 216, LR: 31.2500, Train Loss: 2.0450, Train Accuracy: 28.90%, Temperatures:(0.11, 570.39)\n",
            "Old & New Losses 2038.71488571167 2039.6263599395752 Probab: tensor(0.9984, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 217, LR: 31.2500, Train Loss: 2.0403, Train Accuracy: 27.40%, Temperatures:(0.11, 564.69)\n",
            "Old & New Losses 2041.700839996338 2035.3279113769531 Probab: tensor(1.0113, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 218, LR: 31.2500, Train Loss: 2.0348, Train Accuracy: 29.20%, Temperatures:(0.11, 559.04)\n",
            "Old & New Losses 2031.8853855133057 2036.0090732574463 Probab: tensor(0.9927, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 219, LR: 31.2500, Train Loss: 2.0378, Train Accuracy: 30.30%, Temperatures:(0.11, 553.45)\n",
            "Old & New Losses 2027.9500484466553 2033.3609580993652 Probab: tensor(0.9903, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 220, LR: 31.2500, Train Loss: 2.0388, Train Accuracy: 30.00%, Temperatures:(0.11, 547.91)\n",
            "Old & New Losses 2026.7682075500488 2033.1525802612305 Probab: tensor(0.9884, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 221, LR: 31.2500, Train Loss: 2.0318, Train Accuracy: 29.40%, Temperatures:(0.11, 542.44)\n",
            "Old & New Losses 2034.0092182159424 2032.6063632965088 Probab: tensor(1.0026, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 222, LR: 31.2500, Train Loss: 2.0336, Train Accuracy: 29.00%, Temperatures:(0.11, 537.01)\n",
            "Old & New Losses 2027.3959636688232 2027.9459953308105 Probab: tensor(0.9990, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 223, LR: 31.2500, Train Loss: 2.0316, Train Accuracy: 29.60%, Temperatures:(0.11, 531.64)\n",
            "Old & New Losses 2026.0283946990967 2026.9722938537598 Probab: tensor(0.9982, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 224, LR: 31.2500, Train Loss: 2.0309, Train Accuracy: 27.00%, Temperatures:(0.11, 526.32)\n",
            "Old & New Losses 2023.3111381530762 2027.5862216949463 Probab: tensor(0.9919, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 225, LR: 31.2500, Train Loss: 2.0290, Train Accuracy: 28.70%, Temperatures:(0.10, 521.06)\n",
            "Old & New Losses 2030.2679538726807 2031.2542915344238 Probab: tensor(0.9981, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 226, LR: 31.2500, Train Loss: 2.0328, Train Accuracy: 29.20%, Temperatures:(0.10, 515.85)\n",
            "Old & New Losses 2025.2156257629395 2030.6987762451172 Probab: tensor(0.9894, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 227, LR: 31.2500, Train Loss: 2.0361, Train Accuracy: 30.30%, Temperatures:(0.10, 510.69)\n",
            "Old & New Losses 2026.8192291259766 2024.9114036560059 Probab: tensor(1.0037, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 228, LR: 31.2500, Train Loss: 2.0340, Train Accuracy: 28.80%, Temperatures:(0.10, 505.59)\n",
            "Old & New Losses 2027.9240608215332 2025.4793167114258 Probab: tensor(1.0048, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 229, LR: 31.2500, Train Loss: 2.0294, Train Accuracy: 27.60%, Temperatures:(0.10, 500.53)\n",
            "Old & New Losses 2026.0932445526123 2025.4583358764648 Probab: tensor(1.0013, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 230, LR: 31.2500, Train Loss: 2.0312, Train Accuracy: 30.40%, Temperatures:(0.10, 495.52)\n",
            "Old & New Losses 2028.6290645599365 2035.5663299560547 Probab: tensor(0.9861, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 231, LR: 31.2500, Train Loss: 2.0280, Train Accuracy: 27.90%, Temperatures:(0.10, 490.57)\n",
            "Old & New Losses 2029.8843383789062 2021.8839645385742 Probab: tensor(1.0164, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 232, LR: 31.2500, Train Loss: 2.0361, Train Accuracy: 30.40%, Temperatures:(0.10, 485.66)\n",
            "Old & New Losses 2018.6622142791748 2019.9596881866455 Probab: tensor(0.9973, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 233, LR: 31.2500, Train Loss: 2.0180, Train Accuracy: 28.50%, Temperatures:(0.10, 480.81)\n",
            "Old & New Losses 2020.2107429504395 2034.7702503204346 Probab: tensor(0.9702, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 234, LR: 31.2500, Train Loss: 2.0201, Train Accuracy: 30.20%, Temperatures:(0.10, 476.00)\n",
            "Old & New Losses 2020.3931331634521 2023.8208770751953 Probab: tensor(0.9928, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 235, LR: 31.2500, Train Loss: 2.0323, Train Accuracy: 29.60%, Temperatures:(0.09, 471.24)\n",
            "Old & New Losses 2020.0574398040771 2025.608777999878 Probab: tensor(0.9883, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 236, LR: 31.2500, Train Loss: 2.0298, Train Accuracy: 29.30%, Temperatures:(0.09, 466.53)\n",
            "Old & New Losses 2015.383005142212 2018.4967517852783 Probab: tensor(0.9933, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 237, LR: 31.2500, Train Loss: 2.0228, Train Accuracy: 30.80%, Temperatures:(0.09, 461.86)\n",
            "Old & New Losses 2015.024185180664 2020.3168392181396 Probab: tensor(0.9886, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 238, LR: 31.2500, Train Loss: 2.0180, Train Accuracy: 30.50%, Temperatures:(0.09, 457.24)\n",
            "Old & New Losses 2015.1867866516113 2013.822317123413 Probab: tensor(1.0030, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 239, LR: 31.2500, Train Loss: 2.0214, Train Accuracy: 30.60%, Temperatures:(0.09, 452.67)\n",
            "Old & New Losses 2019.6428298950195 2031.6593647003174 Probab: tensor(0.9738, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 240, LR: 31.2500, Train Loss: 2.0208, Train Accuracy: 32.70%, Temperatures:(0.09, 448.14)\n",
            "Old & New Losses 2017.4808502197266 2018.6350345611572 Probab: tensor(0.9974, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 241, LR: 31.2500, Train Loss: 2.0353, Train Accuracy: 31.30%, Temperatures:(0.09, 443.66)\n",
            "Old & New Losses 2010.8718872070312 2012.1042728424072 Probab: tensor(0.9972, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 242, LR: 31.2500, Train Loss: 2.0131, Train Accuracy: 32.20%, Temperatures:(0.09, 439.23)\n",
            "Old & New Losses 2013.5457515716553 2014.1518115997314 Probab: tensor(0.9986, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 243, LR: 31.2500, Train Loss: 2.0161, Train Accuracy: 30.80%, Temperatures:(0.09, 434.83)\n",
            "Old & New Losses 2011.7077827453613 2011.4245414733887 Probab: tensor(1.0007, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 244, LR: 31.2500, Train Loss: 2.0132, Train Accuracy: 31.30%, Temperatures:(0.09, 430.48)\n",
            "Old & New Losses 2011.1045837402344 2014.1422748565674 Probab: tensor(0.9930, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 245, LR: 31.2500, Train Loss: 2.0142, Train Accuracy: 32.50%, Temperatures:(0.09, 426.18)\n",
            "Old & New Losses 2014.662265777588 2012.129306793213 Probab: tensor(1.0060, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 246, LR: 31.2500, Train Loss: 2.0133, Train Accuracy: 32.20%, Temperatures:(0.08, 421.92)\n",
            "Old & New Losses 2009.9308490753174 2015.2103900909424 Probab: tensor(0.9876, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 247, LR: 31.2500, Train Loss: 2.0150, Train Accuracy: 32.30%, Temperatures:(0.08, 417.70)\n",
            "Old & New Losses 2009.559154510498 2015.143871307373 Probab: tensor(0.9867, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 248, LR: 31.2500, Train Loss: 2.0159, Train Accuracy: 33.40%, Temperatures:(0.08, 413.52)\n",
            "Old & New Losses 2010.2980136871338 2011.8637084960938 Probab: tensor(0.9962, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 249, LR: 31.2500, Train Loss: 2.0119, Train Accuracy: 32.50%, Temperatures:(0.08, 409.39)\n",
            "Old & New Losses 2010.9546184539795 2008.6641311645508 Probab: tensor(1.0056, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 250, LR: 31.2500, Train Loss: 2.0114, Train Accuracy: 32.60%, Temperatures:(0.08, 405.29)\n",
            "Old & New Losses 2005.979061126709 2014.34326171875 Probab: tensor(0.9796, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 251, LR: 31.2500, Train Loss: 2.0079, Train Accuracy: 32.80%, Temperatures:(0.08, 401.24)\n",
            "Old & New Losses 2010.1404190063477 2014.5893096923828 Probab: tensor(0.9890, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 252, LR: 15.6250, Train Loss: 2.0118, Train Accuracy: 31.00%, Temperatures:(0.08, 397.23)\n",
            "Old & New Losses 2009.7601413726807 2016.2017345428467 Probab: tensor(0.9839, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 253, LR: 15.6250, Train Loss: 2.0117, Train Accuracy: 33.30%, Temperatures:(0.08, 393.25)\n",
            "Old & New Losses 2014.3351554870605 2017.8096294403076 Probab: tensor(0.9912, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 254, LR: 15.6250, Train Loss: 2.0204, Train Accuracy: 35.40%, Temperatures:(0.08, 389.32)\n",
            "Old & New Losses 2017.5671577453613 2015.1219367980957 Probab: tensor(1.0063, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 255, LR: 15.6250, Train Loss: 2.0156, Train Accuracy: 33.20%, Temperatures:(0.08, 385.43)\n",
            "Old & New Losses 2013.8449668884277 2014.7171020507812 Probab: tensor(0.9977, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 256, LR: 15.6250, Train Loss: 2.0180, Train Accuracy: 33.60%, Temperatures:(0.08, 381.57)\n",
            "Old & New Losses 2015.6397819519043 2011.7192268371582 Probab: tensor(1.0103, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 257, LR: 15.6250, Train Loss: 2.0148, Train Accuracy: 31.20%, Temperatures:(0.08, 377.76)\n",
            "Old & New Losses 2011.0926628112793 2021.4297771453857 Probab: tensor(0.9730, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 258, LR: 15.6250, Train Loss: 2.0172, Train Accuracy: 32.60%, Temperatures:(0.07, 373.98)\n",
            "Old & New Losses 2013.9048099517822 2010.669231414795 Probab: tensor(1.0087, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 259, LR: 15.6250, Train Loss: 2.0192, Train Accuracy: 30.80%, Temperatures:(0.07, 370.24)\n",
            "Old & New Losses 2019.9227333068848 2020.4181671142578 Probab: tensor(0.9987, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 260, LR: 15.6250, Train Loss: 2.0065, Train Accuracy: 30.00%, Temperatures:(0.07, 366.54)\n",
            "Old & New Losses 2009.0811252593994 2015.347957611084 Probab: tensor(0.9830, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 261, LR: 15.6250, Train Loss: 2.0214, Train Accuracy: 28.10%, Temperatures:(0.07, 362.87)\n",
            "Old & New Losses 2012.1352672576904 2015.043020248413 Probab: tensor(0.9920, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 262, LR: 15.6250, Train Loss: 2.0121, Train Accuracy: 31.60%, Temperatures:(0.07, 359.25)\n",
            "Old & New Losses 2014.056921005249 2015.2311325073242 Probab: tensor(0.9967, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 263, LR: 15.6250, Train Loss: 2.0169, Train Accuracy: 31.20%, Temperatures:(0.07, 355.65)\n",
            "Old & New Losses 2010.9362602233887 2012.1290683746338 Probab: tensor(0.9967, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 264, LR: 15.6250, Train Loss: 2.0144, Train Accuracy: 32.10%, Temperatures:(0.07, 352.10)\n",
            "Old & New Losses 2013.6606693267822 2015.8860683441162 Probab: tensor(0.9937, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 265, LR: 15.6250, Train Loss: 2.0130, Train Accuracy: 31.50%, Temperatures:(0.07, 348.58)\n",
            "Old & New Losses 2009.3848705291748 2012.20703125 Probab: tensor(0.9919, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 266, LR: 15.6250, Train Loss: 2.0176, Train Accuracy: 29.40%, Temperatures:(0.07, 345.09)\n",
            "Old & New Losses 2015.9568786621094 2008.6050033569336 Probab: tensor(1.0215, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 267, LR: 15.6250, Train Loss: 2.0097, Train Accuracy: 30.90%, Temperatures:(0.07, 341.64)\n",
            "Old & New Losses 2015.418529510498 2013.0369663238525 Probab: tensor(1.0070, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 268, LR: 15.6250, Train Loss: 2.0117, Train Accuracy: 29.80%, Temperatures:(0.07, 338.22)\n",
            "Old & New Losses 2011.1160278320312 2011.3720893859863 Probab: tensor(0.9992, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 269, LR: 15.6250, Train Loss: 2.0148, Train Accuracy: 31.20%, Temperatures:(0.07, 334.84)\n",
            "Old & New Losses 2015.3412818908691 2020.6594467163086 Probab: tensor(0.9842, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 270, LR: 15.6250, Train Loss: 2.0127, Train Accuracy: 31.60%, Temperatures:(0.07, 331.49)\n",
            "Old & New Losses 2009.9906921386719 2013.6911869049072 Probab: tensor(0.9889, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 271, LR: 15.6250, Train Loss: 2.0219, Train Accuracy: 31.40%, Temperatures:(0.07, 328.18)\n",
            "Old & New Losses 2011.9755268096924 2009.760856628418 Probab: tensor(1.0068, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 272, LR: 15.6250, Train Loss: 2.0118, Train Accuracy: 27.80%, Temperatures:(0.06, 324.89)\n",
            "Old & New Losses 2010.3249549865723 2005.4621696472168 Probab: tensor(1.0151, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 273, LR: 15.6250, Train Loss: 2.0114, Train Accuracy: 28.40%, Temperatures:(0.06, 321.65)\n",
            "Old & New Losses 2005.2435398101807 2007.3661804199219 Probab: tensor(0.9934, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 274, LR: 15.6250, Train Loss: 2.0069, Train Accuracy: 28.90%, Temperatures:(0.06, 318.43)\n",
            "Old & New Losses 2003.8459300994873 2011.380910873413 Probab: tensor(0.9766, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 275, LR: 15.6250, Train Loss: 2.0111, Train Accuracy: 29.30%, Temperatures:(0.06, 315.25)\n",
            "Old & New Losses 2013.6854648590088 2013.6332511901855 Probab: tensor(1.0002, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 276, LR: 15.6250, Train Loss: 2.0124, Train Accuracy: 27.50%, Temperatures:(0.06, 312.09)\n",
            "Old & New Losses 2008.4891319274902 2005.7036876678467 Probab: tensor(1.0090, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 277, LR: 15.6250, Train Loss: 2.0169, Train Accuracy: 29.40%, Temperatures:(0.06, 308.97)\n",
            "Old & New Losses 2005.7930946350098 2008.5701942443848 Probab: tensor(0.9911, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 278, LR: 15.6250, Train Loss: 2.0062, Train Accuracy: 30.00%, Temperatures:(0.06, 305.88)\n",
            "Old & New Losses 2006.333351135254 2035.2332592010498 Probab: tensor(0.9098, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 279, LR: 15.6250, Train Loss: 2.0100, Train Accuracy: 29.30%, Temperatures:(0.06, 302.82)\n",
            "Old & New Losses 2017.9364681243896 2022.5343704223633 Probab: tensor(0.9849, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 280, LR: 15.6250, Train Loss: 2.0358, Train Accuracy: 27.50%, Temperatures:(0.06, 299.80)\n",
            "Old & New Losses 2011.505365371704 2012.5715732574463 Probab: tensor(0.9964, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 281, LR: 15.6250, Train Loss: 2.0234, Train Accuracy: 30.90%, Temperatures:(0.06, 296.80)\n",
            "Old & New Losses 2006.6545009613037 2009.5443725585938 Probab: tensor(0.9903, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 282, LR: 15.6250, Train Loss: 2.0134, Train Accuracy: 31.10%, Temperatures:(0.06, 293.83)\n",
            "Old & New Losses 2009.0773105621338 2004.4395923614502 Probab: tensor(1.0159, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 283, LR: 15.6250, Train Loss: 2.0144, Train Accuracy: 29.80%, Temperatures:(0.06, 290.89)\n",
            "Old & New Losses 2003.7827491760254 2011.8212699890137 Probab: tensor(0.9727, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 284, LR: 15.6250, Train Loss: 2.0082, Train Accuracy: 30.10%, Temperatures:(0.06, 287.98)\n",
            "Old & New Losses 2003.8220882415771 2025.5370140075684 Probab: tensor(0.9274, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 285, LR: 15.6250, Train Loss: 2.0092, Train Accuracy: 28.30%, Temperatures:(0.06, 285.10)\n",
            "Old & New Losses 2006.8154335021973 2010.770320892334 Probab: tensor(0.9862, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 286, LR: 15.6250, Train Loss: 2.0193, Train Accuracy: 27.90%, Temperatures:(0.06, 282.25)\n",
            "Old & New Losses 2010.206699371338 2007.6854228973389 Probab: tensor(1.0090, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 287, LR: 15.6250, Train Loss: 2.0104, Train Accuracy: 29.30%, Temperatures:(0.06, 279.43)\n",
            "Old & New Losses 2011.667013168335 2009.7763538360596 Probab: tensor(1.0068, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 288, LR: 15.6250, Train Loss: 2.0102, Train Accuracy: 30.10%, Temperatures:(0.06, 276.63)\n",
            "Old & New Losses 2007.6563358306885 2013.0105018615723 Probab: tensor(0.9808, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 289, LR: 15.6250, Train Loss: 2.0117, Train Accuracy: 29.10%, Temperatures:(0.05, 273.87)\n",
            "Old & New Losses 2010.3273391723633 2011.216402053833 Probab: tensor(0.9968, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 290, LR: 15.6250, Train Loss: 2.0107, Train Accuracy: 29.40%, Temperatures:(0.05, 271.13)\n",
            "Old & New Losses 2009.4623565673828 2008.1543922424316 Probab: tensor(1.0048, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 291, LR: 15.6250, Train Loss: 2.0135, Train Accuracy: 29.40%, Temperatures:(0.05, 268.42)\n",
            "Old & New Losses 2015.3582096099854 2010.7285976409912 Probab: tensor(1.0174, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 292, LR: 15.6250, Train Loss: 2.0074, Train Accuracy: 29.90%, Temperatures:(0.05, 265.73)\n",
            "Old & New Losses 2008.9006423950195 2005.5122375488281 Probab: tensor(1.0128, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 293, LR: 15.6250, Train Loss: 2.0090, Train Accuracy: 28.90%, Temperatures:(0.05, 263.08)\n",
            "Old & New Losses 2018.338680267334 2016.8769359588623 Probab: tensor(1.0056, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 294, LR: 15.6250, Train Loss: 2.0048, Train Accuracy: 29.60%, Temperatures:(0.05, 260.45)\n",
            "Old & New Losses 2004.807472229004 2007.4481964111328 Probab: tensor(0.9899, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 295, LR: 15.6250, Train Loss: 2.0131, Train Accuracy: 30.30%, Temperatures:(0.05, 257.84)\n",
            "Old & New Losses 2010.972499847412 2007.154941558838 Probab: tensor(1.0149, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 296, LR: 15.6250, Train Loss: 2.0104, Train Accuracy: 29.10%, Temperatures:(0.05, 255.26)\n",
            "Old & New Losses 2004.894495010376 2007.004976272583 Probab: tensor(0.9918, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 297, LR: 15.6250, Train Loss: 2.0119, Train Accuracy: 29.90%, Temperatures:(0.05, 252.71)\n",
            "Old & New Losses 2010.8091831207275 2015.132188796997 Probab: tensor(0.9830, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 298, LR: 15.6250, Train Loss: 2.0054, Train Accuracy: 31.20%, Temperatures:(0.05, 250.18)\n",
            "Old & New Losses 2000.8931159973145 2008.3892345428467 Probab: tensor(0.9705, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 299, LR: 15.6250, Train Loss: 2.0152, Train Accuracy: 30.20%, Temperatures:(0.05, 247.68)\n",
            "Old & New Losses 2013.2222175598145 2029.9832820892334 Probab: tensor(0.9346, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 300, LR: 15.6250, Train Loss: 2.0042, Train Accuracy: 30.60%, Temperatures:(0.05, 245.20)\n",
            "Old & New Losses 2003.8676261901855 2001.2125968933105 Probab: tensor(1.0109, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 301, LR: 15.6250, Train Loss: 2.0291, Train Accuracy: 29.70%, Temperatures:(0.05, 242.75)\n",
            "Old & New Losses 1999.4747638702393 2002.5160312652588 Probab: tensor(0.9875, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 302, LR: 7.8125, Train Loss: 2.0014, Train Accuracy: 31.80%, Temperatures:(0.05, 240.32)\n",
            "Old & New Losses 2001.9676685333252 2001.1255741119385 Probab: tensor(1.0035, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 303, LR: 7.8125, Train Loss: 1.9993, Train Accuracy: 34.00%, Temperatures:(0.05, 237.92)\n",
            "Old & New Losses 2003.6613941192627 2022.2437381744385 Probab: tensor(0.9249, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 304, LR: 7.8125, Train Loss: 2.0043, Train Accuracy: 36.40%, Temperatures:(0.05, 235.54)\n",
            "Old & New Losses 2004.9238204956055 2005.2497386932373 Probab: tensor(0.9986, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 305, LR: 7.8125, Train Loss: 2.0248, Train Accuracy: 34.30%, Temperatures:(0.05, 233.19)\n",
            "Old & New Losses 2002.3128986358643 1998.8480806350708 Probab: tensor(1.0150, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 306, LR: 7.8125, Train Loss: 2.0045, Train Accuracy: 37.30%, Temperatures:(0.05, 230.86)\n",
            "Old & New Losses 2002.2952556610107 2002.9258728027344 Probab: tensor(0.9973, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 307, LR: 7.8125, Train Loss: 2.0006, Train Accuracy: 34.30%, Temperatures:(0.05, 228.55)\n",
            "Old & New Losses 1998.4205961227417 2002.289056777954 Probab: tensor(0.9832, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 308, LR: 7.8125, Train Loss: 2.0004, Train Accuracy: 36.70%, Temperatures:(0.05, 226.26)\n",
            "Old & New Losses 2004.0020942687988 2004.2366981506348 Probab: tensor(0.9990, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 309, LR: 7.8125, Train Loss: 2.0002, Train Accuracy: 35.20%, Temperatures:(0.04, 224.00)\n",
            "Old & New Losses 2002.3815631866455 2002.439260482788 Probab: tensor(0.9997, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 310, LR: 7.8125, Train Loss: 2.0028, Train Accuracy: 32.70%, Temperatures:(0.04, 221.76)\n",
            "Old & New Losses 1999.4518756866455 2005.2590370178223 Probab: tensor(0.9742, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 311, LR: 7.8125, Train Loss: 2.0013, Train Accuracy: 31.60%, Temperatures:(0.04, 219.54)\n",
            "Old & New Losses 2002.702236175537 2006.917953491211 Probab: tensor(0.9810, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 312, LR: 7.8125, Train Loss: 2.0024, Train Accuracy: 33.10%, Temperatures:(0.04, 217.35)\n",
            "Old & New Losses 2001.2130737304688 2006.772518157959 Probab: tensor(0.9747, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 313, LR: 7.8125, Train Loss: 2.0077, Train Accuracy: 32.80%, Temperatures:(0.04, 215.17)\n",
            "Old & New Losses 2002.2587776184082 2008.5046291351318 Probab: tensor(0.9714, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 314, LR: 7.8125, Train Loss: 2.0035, Train Accuracy: 34.00%, Temperatures:(0.04, 213.02)\n",
            "Old & New Losses 2002.2602081298828 2007.8604221343994 Probab: tensor(0.9741, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 315, LR: 7.8125, Train Loss: 2.0067, Train Accuracy: 32.40%, Temperatures:(0.04, 210.89)\n",
            "Old & New Losses 2012.279748916626 2011.1877918243408 Probab: tensor(1.0052, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 316, LR: 7.8125, Train Loss: 2.0118, Train Accuracy: 34.10%, Temperatures:(0.04, 208.78)\n",
            "Old & New Losses 2005.7637691497803 2005.479335784912 Probab: tensor(1.0014, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 317, LR: 7.8125, Train Loss: 2.0088, Train Accuracy: 35.00%, Temperatures:(0.04, 206.69)\n",
            "Old & New Losses 2005.601406097412 2004.9200057983398 Probab: tensor(1.0033, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 318, LR: 7.8125, Train Loss: 2.0042, Train Accuracy: 35.80%, Temperatures:(0.04, 204.63)\n",
            "Old & New Losses 2006.2906742095947 2007.3931217193604 Probab: tensor(0.9946, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 319, LR: 7.8125, Train Loss: 2.0067, Train Accuracy: 34.70%, Temperatures:(0.04, 202.58)\n",
            "Old & New Losses 2006.3140392303467 1999.4657039642334 Probab: tensor(1.0344, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 320, LR: 7.8125, Train Loss: 2.0076, Train Accuracy: 36.00%, Temperatures:(0.04, 200.55)\n",
            "Old & New Losses 2008.1536769866943 2004.9843788146973 Probab: tensor(1.0159, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 321, LR: 7.8125, Train Loss: 2.0072, Train Accuracy: 35.90%, Temperatures:(0.04, 198.55)\n",
            "Old & New Losses 2009.3252658843994 2013.8473510742188 Probab: tensor(0.9775, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 322, LR: 7.8125, Train Loss: 2.0051, Train Accuracy: 34.50%, Temperatures:(0.04, 196.56)\n",
            "Old & New Losses 2006.1640739440918 2007.591962814331 Probab: tensor(0.9928, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 323, LR: 7.8125, Train Loss: 2.0143, Train Accuracy: 35.10%, Temperatures:(0.04, 194.60)\n",
            "Old & New Losses 2010.2882385253906 2015.1581764221191 Probab: tensor(0.9753, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 324, LR: 7.8125, Train Loss: 2.0070, Train Accuracy: 33.10%, Temperatures:(0.04, 192.65)\n",
            "Old & New Losses 2004.9450397491455 2001.6210079193115 Probab: tensor(1.0174, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 325, LR: 7.8125, Train Loss: 2.0048, Train Accuracy: 33.90%, Temperatures:(0.04, 190.73)\n",
            "Old & New Losses 2010.2448463439941 2012.9878520965576 Probab: tensor(0.9857, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 326, LR: 7.8125, Train Loss: 2.0035, Train Accuracy: 34.60%, Temperatures:(0.04, 188.82)\n",
            "Old & New Losses 2009.7684860229492 2008.6615085601807 Probab: tensor(1.0059, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 327, LR: 7.8125, Train Loss: 2.0147, Train Accuracy: 36.10%, Temperatures:(0.04, 186.93)\n",
            "Old & New Losses 2010.8189582824707 2010.7810497283936 Probab: tensor(1.0002, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 328, LR: 7.8125, Train Loss: 2.0055, Train Accuracy: 33.30%, Temperatures:(0.04, 185.06)\n",
            "Old & New Losses 2013.6449337005615 2006.101369857788 Probab: tensor(1.0416, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 329, LR: 7.8125, Train Loss: 2.0112, Train Accuracy: 36.70%, Temperatures:(0.04, 183.21)\n",
            "Old & New Losses 2008.6498260498047 2014.145851135254 Probab: tensor(0.9704, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 330, LR: 7.8125, Train Loss: 2.0138, Train Accuracy: 33.40%, Temperatures:(0.04, 181.38)\n",
            "Old & New Losses 2004.8704147338867 2009.5901489257812 Probab: tensor(0.9743, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 331, LR: 7.8125, Train Loss: 2.0124, Train Accuracy: 33.20%, Temperatures:(0.04, 179.56)\n",
            "Old & New Losses 2012.7756595611572 2006.575584411621 Probab: tensor(1.0351, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 332, LR: 7.8125, Train Loss: 2.0093, Train Accuracy: 32.40%, Temperatures:(0.04, 177.77)\n",
            "Old & New Losses 2014.341115951538 2013.2091045379639 Probab: tensor(1.0064, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 333, LR: 7.8125, Train Loss: 2.0087, Train Accuracy: 35.90%, Temperatures:(0.04, 175.99)\n",
            "Old & New Losses 2010.7619762420654 2008.3787441253662 Probab: tensor(1.0136, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 334, LR: 7.8125, Train Loss: 2.0143, Train Accuracy: 33.30%, Temperatures:(0.03, 174.23)\n",
            "Old & New Losses 2006.850242614746 2014.3640041351318 Probab: tensor(0.9578, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 335, LR: 7.8125, Train Loss: 2.0131, Train Accuracy: 32.80%, Temperatures:(0.03, 172.49)\n",
            "Old & New Losses 2006.93678855896 2010.1110935211182 Probab: tensor(0.9818, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 336, LR: 7.8125, Train Loss: 2.0105, Train Accuracy: 33.90%, Temperatures:(0.03, 170.76)\n",
            "Old & New Losses 2012.3591423034668 2016.432285308838 Probab: tensor(0.9764, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 337, LR: 7.8125, Train Loss: 2.0091, Train Accuracy: 32.90%, Temperatures:(0.03, 169.06)\n",
            "Old & New Losses 2004.4374465942383 2013.0023956298828 Probab: tensor(0.9506, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 338, LR: 7.8125, Train Loss: 2.0199, Train Accuracy: 33.30%, Temperatures:(0.03, 167.37)\n",
            "Old & New Losses 2018.8329219818115 2018.561601638794 Probab: tensor(1.0016, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 339, LR: 7.8125, Train Loss: 2.0103, Train Accuracy: 33.50%, Temperatures:(0.03, 165.69)\n",
            "Old & New Losses 2008.683204650879 2012.1827125549316 Probab: tensor(0.9791, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 340, LR: 7.8125, Train Loss: 2.0204, Train Accuracy: 32.60%, Temperatures:(0.03, 164.03)\n",
            "Old & New Losses 2021.3429927825928 2018.5980796813965 Probab: tensor(1.0169, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 341, LR: 7.8125, Train Loss: 2.0095, Train Accuracy: 34.40%, Temperatures:(0.03, 162.39)\n",
            "Old & New Losses 2012.8743648529053 2015.617847442627 Probab: tensor(0.9832, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 342, LR: 7.8125, Train Loss: 2.0196, Train Accuracy: 31.40%, Temperatures:(0.03, 160.77)\n",
            "Old & New Losses 2018.5348987579346 2026.353359222412 Probab: tensor(0.9525, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 343, LR: 7.8125, Train Loss: 2.0161, Train Accuracy: 34.60%, Temperatures:(0.03, 159.16)\n",
            "Old & New Losses 2013.380527496338 2014.3241882324219 Probab: tensor(0.9941, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 344, LR: 7.8125, Train Loss: 2.0256, Train Accuracy: 32.20%, Temperatures:(0.03, 157.57)\n",
            "Old & New Losses 2025.3303050994873 2019.9096202850342 Probab: tensor(1.0350, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 345, LR: 7.8125, Train Loss: 2.0160, Train Accuracy: 32.50%, Temperatures:(0.03, 156.00)\n",
            "Old & New Losses 2016.8931484222412 2034.8587036132812 Probab: tensor(0.8912, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 346, LR: 7.8125, Train Loss: 2.0169, Train Accuracy: 32.00%, Temperatures:(0.03, 154.44)\n",
            "Old & New Losses 2008.0511569976807 2018.7454223632812 Probab: tensor(0.9331, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 347, LR: 7.8125, Train Loss: 2.0361, Train Accuracy: 31.00%, Temperatures:(0.03, 152.89)\n",
            "Old & New Losses 2022.4168300628662 2022.029161453247 Probab: tensor(1.0025, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 348, LR: 7.8125, Train Loss: 2.0199, Train Accuracy: 33.30%, Temperatures:(0.03, 151.36)\n",
            "Old & New Losses 2024.1305828094482 2014.1899585723877 Probab: tensor(1.0679, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 349, LR: 7.8125, Train Loss: 2.0220, Train Accuracy: 31.20%, Temperatures:(0.03, 149.85)\n",
            "Old & New Losses 2027.8263092041016 2036.841869354248 Probab: tensor(0.9416, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 350, LR: 7.8125, Train Loss: 2.0152, Train Accuracy: 32.50%, Temperatures:(0.03, 148.35)\n",
            "Old & New Losses 2016.3602828979492 2026.569128036499 Probab: tensor(0.9335, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 351, LR: 7.8125, Train Loss: 2.0326, Train Accuracy: 27.80%, Temperatures:(0.03, 146.87)\n",
            "Old & New Losses 2023.085594177246 2026.6435146331787 Probab: tensor(0.9761, device='cuda:0')\n",
            "Epoch 352, LR: 3.9062, Train Loss: 2.0221, Train Accuracy: 28.80%, Temperatures:(0.03, 145.40)\n",
            "Old & New Losses 2020.4997062683105 2022.2570896148682 Probab: tensor(0.9880, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 353, LR: 3.9062, Train Loss: 2.0232, Train Accuracy: 28.20%, Temperatures:(0.03, 143.94)\n",
            "Old & New Losses 2017.6119804382324 2022.1281051635742 Probab: tensor(0.9691, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 354, LR: 3.9062, Train Loss: 2.0236, Train Accuracy: 28.80%, Temperatures:(0.03, 142.50)\n",
            "Old & New Losses 2022.8700637817383 2022.6244926452637 Probab: tensor(1.0017, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 355, LR: 3.9062, Train Loss: 2.0182, Train Accuracy: 28.20%, Temperatures:(0.03, 141.08)\n",
            "Old & New Losses 2013.8235092163086 2015.3834819793701 Probab: tensor(0.9890, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 356, LR: 3.9062, Train Loss: 2.0303, Train Accuracy: 28.90%, Temperatures:(0.03, 139.67)\n",
            "Old & New Losses 2019.9189186096191 2022.3445892333984 Probab: tensor(0.9828, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 357, LR: 3.9062, Train Loss: 2.0185, Train Accuracy: 30.00%, Temperatures:(0.03, 138.27)\n",
            "Old & New Losses 2013.8599872589111 2018.293857574463 Probab: tensor(0.9684, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 358, LR: 3.9062, Train Loss: 2.0214, Train Accuracy: 27.00%, Temperatures:(0.03, 136.89)\n",
            "Old & New Losses 2020.3766822814941 2023.3488082885742 Probab: tensor(0.9785, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 359, LR: 3.9062, Train Loss: 2.0212, Train Accuracy: 29.30%, Temperatures:(0.03, 135.52)\n",
            "Old & New Losses 2016.9217586517334 2019.6623802185059 Probab: tensor(0.9800, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 360, LR: 3.9062, Train Loss: 2.0166, Train Accuracy: 30.30%, Temperatures:(0.03, 134.17)\n",
            "Old & New Losses 2015.5003070831299 2025.1126289367676 Probab: tensor(0.9309, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 361, LR: 3.9062, Train Loss: 2.0212, Train Accuracy: 25.90%, Temperatures:(0.03, 132.82)\n",
            "Old & New Losses 2019.9313163757324 2020.897388458252 Probab: tensor(0.9928, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 362, LR: 3.9062, Train Loss: 2.0271, Train Accuracy: 25.60%, Temperatures:(0.03, 131.50)\n",
            "Old & New Losses 2020.4992294311523 2031.4373970031738 Probab: tensor(0.9202, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 363, LR: 3.9062, Train Loss: 2.0217, Train Accuracy: 27.30%, Temperatures:(0.03, 130.18)\n",
            "Old & New Losses 2020.0366973876953 2018.8870429992676 Probab: tensor(1.0089, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 364, LR: 3.9062, Train Loss: 2.0299, Train Accuracy: 28.00%, Temperatures:(0.03, 128.88)\n",
            "Old & New Losses 2019.0939903259277 2024.45650100708 Probab: tensor(0.9592, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 365, LR: 3.9062, Train Loss: 2.0216, Train Accuracy: 26.20%, Temperatures:(0.03, 127.59)\n",
            "Old & New Losses 2022.7890014648438 2019.2480087280273 Probab: tensor(1.0281, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 366, LR: 3.9062, Train Loss: 2.0282, Train Accuracy: 27.00%, Temperatures:(0.03, 126.31)\n",
            "Old & New Losses 2023.7679481506348 2019.047498703003 Probab: tensor(1.0381, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 367, LR: 3.9062, Train Loss: 2.0248, Train Accuracy: 28.20%, Temperatures:(0.03, 125.05)\n",
            "Old & New Losses 2021.188497543335 2018.9921855926514 Probab: tensor(1.0177, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 368, LR: 3.9062, Train Loss: 2.0225, Train Accuracy: 26.60%, Temperatures:(0.02, 123.80)\n",
            "Old & New Losses 2017.688512802124 2018.9294815063477 Probab: tensor(0.9900, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 369, LR: 3.9062, Train Loss: 2.0183, Train Accuracy: 28.90%, Temperatures:(0.02, 122.56)\n",
            "Old & New Losses 2021.7902660369873 2021.2807655334473 Probab: tensor(1.0042, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 370, LR: 3.9062, Train Loss: 2.0168, Train Accuracy: 29.40%, Temperatures:(0.02, 121.34)\n",
            "Old & New Losses 2020.371437072754 2018.3639526367188 Probab: tensor(1.0167, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 371, LR: 3.9062, Train Loss: 2.0240, Train Accuracy: 31.90%, Temperatures:(0.02, 120.12)\n",
            "Old & New Losses 2020.251989364624 2019.784688949585 Probab: tensor(1.0039, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 372, LR: 3.9062, Train Loss: 2.0196, Train Accuracy: 30.20%, Temperatures:(0.02, 118.92)\n",
            "Old & New Losses 2020.7386016845703 2018.9950466156006 Probab: tensor(1.0148, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 373, LR: 3.9062, Train Loss: 2.0176, Train Accuracy: 32.90%, Temperatures:(0.02, 117.73)\n",
            "Old & New Losses 2019.467830657959 2022.5167274475098 Probab: tensor(0.9744, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 374, LR: 3.9062, Train Loss: 2.0239, Train Accuracy: 32.40%, Temperatures:(0.02, 116.56)\n",
            "Old & New Losses 2019.6948051452637 2020.8590030670166 Probab: tensor(0.9901, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 375, LR: 3.9062, Train Loss: 2.0210, Train Accuracy: 30.50%, Temperatures:(0.02, 115.39)\n",
            "Old & New Losses 2013.709306716919 2016.8871879577637 Probab: tensor(0.9728, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 376, LR: 3.9062, Train Loss: 2.0176, Train Accuracy: 29.80%, Temperatures:(0.02, 114.24)\n",
            "Old & New Losses 2024.3048667907715 2021.5401649475098 Probab: tensor(1.0245, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 377, LR: 3.9062, Train Loss: 2.0168, Train Accuracy: 30.60%, Temperatures:(0.02, 113.09)\n",
            "Old & New Losses 2022.519588470459 2018.9845561981201 Probab: tensor(1.0318, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 378, LR: 3.9062, Train Loss: 2.0178, Train Accuracy: 31.60%, Temperatures:(0.02, 111.96)\n",
            "Old & New Losses 2018.7952518463135 2016.1852836608887 Probab: tensor(1.0236, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 379, LR: 3.9062, Train Loss: 2.0209, Train Accuracy: 28.20%, Temperatures:(0.02, 110.84)\n",
            "Old & New Losses 2019.132375717163 2022.2821235656738 Probab: tensor(0.9720, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 380, LR: 3.9062, Train Loss: 2.0187, Train Accuracy: 29.00%, Temperatures:(0.02, 109.73)\n",
            "Old & New Losses 2017.4434185028076 2021.8675136566162 Probab: tensor(0.9605, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 381, LR: 3.9062, Train Loss: 2.0234, Train Accuracy: 29.20%, Temperatures:(0.02, 108.64)\n",
            "Old & New Losses 2023.493766784668 2022.5591659545898 Probab: tensor(1.0086, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 382, LR: 3.9062, Train Loss: 2.0209, Train Accuracy: 30.30%, Temperatures:(0.02, 107.55)\n",
            "Old & New Losses 2016.5653228759766 2018.4047222137451 Probab: tensor(0.9830, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 383, LR: 3.9062, Train Loss: 2.0216, Train Accuracy: 30.70%, Temperatures:(0.02, 106.48)\n",
            "Old & New Losses 2022.7701663970947 2024.2805480957031 Probab: tensor(0.9859, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 384, LR: 3.9062, Train Loss: 2.0174, Train Accuracy: 32.40%, Temperatures:(0.02, 105.41)\n",
            "Old & New Losses 2021.0278034210205 2024.474859237671 Probab: tensor(0.9678, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 385, LR: 3.9062, Train Loss: 2.0234, Train Accuracy: 32.30%, Temperatures:(0.02, 104.36)\n",
            "Old & New Losses 2021.8477249145508 2025.132417678833 Probab: tensor(0.9690, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 386, LR: 3.9062, Train Loss: 2.0233, Train Accuracy: 32.50%, Temperatures:(0.02, 103.31)\n",
            "Old & New Losses 2021.5988159179688 2025.47025680542 Probab: tensor(0.9632, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 387, LR: 3.9062, Train Loss: 2.0208, Train Accuracy: 34.00%, Temperatures:(0.02, 102.28)\n",
            "Old & New Losses 2025.853157043457 2021.2187767028809 Probab: tensor(1.0464, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 388, LR: 3.9062, Train Loss: 2.0240, Train Accuracy: 31.70%, Temperatures:(0.02, 101.26)\n",
            "Old & New Losses 2028.7649631500244 2039.048194885254 Probab: tensor(0.9034, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 389, LR: 3.9062, Train Loss: 2.0223, Train Accuracy: 32.00%, Temperatures:(0.02, 100.24)\n",
            "Old & New Losses 2022.5880146026611 2025.505542755127 Probab: tensor(0.9713, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 390, LR: 3.9062, Train Loss: 2.0358, Train Accuracy: 29.60%, Temperatures:(0.02, 99.24)\n",
            "Old & New Losses 2023.958683013916 2028.137445449829 Probab: tensor(0.9588, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 391, LR: 3.9062, Train Loss: 2.0253, Train Accuracy: 29.30%, Temperatures:(0.02, 98.25)\n",
            "Old & New Losses 2019.6187496185303 2027.411699295044 Probab: tensor(0.9237, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 392, LR: 3.9062, Train Loss: 2.0242, Train Accuracy: 30.00%, Temperatures:(0.02, 97.27)\n",
            "Old & New Losses 2021.0437774658203 2022.367238998413 Probab: tensor(0.9865, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 393, LR: 3.9062, Train Loss: 2.0269, Train Accuracy: 29.10%, Temperatures:(0.02, 96.29)\n",
            "Old & New Losses 2024.5146751403809 2020.0207233428955 Probab: tensor(1.0478, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 394, LR: 3.9062, Train Loss: 2.0242, Train Accuracy: 30.40%, Temperatures:(0.02, 95.33)\n",
            "Old & New Losses 2026.374101638794 2035.3610515594482 Probab: tensor(0.9100, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 395, LR: 3.9062, Train Loss: 2.0193, Train Accuracy: 32.40%, Temperatures:(0.02, 94.38)\n",
            "Old & New Losses 2032.7777862548828 2033.6899757385254 Probab: tensor(0.9904, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 396, LR: 3.9062, Train Loss: 2.0339, Train Accuracy: 30.60%, Temperatures:(0.02, 93.43)\n",
            "Old & New Losses 2035.3965759277344 2031.432867050171 Probab: tensor(1.0433, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 397, LR: 3.9062, Train Loss: 2.0350, Train Accuracy: 30.00%, Temperatures:(0.02, 92.50)\n",
            "Old & New Losses 2036.8704795837402 2037.7357006072998 Probab: tensor(0.9907, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 398, LR: 3.9062, Train Loss: 2.0365, Train Accuracy: 28.00%, Temperatures:(0.02, 91.58)\n",
            "Old & New Losses 2035.477638244629 2034.421443939209 Probab: tensor(1.0116, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 399, LR: 3.9062, Train Loss: 2.0408, Train Accuracy: 28.50%, Temperatures:(0.02, 90.66)\n",
            "Old & New Losses 2035.2308750152588 2057.7352046966553 Probab: tensor(0.7802, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 400, LR: 3.9062, Train Loss: 2.0375, Train Accuracy: 28.90%, Temperatures:(0.02, 89.75)\n",
            "Old & New Losses 2049.8287677764893 2057.3666095733643 Probab: tensor(0.9194, device='cuda:0')\n",
            "Epoch 401, LR: 3.9062, Train Loss: 2.0566, Train Accuracy: 26.30%, Temperatures:(0.02, 88.86)\n",
            "Old & New Losses 2052.6113510131836 2053.9517402648926 Probab: tensor(0.9850, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 402, LR: 1.9531, Train Loss: 2.0572, Train Accuracy: 24.20%, Temperatures:(0.02, 87.97)\n",
            "Old & New Losses 2051.1960983276367 2060.4162216186523 Probab: tensor(0.9005, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 403, LR: 1.9531, Train Loss: 2.0551, Train Accuracy: 27.50%, Temperatures:(0.02, 87.09)\n",
            "Old & New Losses 2055.850028991699 2041.3787364959717 Probab: tensor(1.1808, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 404, LR: 1.9531, Train Loss: 2.0558, Train Accuracy: 23.10%, Temperatures:(0.02, 86.22)\n",
            "Old & New Losses 2043.9553260803223 2039.851427078247 Probab: tensor(1.0488, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 405, LR: 1.9531, Train Loss: 2.0459, Train Accuracy: 25.70%, Temperatures:(0.02, 85.35)\n",
            "Old & New Losses 2043.3344841003418 2038.1965637207031 Probab: tensor(1.0620, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 406, LR: 1.9531, Train Loss: 2.0393, Train Accuracy: 25.40%, Temperatures:(0.02, 84.50)\n",
            "Old & New Losses 2032.8130722045898 2039.8406982421875 Probab: tensor(0.9202, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 407, LR: 1.9531, Train Loss: 2.0356, Train Accuracy: 26.40%, Temperatures:(0.02, 83.66)\n",
            "Old & New Losses 2038.693904876709 2039.8144721984863 Probab: tensor(0.9867, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 408, LR: 1.9531, Train Loss: 2.0425, Train Accuracy: 26.80%, Temperatures:(0.02, 82.82)\n",
            "Old & New Losses 2033.689260482788 2039.6339893341064 Probab: tensor(0.9307, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 409, LR: 1.9531, Train Loss: 2.0356, Train Accuracy: 26.40%, Temperatures:(0.02, 81.99)\n",
            "Old & New Losses 2033.0471992492676 2040.6618118286133 Probab: tensor(0.9113, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 410, LR: 1.9531, Train Loss: 2.0329, Train Accuracy: 26.30%, Temperatures:(0.02, 81.17)\n",
            "Old & New Losses 2040.3740406036377 2042.0198440551758 Probab: tensor(0.9799, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 411, LR: 1.9531, Train Loss: 2.0403, Train Accuracy: 27.10%, Temperatures:(0.02, 80.36)\n",
            "Old & New Losses 2035.0160598754883 2037.7626419067383 Probab: tensor(0.9664, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 412, LR: 1.9531, Train Loss: 2.0361, Train Accuracy: 27.30%, Temperatures:(0.02, 79.56)\n",
            "Old & New Losses 2035.2754592895508 2039.379596710205 Probab: tensor(0.9497, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 413, LR: 1.9531, Train Loss: 2.0386, Train Accuracy: 25.80%, Temperatures:(0.02, 78.76)\n",
            "Old & New Losses 2032.0103168487549 2040.8191680908203 Probab: tensor(0.8942, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 414, LR: 1.9531, Train Loss: 2.0402, Train Accuracy: 28.00%, Temperatures:(0.02, 77.97)\n",
            "Old & New Losses 2041.2189960479736 2035.1171493530273 Probab: tensor(1.0814, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 415, LR: 1.9531, Train Loss: 2.0349, Train Accuracy: 28.70%, Temperatures:(0.02, 77.19)\n",
            "Old & New Losses 2032.6554775238037 2037.717580795288 Probab: tensor(0.9365, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 416, LR: 1.9531, Train Loss: 2.0412, Train Accuracy: 29.00%, Temperatures:(0.02, 76.42)\n",
            "Old & New Losses 2038.9072895050049 2045.910120010376 Probab: tensor(0.9124, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 417, LR: 1.9531, Train Loss: 2.0399, Train Accuracy: 30.20%, Temperatures:(0.02, 75.66)\n",
            "Old & New Losses 2039.6223068237305 2043.3146953582764 Probab: tensor(0.9524, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 418, LR: 1.9531, Train Loss: 2.0437, Train Accuracy: 29.40%, Temperatures:(0.01, 74.90)\n",
            "Old & New Losses 2044.77858543396 2045.1714992523193 Probab: tensor(0.9948, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 419, LR: 1.9531, Train Loss: 2.0410, Train Accuracy: 30.20%, Temperatures:(0.01, 74.15)\n",
            "Old & New Losses 2049.01123046875 2047.286033630371 Probab: tensor(1.0235, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 420, LR: 1.9531, Train Loss: 2.0470, Train Accuracy: 25.70%, Temperatures:(0.01, 73.41)\n",
            "Old & New Losses 2048.614263534546 2048.9449501037598 Probab: tensor(0.9955, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 421, LR: 1.9531, Train Loss: 2.0466, Train Accuracy: 27.10%, Temperatures:(0.01, 72.68)\n",
            "Old & New Losses 2048.717975616455 2050.5452156066895 Probab: tensor(0.9752, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 422, LR: 1.9531, Train Loss: 2.0474, Train Accuracy: 25.30%, Temperatures:(0.01, 71.95)\n",
            "Old & New Losses 2043.6272621154785 2042.9325103759766 Probab: tensor(1.0097, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 423, LR: 1.9531, Train Loss: 2.0481, Train Accuracy: 28.30%, Temperatures:(0.01, 71.23)\n",
            "Old & New Losses 2042.9935455322266 2047.9578971862793 Probab: tensor(0.9327, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 424, LR: 1.9531, Train Loss: 2.0495, Train Accuracy: 27.10%, Temperatures:(0.01, 70.52)\n",
            "Old & New Losses 2043.6155796051025 2047.0056533813477 Probab: tensor(0.9531, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 425, LR: 1.9531, Train Loss: 2.0445, Train Accuracy: 28.10%, Temperatures:(0.01, 69.81)\n",
            "Old & New Losses 2046.9717979431152 2049.912929534912 Probab: tensor(0.9587, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 426, LR: 1.9531, Train Loss: 2.0497, Train Accuracy: 26.20%, Temperatures:(0.01, 69.11)\n",
            "Old & New Losses 2046.1294651031494 2040.6742095947266 Probab: tensor(1.0821, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 427, LR: 1.9531, Train Loss: 2.0501, Train Accuracy: 26.10%, Temperatures:(0.01, 68.42)\n",
            "Old & New Losses 2046.5025901794434 2044.238805770874 Probab: tensor(1.0336, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 428, LR: 1.9531, Train Loss: 2.0422, Train Accuracy: 27.00%, Temperatures:(0.01, 67.74)\n",
            "Old & New Losses 2042.7682399749756 2046.1690425872803 Probab: tensor(0.9510, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 429, LR: 1.9531, Train Loss: 2.0429, Train Accuracy: 29.50%, Temperatures:(0.01, 67.06)\n",
            "Old & New Losses 2040.6508445739746 2041.3107872009277 Probab: tensor(0.9902, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 430, LR: 1.9531, Train Loss: 2.0486, Train Accuracy: 26.30%, Temperatures:(0.01, 66.39)\n",
            "Old & New Losses 2040.198802947998 2051.036834716797 Probab: tensor(0.8494, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 431, LR: 1.9531, Train Loss: 2.0419, Train Accuracy: 28.30%, Temperatures:(0.01, 65.73)\n",
            "Old & New Losses 2037.9245281219482 2045.2215671539307 Probab: tensor(0.8949, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 432, LR: 1.9531, Train Loss: 2.0454, Train Accuracy: 26.80%, Temperatures:(0.01, 65.07)\n",
            "Old & New Losses 2043.945074081421 2046.9377040863037 Probab: tensor(0.9550, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 433, LR: 1.9531, Train Loss: 2.0429, Train Accuracy: 28.20%, Temperatures:(0.01, 64.42)\n",
            "Old & New Losses 2048.7964153289795 2049.3857860565186 Probab: tensor(0.9909, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 434, LR: 1.9531, Train Loss: 2.0467, Train Accuracy: 27.40%, Temperatures:(0.01, 63.77)\n",
            "Old & New Losses 2042.1302318572998 2044.9926853179932 Probab: tensor(0.9561, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 435, LR: 1.9531, Train Loss: 2.0494, Train Accuracy: 28.20%, Temperatures:(0.01, 63.14)\n",
            "Old & New Losses 2045.029878616333 2044.5618629455566 Probab: tensor(1.0074, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 436, LR: 1.9531, Train Loss: 2.0471, Train Accuracy: 28.90%, Temperatures:(0.01, 62.51)\n",
            "Old & New Losses 2042.9394245147705 2048.9611625671387 Probab: tensor(0.9082, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 437, LR: 1.9531, Train Loss: 2.0490, Train Accuracy: 28.40%, Temperatures:(0.01, 61.88)\n",
            "Old & New Losses 2043.0219173431396 2046.4708805084229 Probab: tensor(0.9458, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 438, LR: 1.9531, Train Loss: 2.0476, Train Accuracy: 27.10%, Temperatures:(0.01, 61.26)\n",
            "Old & New Losses 2045.6409454345703 2053.621768951416 Probab: tensor(0.8779, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 439, LR: 1.9531, Train Loss: 2.0454, Train Accuracy: 28.40%, Temperatures:(0.01, 60.65)\n",
            "Old & New Losses 2038.8953685760498 2043.6537265777588 Probab: tensor(0.9245, device='cuda:0')\n",
            "Epoch 440, LR: 1.9531, Train Loss: 2.0507, Train Accuracy: 27.70%, Temperatures:(0.01, 60.04)\n",
            "Old & New Losses 2044.3072319030762 2044.9938774108887 Probab: tensor(0.9886, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 441, LR: 1.9531, Train Loss: 2.0464, Train Accuracy: 28.00%, Temperatures:(0.01, 59.44)\n",
            "Old & New Losses 2042.449951171875 2041.3594245910645 Probab: tensor(1.0185, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 442, LR: 1.9531, Train Loss: 2.0451, Train Accuracy: 27.40%, Temperatures:(0.01, 58.85)\n",
            "Old & New Losses 2042.442798614502 2072.8611946105957 Probab: tensor(0.5964, device='cuda:0')\n",
            "Epoch 443, LR: 1.9531, Train Loss: 2.0450, Train Accuracy: 27.30%, Temperatures:(0.01, 58.26)\n",
            "Old & New Losses 2043.3123111724854 2043.9488887786865 Probab: tensor(0.9891, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 444, LR: 1.9531, Train Loss: 2.0453, Train Accuracy: 26.00%, Temperatures:(0.01, 57.68)\n",
            "Old & New Losses 2043.553352355957 2037.9631519317627 Probab: tensor(1.1018, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 445, LR: 1.9531, Train Loss: 2.0415, Train Accuracy: 27.40%, Temperatures:(0.01, 57.10)\n",
            "Old & New Losses 2040.9777164459229 2045.3450679779053 Probab: tensor(0.9264, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 446, LR: 1.9531, Train Loss: 2.0452, Train Accuracy: 25.90%, Temperatures:(0.01, 56.53)\n",
            "Old & New Losses 2040.9455299377441 2046.2024211883545 Probab: tensor(0.9112, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 447, LR: 1.9531, Train Loss: 2.0424, Train Accuracy: 26.70%, Temperatures:(0.01, 55.96)\n",
            "Old & New Losses 2045.8629131317139 2054.4183254241943 Probab: tensor(0.8582, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 448, LR: 1.9531, Train Loss: 2.0510, Train Accuracy: 28.60%, Temperatures:(0.01, 55.40)\n",
            "Old & New Losses 2046.806812286377 2047.316551208496 Probab: tensor(0.9908, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 449, LR: 1.9531, Train Loss: 2.0466, Train Accuracy: 27.70%, Temperatures:(0.01, 54.85)\n",
            "Old & New Losses 2046.5753078460693 2046.3221073150635 Probab: tensor(1.0046, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 450, LR: 1.9531, Train Loss: 2.0452, Train Accuracy: 29.10%, Temperatures:(0.01, 54.30)\n",
            "Old & New Losses 2052.1066188812256 2045.3808307647705 Probab: tensor(1.1319, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 451, LR: 1.9531, Train Loss: 2.0462, Train Accuracy: 26.80%, Temperatures:(0.01, 53.76)\n",
            "Old & New Losses 2043.2991981506348 2050.3273010253906 Probab: tensor(0.8774, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 452, LR: 0.9766, Train Loss: 2.0473, Train Accuracy: 27.20%, Temperatures:(0.01, 53.22)\n",
            "Old & New Losses 2046.313762664795 2044.0232753753662 Probab: tensor(1.0440, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 453, LR: 0.9766, Train Loss: 2.0494, Train Accuracy: 27.70%, Temperatures:(0.01, 52.69)\n",
            "Old & New Losses 2047.9562282562256 2043.1413650512695 Probab: tensor(1.0957, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 454, LR: 0.9766, Train Loss: 2.0460, Train Accuracy: 28.20%, Temperatures:(0.01, 52.16)\n",
            "Old & New Losses 2047.7995872497559 2044.6052551269531 Probab: tensor(1.0632, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 455, LR: 0.9766, Train Loss: 2.0478, Train Accuracy: 26.40%, Temperatures:(0.01, 51.64)\n",
            "Old & New Losses 2048.1138229370117 2048.6629009246826 Probab: tensor(0.9894, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 456, LR: 0.9766, Train Loss: 2.0465, Train Accuracy: 27.10%, Temperatures:(0.01, 51.12)\n",
            "Old & New Losses 2048.0265617370605 2061.0759258270264 Probab: tensor(0.7747, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 457, LR: 0.9766, Train Loss: 2.0525, Train Accuracy: 28.60%, Temperatures:(0.01, 50.61)\n",
            "Old & New Losses 2059.215784072876 2058.2079887390137 Probab: tensor(1.0201, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 458, LR: 0.9766, Train Loss: 2.0543, Train Accuracy: 26.60%, Temperatures:(0.01, 50.11)\n",
            "Old & New Losses 2057.880401611328 2061.9099140167236 Probab: tensor(0.9227, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 459, LR: 0.9766, Train Loss: 2.0546, Train Accuracy: 25.90%, Temperatures:(0.01, 49.60)\n",
            "Old & New Losses 2060.694932937622 2073.502540588379 Probab: tensor(0.7724, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 460, LR: 0.9766, Train Loss: 2.0656, Train Accuracy: 25.10%, Temperatures:(0.01, 49.11)\n",
            "Old & New Losses 2065.7756328582764 2066.490888595581 Probab: tensor(0.9855, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 461, LR: 0.9766, Train Loss: 2.0713, Train Accuracy: 26.40%, Temperatures:(0.01, 48.62)\n",
            "Old & New Losses 2062.8674030303955 2085.3822231292725 Probab: tensor(0.6293, device='cuda:0')\n",
            "Epoch 462, LR: 0.9766, Train Loss: 2.0689, Train Accuracy: 25.70%, Temperatures:(0.01, 48.13)\n",
            "Old & New Losses 2068.7365531921387 2062.411069869995 Probab: tensor(1.1404, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 463, LR: 0.9766, Train Loss: 2.0683, Train Accuracy: 26.00%, Temperatures:(0.01, 47.65)\n",
            "Old & New Losses 2063.723564147949 2068.553924560547 Probab: tensor(0.9036, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 464, LR: 0.9766, Train Loss: 2.0666, Train Accuracy: 24.90%, Temperatures:(0.01, 47.17)\n",
            "Old & New Losses 2064.5391941070557 2073.3482837677 Probab: tensor(0.8297, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 465, LR: 0.9766, Train Loss: 2.0721, Train Accuracy: 23.70%, Temperatures:(0.01, 46.70)\n",
            "Old & New Losses 2062.6442432403564 2069.7405338287354 Probab: tensor(0.8590, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 466, LR: 0.9766, Train Loss: 2.0617, Train Accuracy: 26.30%, Temperatures:(0.01, 46.23)\n",
            "Old & New Losses 2063.669204711914 2070.4433917999268 Probab: tensor(0.8637, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 467, LR: 0.9766, Train Loss: 2.0641, Train Accuracy: 25.00%, Temperatures:(0.01, 45.77)\n",
            "Old & New Losses 2067.9726600646973 2072.704076766968 Probab: tensor(0.9018, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 468, LR: 0.9766, Train Loss: 2.0726, Train Accuracy: 23.60%, Temperatures:(0.01, 45.31)\n",
            "Old & New Losses 2070.3859329223633 2085.3171348571777 Probab: tensor(0.7193, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 469, LR: 0.9766, Train Loss: 2.0732, Train Accuracy: 22.30%, Temperatures:(0.01, 44.86)\n",
            "Old & New Losses 2083.9192867279053 2071.4468955993652 Probab: tensor(1.3205, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 470, LR: 0.9766, Train Loss: 2.0851, Train Accuracy: 25.20%, Temperatures:(0.01, 44.41)\n",
            "Old & New Losses 2068.5863494873047 2083.8749408721924 Probab: tensor(0.7088, device='cuda:0')\n",
            "Epoch 471, LR: 0.9766, Train Loss: 2.0760, Train Accuracy: 22.40%, Temperatures:(0.01, 43.97)\n",
            "Old & New Losses 2070.671319961548 2075.425624847412 Probab: tensor(0.8975, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 472, LR: 0.9766, Train Loss: 2.0770, Train Accuracy: 22.50%, Temperatures:(0.01, 43.53)\n",
            "Old & New Losses 2073.383331298828 2072.690010070801 Probab: tensor(1.0161, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 473, LR: 0.9766, Train Loss: 2.0762, Train Accuracy: 21.60%, Temperatures:(0.01, 43.09)\n",
            "Old & New Losses 2069.1821575164795 2069.6768760681152 Probab: tensor(0.9886, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 474, LR: 0.9766, Train Loss: 2.0699, Train Accuracy: 24.80%, Temperatures:(0.01, 42.66)\n",
            "Old & New Losses 2076.35235786438 2073.239803314209 Probab: tensor(1.0757, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 475, LR: 0.9766, Train Loss: 2.0734, Train Accuracy: 24.60%, Temperatures:(0.01, 42.24)\n",
            "Old & New Losses 2074.9051570892334 2093.477487564087 Probab: tensor(0.6442, device='cuda:0')\n",
            "Epoch 476, LR: 0.9766, Train Loss: 2.0729, Train Accuracy: 20.80%, Temperatures:(0.01, 41.81)\n",
            "Old & New Losses 2063.9986991882324 2077.371597290039 Probab: tensor(0.7263, device='cuda:0')\n",
            "Epoch 477, LR: 0.9766, Train Loss: 2.0703, Train Accuracy: 22.60%, Temperatures:(0.01, 41.40)\n",
            "Old & New Losses 2068.805456161499 2081.392526626587 Probab: tensor(0.7378, device='cuda:0')\n",
            "Epoch 478, LR: 0.9766, Train Loss: 2.0707, Train Accuracy: 23.40%, Temperatures:(0.01, 40.98)\n",
            "Old & New Losses 2075.5093097686768 2073.6262798309326 Probab: tensor(1.0470, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 479, LR: 0.9766, Train Loss: 2.0719, Train Accuracy: 23.30%, Temperatures:(0.01, 40.57)\n",
            "Old & New Losses 2071.9432830810547 2073.93741607666 Probab: tensor(0.9520, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 480, LR: 0.9766, Train Loss: 2.0726, Train Accuracy: 21.60%, Temperatures:(0.01, 40.17)\n",
            "Old & New Losses 2069.9377059936523 2074.7451782226562 Probab: tensor(0.8872, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 481, LR: 0.9766, Train Loss: 2.0738, Train Accuracy: 21.00%, Temperatures:(0.01, 39.76)\n",
            "Old & New Losses 2075.721502304077 2080.01446723938 Probab: tensor(0.8977, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 482, LR: 0.9766, Train Loss: 2.0810, Train Accuracy: 20.70%, Temperatures:(0.01, 39.37)\n",
            "Old & New Losses 2076.9057273864746 2080.918788909912 Probab: tensor(0.9031, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 483, LR: 0.9766, Train Loss: 2.0748, Train Accuracy: 20.50%, Temperatures:(0.01, 38.97)\n",
            "Old & New Losses 2080.2130699157715 2108.88934135437 Probab: tensor(0.4791, device='cuda:0')\n",
            "Epoch 484, LR: 0.9766, Train Loss: 2.0766, Train Accuracy: 21.10%, Temperatures:(0.01, 38.58)\n",
            "Old & New Losses 2077.545642852783 2085.8383178710938 Probab: tensor(0.8066, device='cuda:0')\n",
            "Epoch 485, LR: 0.9766, Train Loss: 2.0733, Train Accuracy: 19.80%, Temperatures:(0.01, 38.20)\n",
            "Old & New Losses 2075.0162601470947 2089.2014503479004 Probab: tensor(0.6898, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 486, LR: 0.9766, Train Loss: 2.0712, Train Accuracy: 22.60%, Temperatures:(0.01, 37.82)\n",
            "Old & New Losses 2079.6151161193848 2086.463689804077 Probab: tensor(0.8343, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 487, LR: 0.9766, Train Loss: 2.0871, Train Accuracy: 22.50%, Temperatures:(0.01, 37.44)\n",
            "Old & New Losses 2081.136703491211 2086.1546993255615 Probab: tensor(0.8746, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 488, LR: 0.9766, Train Loss: 2.0849, Train Accuracy: 22.30%, Temperatures:(0.01, 37.06)\n",
            "Old & New Losses 2088.2160663604736 2091.3071632385254 Probab: tensor(0.9200, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 489, LR: 0.9766, Train Loss: 2.0818, Train Accuracy: 21.80%, Temperatures:(0.01, 36.69)\n",
            "Old & New Losses 2103.5995483398438 2097.9514122009277 Probab: tensor(1.1664, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 490, LR: 0.9766, Train Loss: 2.0988, Train Accuracy: 21.00%, Temperatures:(0.01, 36.33)\n",
            "Old & New Losses 2095.0615406036377 2095.6966876983643 Probab: tensor(0.9827, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 491, LR: 0.9766, Train Loss: 2.0968, Train Accuracy: 21.10%, Temperatures:(0.01, 35.96)\n",
            "Old & New Losses 2096.269369125366 2095.35551071167 Probab: tensor(1.0257, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 492, LR: 0.9766, Train Loss: 2.0982, Train Accuracy: 22.40%, Temperatures:(0.01, 35.60)\n",
            "Old & New Losses 2095.6904888153076 2104.1722297668457 Probab: tensor(0.7880, device='cuda:0')\n",
            "Epoch 493, LR: 0.9766, Train Loss: 2.0993, Train Accuracy: 23.10%, Temperatures:(0.01, 35.25)\n",
            "Old & New Losses 2092.763900756836 2100.400686264038 Probab: tensor(0.8052, device='cuda:0')\n",
            "Epoch 494, LR: 0.9766, Train Loss: 2.0946, Train Accuracy: 23.40%, Temperatures:(0.01, 34.89)\n",
            "Old & New Losses 2095.273494720459 2090.475559234619 Probab: tensor(1.1474, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 495, LR: 0.9766, Train Loss: 2.0926, Train Accuracy: 24.50%, Temperatures:(0.01, 34.55)\n",
            "Old & New Losses 2088.2983207702637 2095.649480819702 Probab: tensor(0.8083, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 496, LR: 0.9766, Train Loss: 2.0930, Train Accuracy: 24.20%, Temperatures:(0.01, 34.20)\n",
            "Old & New Losses 2092.7512645721436 2089.8702144622803 Probab: tensor(1.0879, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 497, LR: 0.9766, Train Loss: 2.0974, Train Accuracy: 23.90%, Temperatures:(0.01, 33.86)\n",
            "Old & New Losses 2092.459201812744 2094.9292182922363 Probab: tensor(0.9296, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 498, LR: 0.9766, Train Loss: 2.0910, Train Accuracy: 24.70%, Temperatures:(0.01, 33.52)\n",
            "Old & New Losses 2085.4485034942627 2095.0183868408203 Probab: tensor(0.7516, device='cuda:0')\n",
            "Epoch 499, LR: 0.9766, Train Loss: 2.0980, Train Accuracy: 24.50%, Temperatures:(0.01, 33.18)\n",
            "Old & New Losses 2086.378335952759 2094.482660293579 Probab: tensor(0.7833, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 500, LR: 0.9766, Train Loss: 2.0927, Train Accuracy: 27.00%, Temperatures:(0.01, 32.85)\n",
            "Old & New Losses 2092.8385257720947 2092.7815437316895 Probab: tensor(1.0017, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 501, LR: 0.9766, Train Loss: 2.0922, Train Accuracy: 27.00%, Temperatures:(0.01, 32.52)\n",
            "Old & New Losses 2091.510057449341 2090.569019317627 Probab: tensor(1.0294, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 502, LR: 0.4883, Train Loss: 2.0951, Train Accuracy: 25.50%, Temperatures:(0.01, 32.20)\n",
            "Old & New Losses 2092.9505825042725 2095.111846923828 Probab: tensor(0.9351, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 503, LR: 0.4883, Train Loss: 2.0919, Train Accuracy: 28.40%, Temperatures:(0.01, 31.88)\n",
            "Old & New Losses 2089.6496772766113 2096.43292427063 Probab: tensor(0.8083, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 504, LR: 0.4883, Train Loss: 2.0947, Train Accuracy: 25.70%, Temperatures:(0.01, 31.56)\n",
            "Old & New Losses 2091.9039249420166 2095.168113708496 Probab: tensor(0.9017, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 505, LR: 0.4883, Train Loss: 2.0905, Train Accuracy: 25.80%, Temperatures:(0.01, 31.24)\n",
            "Old & New Losses 2091.8774604797363 2088.606119155884 Probab: tensor(1.1104, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 506, LR: 0.4883, Train Loss: 2.0954, Train Accuracy: 24.30%, Temperatures:(0.01, 30.93)\n",
            "Old & New Losses 2091.163158416748 2091.660499572754 Probab: tensor(0.9840, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 507, LR: 0.4883, Train Loss: 2.0943, Train Accuracy: 23.70%, Temperatures:(0.01, 30.62)\n",
            "Old & New Losses 2091.0496711730957 2093.705177307129 Probab: tensor(0.9169, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 508, LR: 0.4883, Train Loss: 2.0899, Train Accuracy: 24.40%, Temperatures:(0.01, 30.31)\n",
            "Old & New Losses 2089.324951171875 2101.5610694885254 Probab: tensor(0.6679, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 509, LR: 0.4883, Train Loss: 2.0975, Train Accuracy: 23.80%, Temperatures:(0.01, 30.01)\n",
            "Old & New Losses 2092.893600463867 2097.500801086426 Probab: tensor(0.8577, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 510, LR: 0.4883, Train Loss: 2.0970, Train Accuracy: 24.80%, Temperatures:(0.01, 29.71)\n",
            "Old & New Losses 2097.1949100494385 2096.6336727142334 Probab: tensor(1.0191, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 511, LR: 0.4883, Train Loss: 2.0952, Train Accuracy: 24.00%, Temperatures:(0.01, 29.41)\n",
            "Old & New Losses 2095.9300994873047 2098.6084938049316 Probab: tensor(0.9130, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 512, LR: 0.4883, Train Loss: 2.0994, Train Accuracy: 23.90%, Temperatures:(0.01, 29.12)\n",
            "Old & New Losses 2098.684787750244 2102.1950244903564 Probab: tensor(0.8864, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 513, LR: 0.4883, Train Loss: 2.1007, Train Accuracy: 23.70%, Temperatures:(0.01, 28.83)\n",
            "Old & New Losses 2096.672296524048 2102.860689163208 Probab: tensor(0.8068, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 514, LR: 0.4883, Train Loss: 2.1013, Train Accuracy: 24.80%, Temperatures:(0.01, 28.54)\n",
            "Old & New Losses 2097.252368927002 2094.6967601776123 Probab: tensor(1.0937, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 515, LR: 0.4883, Train Loss: 2.1019, Train Accuracy: 23.60%, Temperatures:(0.01, 28.25)\n",
            "Old & New Losses 2096.0536003112793 2101.527690887451 Probab: tensor(0.8239, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 516, LR: 0.4883, Train Loss: 2.1020, Train Accuracy: 24.00%, Temperatures:(0.01, 27.97)\n",
            "Old & New Losses 2096.5704917907715 2108.363628387451 Probab: tensor(0.6560, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 517, LR: 0.4883, Train Loss: 2.1067, Train Accuracy: 24.10%, Temperatures:(0.01, 27.69)\n",
            "Old & New Losses 2101.459741592407 2103.3825874328613 Probab: tensor(0.9329, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 518, LR: 0.4883, Train Loss: 2.1035, Train Accuracy: 25.10%, Temperatures:(0.01, 27.42)\n",
            "Old & New Losses 2097.9840755462646 2109.2171669006348 Probab: tensor(0.6638, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 519, LR: 0.4883, Train Loss: 2.1066, Train Accuracy: 23.60%, Temperatures:(0.01, 27.14)\n",
            "Old & New Losses 2102.9255390167236 2105.0333976745605 Probab: tensor(0.9253, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 520, LR: 0.4883, Train Loss: 2.1079, Train Accuracy: 23.90%, Temperatures:(0.01, 26.87)\n",
            "Old & New Losses 2106.708526611328 2106.0791015625 Probab: tensor(1.0237, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 521, LR: 0.4883, Train Loss: 2.1049, Train Accuracy: 21.50%, Temperatures:(0.01, 26.60)\n",
            "Old & New Losses 2104.281187057495 2106.4670085906982 Probab: tensor(0.9211, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 522, LR: 0.4883, Train Loss: 2.1028, Train Accuracy: 22.90%, Temperatures:(0.01, 26.34)\n",
            "Old & New Losses 2102.7047634124756 2105.9162616729736 Probab: tensor(0.8852, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 523, LR: 0.4883, Train Loss: 2.1033, Train Accuracy: 24.70%, Temperatures:(0.01, 26.07)\n",
            "Old & New Losses 2105.9632301330566 2109.7018718719482 Probab: tensor(0.8664, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 524, LR: 0.4883, Train Loss: 2.1115, Train Accuracy: 23.10%, Temperatures:(0.01, 25.81)\n",
            "Old & New Losses 2105.3693294525146 2110.1839542388916 Probab: tensor(0.8298, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 525, LR: 0.4883, Train Loss: 2.1074, Train Accuracy: 23.70%, Temperatures:(0.01, 25.55)\n",
            "Old & New Losses 2106.792449951172 2117.790937423706 Probab: tensor(0.6502, device='cuda:0')\n",
            "Epoch 526, LR: 0.4883, Train Loss: 2.1158, Train Accuracy: 23.00%, Temperatures:(0.01, 25.30)\n",
            "Old & New Losses 2107.696533203125 2112.8251552581787 Probab: tensor(0.8165, device='cuda:0')\n",
            "Epoch 527, LR: 0.4883, Train Loss: 2.1144, Train Accuracy: 23.10%, Temperatures:(0.01, 25.04)\n",
            "Old & New Losses 2103.0936241149902 2110.7397079467773 Probab: tensor(0.7369, device='cuda:0')\n",
            "Epoch 528, LR: 0.4883, Train Loss: 2.1132, Train Accuracy: 24.10%, Temperatures:(0.00, 24.79)\n",
            "Old & New Losses 2103.2955646514893 2104.2795181274414 Probab: tensor(0.9611, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 529, LR: 0.4883, Train Loss: 2.1069, Train Accuracy: 23.80%, Temperatures:(0.00, 24.55)\n",
            "Old & New Losses 2103.1746864318848 2106.245994567871 Probab: tensor(0.8824, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 530, LR: 0.4883, Train Loss: 2.1044, Train Accuracy: 23.50%, Temperatures:(0.00, 24.30)\n",
            "Old & New Losses 2101.0262966156006 2103.107452392578 Probab: tensor(0.9179, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 531, LR: 0.4883, Train Loss: 2.1078, Train Accuracy: 23.60%, Temperatures:(0.00, 24.06)\n",
            "Old & New Losses 2110.1090908050537 2106.8124771118164 Probab: tensor(1.1469, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 532, LR: 0.4883, Train Loss: 2.1092, Train Accuracy: 23.40%, Temperatures:(0.00, 23.82)\n",
            "Old & New Losses 2104.52938079834 2121.934652328491 Probab: tensor(0.4815, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 533, LR: 0.4883, Train Loss: 2.1121, Train Accuracy: 24.60%, Temperatures:(0.00, 23.58)\n",
            "Old & New Losses 2114.753484725952 2122.378349304199 Probab: tensor(0.7237, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 534, LR: 0.4883, Train Loss: 2.1218, Train Accuracy: 23.40%, Temperatures:(0.00, 23.34)\n",
            "Old & New Losses 2120.0478076934814 2129.363536834717 Probab: tensor(0.6709, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 535, LR: 0.4883, Train Loss: 2.1258, Train Accuracy: 25.20%, Temperatures:(0.00, 23.11)\n",
            "Old & New Losses 2130.23042678833 2124.569892883301 Probab: tensor(1.2775, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 536, LR: 0.4883, Train Loss: 2.1306, Train Accuracy: 23.10%, Temperatures:(0.00, 22.88)\n",
            "Old & New Losses 2127.601146697998 2126.314401626587 Probab: tensor(1.0579, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 537, LR: 0.4883, Train Loss: 2.1301, Train Accuracy: 24.00%, Temperatures:(0.00, 22.65)\n",
            "Old & New Losses 2116.4422035217285 2122.5175857543945 Probab: tensor(0.7647, device='cuda:0')\n",
            "Epoch 538, LR: 0.4883, Train Loss: 2.1241, Train Accuracy: 24.00%, Temperatures:(0.00, 22.42)\n",
            "Old & New Losses 2120.370864868164 2120.441675186157 Probab: tensor(0.9968, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 539, LR: 0.4883, Train Loss: 2.1209, Train Accuracy: 23.70%, Temperatures:(0.00, 22.20)\n",
            "Old & New Losses 2114.8579120635986 2113.241672515869 Probab: tensor(1.0755, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 540, LR: 0.4883, Train Loss: 2.1152, Train Accuracy: 25.40%, Temperatures:(0.00, 21.98)\n",
            "Old & New Losses 2116.9838905334473 2116.4963245391846 Probab: tensor(1.0224, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 541, LR: 0.4883, Train Loss: 2.1140, Train Accuracy: 24.70%, Temperatures:(0.00, 21.76)\n",
            "Old & New Losses 2108.480930328369 2117.3741817474365 Probab: tensor(0.6645, device='cuda:0')\n",
            "Epoch 542, LR: 0.4883, Train Loss: 2.1124, Train Accuracy: 23.90%, Temperatures:(0.00, 21.54)\n",
            "Old & New Losses 2113.5714054107666 2107.9330444335938 Probab: tensor(1.2992, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 543, LR: 0.4883, Train Loss: 2.1090, Train Accuracy: 26.00%, Temperatures:(0.00, 21.32)\n",
            "Old & New Losses 2111.405849456787 2106.125593185425 Probab: tensor(1.2810, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 544, LR: 0.4883, Train Loss: 2.1107, Train Accuracy: 24.90%, Temperatures:(0.00, 21.11)\n",
            "Old & New Losses 2112.049341201782 2109.8721027374268 Probab: tensor(1.1086, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 545, LR: 0.4883, Train Loss: 2.1095, Train Accuracy: 23.40%, Temperatures:(0.00, 20.90)\n",
            "Old & New Losses 2105.292320251465 2115.800380706787 Probab: tensor(0.6049, device='cuda:0')\n",
            "Epoch 546, LR: 0.4883, Train Loss: 2.1116, Train Accuracy: 23.10%, Temperatures:(0.00, 20.69)\n",
            "Old & New Losses 2112.6656532287598 2106.5633296966553 Probab: tensor(1.3430, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 547, LR: 0.4883, Train Loss: 2.1108, Train Accuracy: 24.40%, Temperatures:(0.00, 20.48)\n",
            "Old & New Losses 2105.287790298462 2120.7401752471924 Probab: tensor(0.4703, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 548, LR: 0.4883, Train Loss: 2.1111, Train Accuracy: 23.70%, Temperatures:(0.00, 20.28)\n",
            "Old & New Losses 2115.5004501342773 2133.4056854248047 Probab: tensor(0.4136, device='cuda:0')\n",
            "Epoch 549, LR: 0.4883, Train Loss: 2.1158, Train Accuracy: 23.60%, Temperatures:(0.00, 20.08)\n",
            "Old & New Losses 2114.802360534668 2122.6890087127686 Probab: tensor(0.6751, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 550, LR: 0.4883, Train Loss: 2.1110, Train Accuracy: 23.70%, Temperatures:(0.00, 19.88)\n",
            "Old & New Losses 2119.241714477539 2110.3837490081787 Probab: tensor(1.5615, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 551, LR: 0.4883, Train Loss: 2.1246, Train Accuracy: 22.90%, Temperatures:(0.00, 19.68)\n",
            "Old & New Losses 2115.8981323242188 2113.8100624084473 Probab: tensor(1.1120, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 552, LR: 0.2441, Train Loss: 2.1107, Train Accuracy: 23.70%, Temperatures:(0.00, 19.48)\n",
            "Old & New Losses 2109.684467315674 2118.5364723205566 Probab: tensor(0.6348, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 553, LR: 0.2441, Train Loss: 2.1140, Train Accuracy: 24.80%, Temperatures:(0.00, 19.29)\n",
            "Old & New Losses 2118.6439990997314 2118.8666820526123 Probab: tensor(0.9885, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 554, LR: 0.2441, Train Loss: 2.1184, Train Accuracy: 23.50%, Temperatures:(0.00, 19.09)\n",
            "Old & New Losses 2114.0928268432617 2113.008737564087 Probab: tensor(1.0584, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 555, LR: 0.2441, Train Loss: 2.1195, Train Accuracy: 24.20%, Temperatures:(0.00, 18.90)\n",
            "Old & New Losses 2115.537166595459 2116.1653995513916 Probab: tensor(0.9673, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 556, LR: 0.2441, Train Loss: 2.1183, Train Accuracy: 24.30%, Temperatures:(0.00, 18.71)\n",
            "Old & New Losses 2116.3463592529297 2118.7517642974854 Probab: tensor(0.8794, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 557, LR: 0.2441, Train Loss: 2.1176, Train Accuracy: 23.40%, Temperatures:(0.00, 18.53)\n",
            "Old & New Losses 2116.0192489624023 2132.7028274536133 Probab: tensor(0.4063, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 558, LR: 0.2441, Train Loss: 2.1152, Train Accuracy: 25.60%, Temperatures:(0.00, 18.34)\n",
            "Old & New Losses 2130.497694015503 2155.076026916504 Probab: tensor(0.2618, device='cuda:0')\n",
            "Epoch 559, LR: 0.2441, Train Loss: 2.1308, Train Accuracy: 23.30%, Temperatures:(0.00, 18.16)\n",
            "Old & New Losses 2127.516508102417 2130.7294368743896 Probab: tensor(0.8378, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 560, LR: 0.2441, Train Loss: 2.1284, Train Accuracy: 22.50%, Temperatures:(0.00, 17.98)\n",
            "Old & New Losses 2127.0313262939453 2124.465227127075 Probab: tensor(1.1534, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 561, LR: 0.2441, Train Loss: 2.1255, Train Accuracy: 23.80%, Temperatures:(0.00, 17.80)\n",
            "Old & New Losses 2123.358964920044 2123.0101585388184 Probab: tensor(1.0198, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 562, LR: 0.2441, Train Loss: 2.1267, Train Accuracy: 22.90%, Temperatures:(0.00, 17.62)\n",
            "Old & New Losses 2121.298313140869 2121.7901706695557 Probab: tensor(0.9725, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 563, LR: 0.2441, Train Loss: 2.1218, Train Accuracy: 24.30%, Temperatures:(0.00, 17.44)\n",
            "Old & New Losses 2123.2800483703613 2119.1725730895996 Probab: tensor(1.2655, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 564, LR: 0.2441, Train Loss: 2.1297, Train Accuracy: 23.70%, Temperatures:(0.00, 17.27)\n",
            "Old & New Losses 2123.0568885803223 2124.210834503174 Probab: tensor(0.9354, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 565, LR: 0.2441, Train Loss: 2.1233, Train Accuracy: 23.20%, Temperatures:(0.00, 17.09)\n",
            "Old & New Losses 2126.7902851104736 2132.683038711548 Probab: tensor(0.7084, device='cuda:0')\n",
            "Epoch 566, LR: 0.2441, Train Loss: 2.1233, Train Accuracy: 24.00%, Temperatures:(0.00, 16.92)\n",
            "Old & New Losses 2118.1485652923584 2124.492645263672 Probab: tensor(0.6874, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 567, LR: 0.2441, Train Loss: 2.1235, Train Accuracy: 23.50%, Temperatures:(0.00, 16.75)\n",
            "Old & New Losses 2123.4326362609863 2121.886730194092 Probab: tensor(1.0967, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 568, LR: 0.2441, Train Loss: 2.1208, Train Accuracy: 25.40%, Temperatures:(0.00, 16.59)\n",
            "Old & New Losses 2121.1626529693604 2123.0478286743164 Probab: tensor(0.8926, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 569, LR: 0.2441, Train Loss: 2.1231, Train Accuracy: 24.70%, Temperatures:(0.00, 16.42)\n",
            "Old & New Losses 2125.481605529785 2123.232841491699 Probab: tensor(1.1468, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 570, LR: 0.2441, Train Loss: 2.1271, Train Accuracy: 22.90%, Temperatures:(0.00, 16.26)\n",
            "Old & New Losses 2123.0649948120117 2130.28883934021 Probab: tensor(0.6412, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 571, LR: 0.2441, Train Loss: 2.1191, Train Accuracy: 24.20%, Temperatures:(0.00, 16.09)\n",
            "Old & New Losses 2131.758213043213 2127.5572776794434 Probab: tensor(1.2983, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 572, LR: 0.2441, Train Loss: 2.1283, Train Accuracy: 24.40%, Temperatures:(0.00, 15.93)\n",
            "Old & New Losses 2121.882915496826 2126.887798309326 Probab: tensor(0.7304, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 573, LR: 0.2441, Train Loss: 2.1343, Train Accuracy: 24.40%, Temperatures:(0.00, 15.77)\n",
            "Old & New Losses 2128.704786300659 2127.41756439209 Probab: tensor(1.0850, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 574, LR: 0.2441, Train Loss: 2.1264, Train Accuracy: 23.80%, Temperatures:(0.00, 15.62)\n",
            "Old & New Losses 2127.5582313537598 2119.8668479919434 Probab: tensor(1.6365, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 575, LR: 0.2441, Train Loss: 2.1258, Train Accuracy: 23.90%, Temperatures:(0.00, 15.46)\n",
            "Old & New Losses 2119.807720184326 2126.15966796875 Probab: tensor(0.6631, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 576, LR: 0.2441, Train Loss: 2.1218, Train Accuracy: 23.80%, Temperatures:(0.00, 15.31)\n",
            "Old & New Losses 2122.5736141204834 2121.232032775879 Probab: tensor(1.0916, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 577, LR: 0.2441, Train Loss: 2.1241, Train Accuracy: 25.00%, Temperatures:(0.00, 15.15)\n",
            "Old & New Losses 2121.3173866271973 2120.8019256591797 Probab: tensor(1.0346, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 578, LR: 0.2441, Train Loss: 2.1208, Train Accuracy: 25.10%, Temperatures:(0.00, 15.00)\n",
            "Old & New Losses 2122.6937770843506 2119.8787689208984 Probab: tensor(1.2064, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 579, LR: 0.2441, Train Loss: 2.1219, Train Accuracy: 24.40%, Temperatures:(0.00, 14.85)\n",
            "Old & New Losses 2122.511863708496 2123.9542961120605 Probab: tensor(0.9074, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 580, LR: 0.2441, Train Loss: 2.1190, Train Accuracy: 25.50%, Temperatures:(0.00, 14.70)\n",
            "Old & New Losses 2123.1064796447754 2120.248556137085 Probab: tensor(1.2146, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 581, LR: 0.2441, Train Loss: 2.1199, Train Accuracy: 24.10%, Temperatures:(0.00, 14.56)\n",
            "Old & New Losses 2121.6254234313965 2120.0180053710938 Probab: tensor(1.1168, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 582, LR: 0.2441, Train Loss: 2.1226, Train Accuracy: 22.40%, Temperatures:(0.00, 14.41)\n",
            "Old & New Losses 2121.368408203125 2122.067451477051 Probab: tensor(0.9526, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 583, LR: 0.2441, Train Loss: 2.1227, Train Accuracy: 24.90%, Temperatures:(0.00, 14.27)\n",
            "Old & New Losses 2114.795207977295 2123.063564300537 Probab: tensor(0.5601, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 584, LR: 0.2441, Train Loss: 2.1143, Train Accuracy: 25.10%, Temperatures:(0.00, 14.12)\n",
            "Old & New Losses 2112.22243309021 2117.450714111328 Probab: tensor(0.6906, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 585, LR: 0.2441, Train Loss: 2.1146, Train Accuracy: 23.50%, Temperatures:(0.00, 13.98)\n",
            "Old & New Losses 2115.8266067504883 2120.7377910614014 Probab: tensor(0.7038, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 586, LR: 0.2441, Train Loss: 2.1227, Train Accuracy: 24.00%, Temperatures:(0.00, 13.84)\n",
            "Old & New Losses 2121.68550491333 2118.20912361145 Probab: tensor(1.2855, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 587, LR: 0.2441, Train Loss: 2.1208, Train Accuracy: 23.90%, Temperatures:(0.00, 13.70)\n",
            "Old & New Losses 2119.6091175079346 2119.1930770874023 Probab: tensor(1.0308, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 588, LR: 0.2441, Train Loss: 2.1213, Train Accuracy: 23.90%, Temperatures:(0.00, 13.57)\n",
            "Old & New Losses 2121.392250061035 2113.08217048645 Probab: tensor(1.8451, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 589, LR: 0.2441, Train Loss: 2.1221, Train Accuracy: 23.30%, Temperatures:(0.00, 13.43)\n",
            "Old & New Losses 2121.6959953308105 2132.5159072875977 Probab: tensor(0.4468, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 590, LR: 0.2441, Train Loss: 2.1223, Train Accuracy: 23.90%, Temperatures:(0.00, 13.30)\n",
            "Old & New Losses 2119.823455810547 2139.3396854400635 Probab: tensor(0.2304, device='cuda:0')\n",
            "Epoch 591, LR: 0.2441, Train Loss: 2.1309, Train Accuracy: 24.60%, Temperatures:(0.00, 13.16)\n",
            "Old & New Losses 2120.896816253662 2122.98321723938 Probab: tensor(0.8534, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 592, LR: 0.2441, Train Loss: 2.1253, Train Accuracy: 24.20%, Temperatures:(0.00, 13.03)\n",
            "Old & New Losses 2121.9546794891357 2122.7924823760986 Probab: tensor(0.9377, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 593, LR: 0.2441, Train Loss: 2.1221, Train Accuracy: 24.50%, Temperatures:(0.00, 12.90)\n",
            "Old & New Losses 2122.1537590026855 2116.311550140381 Probab: tensor(1.5728, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 594, LR: 0.2441, Train Loss: 2.1216, Train Accuracy: 25.90%, Temperatures:(0.00, 12.77)\n",
            "Old & New Losses 2121.1891174316406 2122.9636669158936 Probab: tensor(0.8703, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 595, LR: 0.2441, Train Loss: 2.1210, Train Accuracy: 25.10%, Temperatures:(0.00, 12.64)\n",
            "Old & New Losses 2120.197534561157 2139.641284942627 Probab: tensor(0.2149, device='cuda:0')\n",
            "Epoch 596, LR: 0.2441, Train Loss: 2.1251, Train Accuracy: 23.50%, Temperatures:(0.00, 12.52)\n",
            "Old & New Losses 2119.658946990967 2122.410297393799 Probab: tensor(0.8027, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 597, LR: 0.2441, Train Loss: 2.1255, Train Accuracy: 26.00%, Temperatures:(0.00, 12.39)\n",
            "Old & New Losses 2118.3085441589355 2121.3181018829346 Probab: tensor(0.7844, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 598, LR: 0.2441, Train Loss: 2.1206, Train Accuracy: 25.20%, Temperatures:(0.00, 12.27)\n",
            "Old & New Losses 2118.7453269958496 2120.0900077819824 Probab: tensor(0.8962, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 599, LR: 0.2441, Train Loss: 2.1214, Train Accuracy: 25.40%, Temperatures:(0.00, 12.15)\n",
            "Old & New Losses 2117.2568798065186 2113.985776901245 Probab: tensor(1.3091, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 600, LR: 0.2441, Train Loss: 2.1169, Train Accuracy: 25.10%, Temperatures:(0.00, 12.03)\n",
            "Old & New Losses 2112.9062175750732 2118.1650161743164 Probab: tensor(0.6458, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 601, LR: 0.2441, Train Loss: 2.1139, Train Accuracy: 24.10%, Temperatures:(0.00, 11.90)\n",
            "Old & New Losses 2117.997646331787 2121.654272079468 Probab: tensor(0.7355, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 602, LR: 0.1221, Train Loss: 2.1184, Train Accuracy: 24.00%, Temperatures:(0.00, 11.79)\n",
            "Old & New Losses 2120.7926273345947 2118.50643157959 Probab: tensor(1.2141, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 603, LR: 0.1221, Train Loss: 2.1125, Train Accuracy: 24.70%, Temperatures:(0.00, 11.67)\n",
            "Old & New Losses 2109.438419342041 2114.199161529541 Probab: tensor(0.6650, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 604, LR: 0.1221, Train Loss: 2.1175, Train Accuracy: 24.20%, Temperatures:(0.00, 11.55)\n",
            "Old & New Losses 2118.720293045044 2112.639904022217 Probab: tensor(1.6928, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 605, LR: 0.1221, Train Loss: 2.1106, Train Accuracy: 28.00%, Temperatures:(0.00, 11.44)\n",
            "Old & New Losses 2108.7334156036377 2123.1541633605957 Probab: tensor(0.2834, device='cuda:0')\n",
            "Epoch 606, LR: 0.1221, Train Loss: 2.1092, Train Accuracy: 27.10%, Temperatures:(0.00, 11.32)\n",
            "Old & New Losses 2113.0378246307373 2122.699499130249 Probab: tensor(0.4260, device='cuda:0')\n",
            "Epoch 607, LR: 0.1221, Train Loss: 2.1069, Train Accuracy: 26.00%, Temperatures:(0.00, 11.21)\n",
            "Old & New Losses 2108.0009937286377 2111.521005630493 Probab: tensor(0.7305, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 608, LR: 0.1221, Train Loss: 2.1070, Train Accuracy: 26.20%, Temperatures:(0.00, 11.10)\n",
            "Old & New Losses 2111.187696456909 2115.8318519592285 Probab: tensor(0.6580, device='cuda:0')\n",
            "Epoch 609, LR: 0.1221, Train Loss: 2.1075, Train Accuracy: 26.20%, Temperatures:(0.00, 10.99)\n",
            "Old & New Losses 2113.220691680908 2115.0448322296143 Probab: tensor(0.8470, device='cuda:0')\n",
            "Epoch 610, LR: 0.1221, Train Loss: 2.1091, Train Accuracy: 27.80%, Temperatures:(0.00, 10.88)\n",
            "Old & New Losses 2106.2142848968506 2128.2317638397217 Probab: tensor(0.1321, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 611, LR: 0.1221, Train Loss: 2.1113, Train Accuracy: 25.60%, Temperatures:(0.00, 10.77)\n",
            "Old & New Losses 2129.487991333008 2133.0342292785645 Probab: tensor(0.7194, device='cuda:0')\n",
            "Epoch 612, LR: 0.1221, Train Loss: 2.1251, Train Accuracy: 25.90%, Temperatures:(0.00, 10.66)\n",
            "Old & New Losses 2129.615068435669 2168.555974960327 Probab: tensor(0.0259, device='cuda:0')\n",
            "Epoch 613, LR: 0.1221, Train Loss: 2.1286, Train Accuracy: 25.80%, Temperatures:(0.00, 10.55)\n",
            "Old & New Losses 2123.0173110961914 2127.90584564209 Probab: tensor(0.6292, device='cuda:0')\n",
            "Epoch 614, LR: 0.1221, Train Loss: 2.1258, Train Accuracy: 24.50%, Temperatures:(0.00, 10.45)\n",
            "Old & New Losses 2127.9611587524414 2120.4943656921387 Probab: tensor(2.0437, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 615, LR: 0.1221, Train Loss: 2.1255, Train Accuracy: 25.40%, Temperatures:(0.00, 10.34)\n",
            "Old & New Losses 2117.4304485321045 2121.8483448028564 Probab: tensor(0.6524, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 616, LR: 0.1221, Train Loss: 2.1226, Train Accuracy: 25.50%, Temperatures:(0.00, 10.24)\n",
            "Old & New Losses 2113.9490604400635 2113.9657497406006 Probab: tensor(0.9984, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 617, LR: 0.1221, Train Loss: 2.1125, Train Accuracy: 26.90%, Temperatures:(0.00, 10.14)\n",
            "Old & New Losses 2119.525671005249 2114.0360832214355 Probab: tensor(1.7187, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 618, LR: 0.1221, Train Loss: 2.1173, Train Accuracy: 26.90%, Temperatures:(0.00, 10.04)\n",
            "Old & New Losses 2117.2008514404297 2117.938280105591 Probab: tensor(0.9291, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 619, LR: 0.1221, Train Loss: 2.1147, Train Accuracy: 26.90%, Temperatures:(0.00, 9.93)\n",
            "Old & New Losses 2116.0972118377686 2122.6863861083984 Probab: tensor(0.5152, device='cuda:0')\n",
            "Epoch 620, LR: 0.1221, Train Loss: 2.1201, Train Accuracy: 26.20%, Temperatures:(0.00, 9.84)\n",
            "Old & New Losses 2118.8302040100098 2110.2235317230225 Probab: tensor(2.3991, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 621, LR: 0.1221, Train Loss: 2.1169, Train Accuracy: 27.80%, Temperatures:(0.00, 9.74)\n",
            "Old & New Losses 2113.8577461242676 2112.471103668213 Probab: tensor(1.1530, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 622, LR: 0.1221, Train Loss: 2.1075, Train Accuracy: 25.70%, Temperatures:(0.00, 9.64)\n",
            "Old & New Losses 2111.7396354675293 2117.3386573791504 Probab: tensor(0.5594, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 623, LR: 0.1221, Train Loss: 2.1109, Train Accuracy: 25.70%, Temperatures:(0.00, 9.54)\n",
            "Old & New Losses 2115.370750427246 2116.852283477783 Probab: tensor(0.8562, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 624, LR: 0.1221, Train Loss: 2.1178, Train Accuracy: 26.30%, Temperatures:(0.00, 9.45)\n",
            "Old & New Losses 2115.184545516968 2123.180627822876 Probab: tensor(0.4290, device='cuda:0')\n",
            "Epoch 625, LR: 0.1221, Train Loss: 2.1180, Train Accuracy: 27.20%, Temperatures:(0.00, 9.35)\n",
            "Old & New Losses 2120.13840675354 2123.948335647583 Probab: tensor(0.6654, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 626, LR: 0.1221, Train Loss: 2.1148, Train Accuracy: 25.90%, Temperatures:(0.00, 9.26)\n",
            "Old & New Losses 2120.4118728637695 2117.6929473876953 Probab: tensor(1.3413, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 627, LR: 0.1221, Train Loss: 2.1157, Train Accuracy: 26.50%, Temperatures:(0.00, 9.17)\n",
            "Old & New Losses 2124.5858669281006 2124.612808227539 Probab: tensor(0.9971, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 628, LR: 0.1221, Train Loss: 2.1259, Train Accuracy: 24.60%, Temperatures:(0.00, 9.08)\n",
            "Old & New Losses 2124.9120235443115 2123.044729232788 Probab: tensor(1.2284, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 629, LR: 0.1221, Train Loss: 2.1279, Train Accuracy: 24.40%, Temperatures:(0.00, 8.98)\n",
            "Old & New Losses 2128.856420516968 2122.5671768188477 Probab: tensor(2.0137, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 630, LR: 0.1221, Train Loss: 2.1272, Train Accuracy: 24.90%, Temperatures:(0.00, 8.89)\n",
            "Old & New Losses 2128.9703845977783 2128.354549407959 Probab: tensor(1.0717, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 631, LR: 0.1221, Train Loss: 2.1198, Train Accuracy: 27.40%, Temperatures:(0.00, 8.81)\n",
            "Old & New Losses 2119.8434829711914 2132.9386234283447 Probab: tensor(0.2260, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 632, LR: 0.1221, Train Loss: 2.1246, Train Accuracy: 25.00%, Temperatures:(0.00, 8.72)\n",
            "Old & New Losses 2131.843328475952 2139.418840408325 Probab: tensor(0.4194, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 633, LR: 0.1221, Train Loss: 2.1316, Train Accuracy: 23.80%, Temperatures:(0.00, 8.63)\n",
            "Old & New Losses 2137.9172801971436 2134.9215507507324 Probab: tensor(1.4150, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 634, LR: 0.1221, Train Loss: 2.1309, Train Accuracy: 25.20%, Temperatures:(0.00, 8.54)\n",
            "Old & New Losses 2138.0035877227783 2137.43257522583 Probab: tensor(1.0691, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 635, LR: 0.1221, Train Loss: 2.1328, Train Accuracy: 24.30%, Temperatures:(0.00, 8.46)\n",
            "Old & New Losses 2136.6631984710693 2132.1024894714355 Probab: tensor(1.7146, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 636, LR: 0.1221, Train Loss: 2.1399, Train Accuracy: 24.20%, Temperatures:(0.00, 8.37)\n",
            "Old & New Losses 2138.22603225708 2132.917881011963 Probab: tensor(1.8849, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 637, LR: 0.1221, Train Loss: 2.1334, Train Accuracy: 23.70%, Temperatures:(0.00, 8.29)\n",
            "Old & New Losses 2129.6749114990234 2141.5045261383057 Probab: tensor(0.2401, device='cuda:0')\n",
            "Epoch 638, LR: 0.1221, Train Loss: 2.1257, Train Accuracy: 23.90%, Temperatures:(0.00, 8.21)\n",
            "Old & New Losses 2131.4456462860107 2135.721206665039 Probab: tensor(0.5940, device='cuda:0')\n",
            "Epoch 639, LR: 0.1221, Train Loss: 2.1333, Train Accuracy: 24.50%, Temperatures:(0.00, 8.13)\n",
            "Old & New Losses 2129.72092628479 2136.3539695739746 Probab: tensor(0.4421, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 640, LR: 0.1221, Train Loss: 2.1356, Train Accuracy: 24.10%, Temperatures:(0.00, 8.04)\n",
            "Old & New Losses 2134.8135471343994 2140.8894062042236 Probab: tensor(0.4699, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 641, LR: 0.1221, Train Loss: 2.1339, Train Accuracy: 24.00%, Temperatures:(0.00, 7.96)\n",
            "Old & New Losses 2138.8165950775146 2139.6796703338623 Probab: tensor(0.8973, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 642, LR: 0.1221, Train Loss: 2.1414, Train Accuracy: 23.30%, Temperatures:(0.00, 7.88)\n",
            "Old & New Losses 2144.118547439575 2141.02840423584 Probab: tensor(1.4798, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 643, LR: 0.1221, Train Loss: 2.1433, Train Accuracy: 22.40%, Temperatures:(0.00, 7.81)\n",
            "Old & New Losses 2134.523868560791 2141.3707733154297 Probab: tensor(0.4159, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 644, LR: 0.1221, Train Loss: 2.1410, Train Accuracy: 23.20%, Temperatures:(0.00, 7.73)\n",
            "Old & New Losses 2139.4405364990234 2143.1522369384766 Probab: tensor(0.6186, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 645, LR: 0.1221, Train Loss: 2.1396, Train Accuracy: 24.00%, Temperatures:(0.00, 7.65)\n",
            "Old & New Losses 2138.2412910461426 2134.9501609802246 Probab: tensor(1.5376, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 646, LR: 0.1221, Train Loss: 2.1399, Train Accuracy: 24.20%, Temperatures:(0.00, 7.57)\n",
            "Old & New Losses 2134.52410697937 2134.9923610687256 Probab: tensor(0.9400, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 647, LR: 0.1221, Train Loss: 2.1321, Train Accuracy: 23.30%, Temperatures:(0.00, 7.50)\n",
            "Old & New Losses 2142.4760818481445 2136.538505554199 Probab: tensor(2.2076, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 648, LR: 0.1221, Train Loss: 2.1357, Train Accuracy: 25.60%, Temperatures:(0.00, 7.42)\n",
            "Old & New Losses 2138.6311054229736 2146.169900894165 Probab: tensor(0.3622, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 649, LR: 0.1221, Train Loss: 2.1412, Train Accuracy: 24.30%, Temperatures:(0.00, 7.35)\n",
            "Old & New Losses 2142.669439315796 2143.61310005188 Probab: tensor(0.8795, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 650, LR: 0.1221, Train Loss: 2.1425, Train Accuracy: 24.90%, Temperatures:(0.00, 7.28)\n",
            "Old & New Losses 2140.3841972351074 2144.402265548706 Probab: tensor(0.5756, device='cuda:0')\n",
            "Epoch 651, LR: 0.1221, Train Loss: 2.1400, Train Accuracy: 23.60%, Temperatures:(0.00, 7.20)\n",
            "Old & New Losses 2141.463041305542 2143.643617630005 Probab: tensor(0.7388, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 652, LR: 0.0610, Train Loss: 2.1406, Train Accuracy: 24.60%, Temperatures:(0.00, 7.13)\n",
            "Old & New Losses 2146.7745304107666 2144.652843475342 Probab: tensor(1.3466, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 653, LR: 0.0610, Train Loss: 2.1457, Train Accuracy: 24.10%, Temperatures:(0.00, 7.06)\n",
            "Old & New Losses 2142.850399017334 2142.9452896118164 Probab: tensor(0.9866, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 654, LR: 0.0610, Train Loss: 2.1465, Train Accuracy: 23.60%, Temperatures:(0.00, 6.99)\n",
            "Old & New Losses 2141.7949199676514 2151.210308074951 Probab: tensor(0.2600, device='cuda:0')\n",
            "Epoch 655, LR: 0.0610, Train Loss: 2.1480, Train Accuracy: 23.60%, Temperatures:(0.00, 6.92)\n",
            "Old & New Losses 2146.918535232544 2146.8868255615234 Probab: tensor(1.0046, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 656, LR: 0.0610, Train Loss: 2.1445, Train Accuracy: 23.60%, Temperatures:(0.00, 6.85)\n",
            "Old & New Losses 2148.8852500915527 2143.0177688598633 Probab: tensor(2.3552, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 657, LR: 0.0610, Train Loss: 2.1501, Train Accuracy: 22.50%, Temperatures:(0.00, 6.78)\n",
            "Old & New Losses 2148.5776901245117 2153.1591415405273 Probab: tensor(0.5088, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 658, LR: 0.0610, Train Loss: 2.1446, Train Accuracy: 25.30%, Temperatures:(0.00, 6.71)\n",
            "Old & New Losses 2148.318290710449 2165.9748554229736 Probab: tensor(0.0721, device='cuda:0')\n",
            "Epoch 659, LR: 0.0610, Train Loss: 2.1512, Train Accuracy: 23.90%, Temperatures:(0.00, 6.65)\n",
            "Old & New Losses 2148.181200027466 2150.1057147979736 Probab: tensor(0.7486, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 660, LR: 0.0610, Train Loss: 2.1473, Train Accuracy: 23.70%, Temperatures:(0.00, 6.58)\n",
            "Old & New Losses 2152.2836685180664 2145.1332569122314 Probab: tensor(2.9646, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 661, LR: 0.0610, Train Loss: 2.1513, Train Accuracy: 21.90%, Temperatures:(0.00, 6.51)\n",
            "Old & New Losses 2144.1924571990967 2143.003225326538 Probab: tensor(1.2003, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 662, LR: 0.0610, Train Loss: 2.1478, Train Accuracy: 22.40%, Temperatures:(0.00, 6.45)\n",
            "Old & New Losses 2143.9530849456787 2144.8655128479004 Probab: tensor(0.8681, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 663, LR: 0.0610, Train Loss: 2.1427, Train Accuracy: 24.80%, Temperatures:(0.00, 6.38)\n",
            "Old & New Losses 2145.2040672302246 2148.7581729888916 Probab: tensor(0.5731, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 664, LR: 0.0610, Train Loss: 2.1479, Train Accuracy: 20.90%, Temperatures:(0.00, 6.32)\n",
            "Old & New Losses 2149.5349407196045 2151.024341583252 Probab: tensor(0.7901, device='cuda:0')\n",
            "Epoch 665, LR: 0.0610, Train Loss: 2.1444, Train Accuracy: 23.00%, Temperatures:(0.00, 6.26)\n",
            "Old & New Losses 2144.3212032318115 2149.970054626465 Probab: tensor(0.4054, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 666, LR: 0.0610, Train Loss: 2.1474, Train Accuracy: 23.30%, Temperatures:(0.00, 6.19)\n",
            "Old & New Losses 2149.930238723755 2147.383213043213 Probab: tensor(1.5086, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 667, LR: 0.0610, Train Loss: 2.1442, Train Accuracy: 22.20%, Temperatures:(0.00, 6.13)\n",
            "Old & New Losses 2146.516799926758 2147.770643234253 Probab: tensor(0.8151, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 668, LR: 0.0610, Train Loss: 2.1503, Train Accuracy: 23.30%, Temperatures:(0.00, 6.07)\n",
            "Old & New Losses 2148.775577545166 2145.4646587371826 Probab: tensor(1.7252, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 669, LR: 0.0610, Train Loss: 2.1443, Train Accuracy: 23.90%, Temperatures:(0.00, 6.01)\n",
            "Old & New Losses 2144.9947357177734 2160.8667373657227 Probab: tensor(0.0713, device='cuda:0')\n",
            "Epoch 670, LR: 0.0610, Train Loss: 2.1503, Train Accuracy: 22.20%, Temperatures:(0.00, 5.95)\n",
            "Old & New Losses 2147.3779678344727 2170.0565814971924 Probab: tensor(0.0221, device='cuda:0')\n",
            "Epoch 671, LR: 0.0610, Train Loss: 2.1424, Train Accuracy: 23.70%, Temperatures:(0.00, 5.89)\n",
            "Old & New Losses 2147.071361541748 2152.937650680542 Probab: tensor(0.3694, device='cuda:0')\n",
            "Epoch 672, LR: 0.0610, Train Loss: 2.1447, Train Accuracy: 22.90%, Temperatures:(0.00, 5.83)\n",
            "Old & New Losses 2144.615650177002 2148.509979248047 Probab: tensor(0.5129, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 673, LR: 0.0610, Train Loss: 2.1416, Train Accuracy: 23.70%, Temperatures:(0.00, 5.77)\n",
            "Old & New Losses 2147.857189178467 2153.428316116333 Probab: tensor(0.3810, device='cuda:0')\n",
            "Epoch 674, LR: 0.0610, Train Loss: 2.1537, Train Accuracy: 21.80%, Temperatures:(0.00, 5.72)\n",
            "Old & New Losses 2152.406692504883 2148.5016345977783 Probab: tensor(1.9802, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 675, LR: 0.0610, Train Loss: 2.1477, Train Accuracy: 21.70%, Temperatures:(0.00, 5.66)\n",
            "Old & New Losses 2142.554521560669 2147.096872329712 Probab: tensor(0.4481, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 676, LR: 0.0610, Train Loss: 2.1489, Train Accuracy: 22.30%, Temperatures:(0.00, 5.60)\n",
            "Old & New Losses 2142.7810192108154 2149.9433517456055 Probab: tensor(0.2785, device='cuda:0')\n",
            "Epoch 677, LR: 0.0610, Train Loss: 2.1505, Train Accuracy: 22.70%, Temperatures:(0.00, 5.55)\n",
            "Old & New Losses 2146.038770675659 2140.913963317871 Probab: tensor(2.5194, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 678, LR: 0.0610, Train Loss: 2.1471, Train Accuracy: 22.40%, Temperatures:(0.00, 5.49)\n",
            "Old & New Losses 2140.4709815979004 2149.9617099761963 Probab: tensor(0.1776, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 679, LR: 0.0610, Train Loss: 2.1407, Train Accuracy: 21.90%, Temperatures:(0.00, 5.44)\n",
            "Old & New Losses 2147.392511367798 2151.58748626709 Probab: tensor(0.4622, device='cuda:0')\n",
            "Epoch 680, LR: 0.0610, Train Loss: 2.1489, Train Accuracy: 22.90%, Temperatures:(0.00, 5.38)\n",
            "Old & New Losses 2142.4052715301514 2148.3142375946045 Probab: tensor(0.3335, device='cuda:0')\n",
            "Epoch 681, LR: 0.0610, Train Loss: 2.1513, Train Accuracy: 23.20%, Temperatures:(0.00, 5.33)\n",
            "Old & New Losses 2144.224166870117 2140.404462814331 Probab: tensor(2.0482, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 682, LR: 0.0610, Train Loss: 2.1513, Train Accuracy: 22.60%, Temperatures:(0.00, 5.27)\n",
            "Old & New Losses 2142.0185565948486 2148.1142044067383 Probab: tensor(0.3148, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 683, LR: 0.0610, Train Loss: 2.1446, Train Accuracy: 22.20%, Temperatures:(0.00, 5.22)\n",
            "Old & New Losses 2147.1681594848633 2152.1191596984863 Probab: tensor(0.3875, device='cuda:0')\n",
            "Epoch 684, LR: 0.0610, Train Loss: 2.1490, Train Accuracy: 22.50%, Temperatures:(0.00, 5.17)\n",
            "Old & New Losses 2143.7222957611084 2155.416488647461 Probab: tensor(0.1041, device='cuda:0')\n",
            "Epoch 685, LR: 0.0610, Train Loss: 2.1430, Train Accuracy: 23.00%, Temperatures:(0.00, 5.12)\n",
            "Old & New Losses 2150.4342555999756 2151.5145301818848 Probab: tensor(0.8097, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 686, LR: 0.0610, Train Loss: 2.1447, Train Accuracy: 21.30%, Temperatures:(0.00, 5.07)\n",
            "Old & New Losses 2150.6690979003906 2146.1429595947266 Probab: tensor(2.4433, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 687, LR: 0.0610, Train Loss: 2.1472, Train Accuracy: 23.70%, Temperatures:(0.00, 5.02)\n",
            "Old & New Losses 2152.1589756011963 2150.7039070129395 Probab: tensor(1.3365, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 688, LR: 0.0610, Train Loss: 2.1469, Train Accuracy: 22.80%, Temperatures:(0.00, 4.97)\n",
            "Old & New Losses 2148.818254470825 2149.319648742676 Probab: tensor(0.9040, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 689, LR: 0.0610, Train Loss: 2.1488, Train Accuracy: 23.60%, Temperatures:(0.00, 4.92)\n",
            "Old & New Losses 2149.3797302246094 2149.6291160583496 Probab: tensor(0.9505, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 690, LR: 0.0610, Train Loss: 2.1476, Train Accuracy: 24.40%, Temperatures:(0.00, 4.87)\n",
            "Old & New Losses 2149.4414806365967 2145.6332206726074 Probab: tensor(2.1869, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 691, LR: 0.0610, Train Loss: 2.1501, Train Accuracy: 22.70%, Temperatures:(0.00, 4.82)\n",
            "Old & New Losses 2147.8936672210693 2152.1353721618652 Probab: tensor(0.4146, device='cuda:0')\n",
            "Epoch 692, LR: 0.0610, Train Loss: 2.1485, Train Accuracy: 21.60%, Temperatures:(0.00, 4.77)\n",
            "Old & New Losses 2148.803234100342 2149.125099182129 Probab: tensor(0.9348, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 693, LR: 0.0610, Train Loss: 2.1490, Train Accuracy: 22.30%, Temperatures:(0.00, 4.72)\n",
            "Old & New Losses 2145.2558040618896 2146.1427211761475 Probab: tensor(0.8288, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 694, LR: 0.0610, Train Loss: 2.1453, Train Accuracy: 24.90%, Temperatures:(0.00, 4.68)\n",
            "Old & New Losses 2149.3639945983887 2147.0158100128174 Probab: tensor(1.6525, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 695, LR: 0.0610, Train Loss: 2.1494, Train Accuracy: 23.00%, Temperatures:(0.00, 4.63)\n",
            "Old & New Losses 2143.217086791992 2159.8470211029053 Probab: tensor(0.0275, device='cuda:0')\n",
            "Epoch 696, LR: 0.0610, Train Loss: 2.1446, Train Accuracy: 24.70%, Temperatures:(0.00, 4.58)\n",
            "Old & New Losses 2145.9338665008545 2151.7460346221924 Probab: tensor(0.2813, device='cuda:0')\n",
            "Epoch 697, LR: 0.0610, Train Loss: 2.1458, Train Accuracy: 23.60%, Temperatures:(0.00, 4.54)\n",
            "Old & New Losses 2148.98943901062 2147.463798522949 Probab: tensor(1.3998, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 698, LR: 0.0610, Train Loss: 2.1494, Train Accuracy: 22.00%, Temperatures:(0.00, 4.49)\n",
            "Old & New Losses 2142.9741382598877 2146.3968753814697 Probab: tensor(0.4667, device='cuda:0')\n",
            "Epoch 699, LR: 0.0610, Train Loss: 2.1511, Train Accuracy: 22.00%, Temperatures:(0.00, 4.45)\n",
            "Old & New Losses 2147.7131843566895 2146.200656890869 Probab: tensor(1.4052, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 700, LR: 0.0610, Train Loss: 2.1502, Train Accuracy: 24.20%, Temperatures:(0.00, 4.40)\n",
            "Old & New Losses 2149.6400833129883 2165.7745838165283 Probab: tensor(0.0256, device='cuda:0')\n",
            "Epoch 701, LR: 0.0610, Train Loss: 2.1497, Train Accuracy: 23.30%, Temperatures:(0.00, 4.36)\n",
            "Old & New Losses 2149.9836444854736 2151.902198791504 Probab: tensor(0.6439, device='cuda:0')\n",
            "Epoch 702, LR: 0.0305, Train Loss: 2.1487, Train Accuracy: 22.60%, Temperatures:(0.00, 4.31)\n",
            "Old & New Losses 2145.7152366638184 2148.0553150177 Probab: tensor(0.5813, device='cuda:0')\n",
            "Epoch 703, LR: 0.0305, Train Loss: 2.1485, Train Accuracy: 22.60%, Temperatures:(0.00, 4.27)\n",
            "Old & New Losses 2150.14910697937 2148.174047470093 Probab: tensor(1.5880, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 704, LR: 0.0305, Train Loss: 2.1489, Train Accuracy: 22.90%, Temperatures:(0.00, 4.23)\n",
            "Old & New Losses 2146.3537216186523 2146.1503505706787 Probab: tensor(1.0493, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 705, LR: 0.0305, Train Loss: 2.1452, Train Accuracy: 23.40%, Temperatures:(0.00, 4.19)\n",
            "Old & New Losses 2150.887966156006 2148.125410079956 Probab: tensor(1.9347, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 706, LR: 0.0305, Train Loss: 2.1467, Train Accuracy: 22.80%, Temperatures:(0.00, 4.14)\n",
            "Old & New Losses 2150.3963470458984 2152.705669403076 Probab: tensor(0.5728, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 707, LR: 0.0305, Train Loss: 2.1504, Train Accuracy: 22.50%, Temperatures:(0.00, 4.10)\n",
            "Old & New Losses 2152.5840759277344 2171.079158782959 Probab: tensor(0.0110, device='cuda:0')\n",
            "Epoch 708, LR: 0.0305, Train Loss: 2.1515, Train Accuracy: 22.20%, Temperatures:(0.00, 4.06)\n",
            "Old & New Losses 2155.6200981140137 2156.182289123535 Probab: tensor(0.8707, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 709, LR: 0.0305, Train Loss: 2.1529, Train Accuracy: 22.40%, Temperatures:(0.00, 4.02)\n",
            "Old & New Losses 2156.0275554656982 2156.4462184906006 Probab: tensor(0.9011, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 710, LR: 0.0305, Train Loss: 2.1531, Train Accuracy: 23.00%, Temperatures:(0.00, 3.98)\n",
            "Old & New Losses 2161.9465351104736 2162.1859073638916 Probab: tensor(0.9416, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 711, LR: 0.0305, Train Loss: 2.1602, Train Accuracy: 20.70%, Temperatures:(0.00, 3.94)\n",
            "Old & New Losses 2164.119243621826 2155.9369564056396 Probab: tensor(7.9746, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 712, LR: 0.0305, Train Loss: 2.1614, Train Accuracy: 20.40%, Temperatures:(0.00, 3.90)\n",
            "Old & New Losses 2160.498857498169 2162.273406982422 Probab: tensor(0.6345, device='cuda:0')\n",
            "Epoch 713, LR: 0.0305, Train Loss: 2.1550, Train Accuracy: 21.60%, Temperatures:(0.00, 3.86)\n",
            "Old & New Losses 2164.123773574829 2156.9623947143555 Probab: tensor(6.3859, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 714, LR: 0.0305, Train Loss: 2.1563, Train Accuracy: 21.30%, Temperatures:(0.00, 3.82)\n",
            "Old & New Losses 2157.3312282562256 2154.195785522461 Probab: tensor(2.2704, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 715, LR: 0.0305, Train Loss: 2.1536, Train Accuracy: 21.80%, Temperatures:(0.00, 3.79)\n",
            "Old & New Losses 2152.3571014404297 2162.677526473999 Probab: tensor(0.0655, device='cuda:0')\n",
            "Epoch 716, LR: 0.0305, Train Loss: 2.1544, Train Accuracy: 22.00%, Temperatures:(0.00, 3.75)\n",
            "Old & New Losses 2152.7812480926514 2157.4172973632812 Probab: tensor(0.2902, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 717, LR: 0.0305, Train Loss: 2.1593, Train Accuracy: 21.80%, Temperatures:(0.00, 3.71)\n",
            "Old & New Losses 2159.942388534546 2158.73646736145 Probab: tensor(1.3841, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 718, LR: 0.0305, Train Loss: 2.1572, Train Accuracy: 21.00%, Temperatures:(0.00, 3.67)\n",
            "Old & New Losses 2159.708023071289 2155.578136444092 Probab: tensor(3.0782, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 719, LR: 0.0305, Train Loss: 2.1569, Train Accuracy: 21.50%, Temperatures:(0.00, 3.64)\n",
            "Old & New Losses 2155.66349029541 2157.703161239624 Probab: tensor(0.5707, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 720, LR: 0.0305, Train Loss: 2.1573, Train Accuracy: 21.10%, Temperatures:(0.00, 3.60)\n",
            "Old & New Losses 2161.57603263855 2157.8304767608643 Probab: tensor(2.8304, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 721, LR: 0.0305, Train Loss: 2.1645, Train Accuracy: 20.20%, Temperatures:(0.00, 3.56)\n",
            "Old & New Losses 2156.0328006744385 2155.2042961120605 Probab: tensor(1.2617, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 722, LR: 0.0305, Train Loss: 2.1590, Train Accuracy: 22.10%, Temperatures:(0.00, 3.53)\n",
            "Old & New Losses 2157.416343688965 2157.957077026367 Probab: tensor(0.8579, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 723, LR: 0.0305, Train Loss: 2.1514, Train Accuracy: 20.90%, Temperatures:(0.00, 3.49)\n",
            "Old & New Losses 2156.587600708008 2161.146879196167 Probab: tensor(0.2711, device='cuda:0')\n",
            "Epoch 724, LR: 0.0305, Train Loss: 2.1578, Train Accuracy: 20.80%, Temperatures:(0.00, 3.46)\n",
            "Old & New Losses 2158.977746963501 2152.0261764526367 Probab: tensor(7.4646, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 725, LR: 0.0305, Train Loss: 2.1569, Train Accuracy: 22.60%, Temperatures:(0.00, 3.42)\n",
            "Old & New Losses 2152.942419052124 2156.45694732666 Probab: tensor(0.3582, device='cuda:0')\n",
            "Epoch 726, LR: 0.0305, Train Loss: 2.1558, Train Accuracy: 21.50%, Temperatures:(0.00, 3.39)\n",
            "Old & New Losses 2155.6193828582764 2153.813362121582 Probab: tensor(1.7038, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 727, LR: 0.0305, Train Loss: 2.1549, Train Accuracy: 22.30%, Temperatures:(0.00, 3.36)\n",
            "Old & New Losses 2155.6599140167236 2153.496026992798 Probab: tensor(1.9058, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 728, LR: 0.0305, Train Loss: 2.1573, Train Accuracy: 20.60%, Temperatures:(0.00, 3.32)\n",
            "Old & New Losses 2150.4828929901123 2155.714988708496 Probab: tensor(0.2070, device='cuda:0')\n",
            "Epoch 729, LR: 0.0305, Train Loss: 2.1537, Train Accuracy: 21.70%, Temperatures:(0.00, 3.29)\n",
            "Old & New Losses 2152.1739959716797 2157.156229019165 Probab: tensor(0.2198, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 730, LR: 0.0305, Train Loss: 2.1584, Train Accuracy: 20.10%, Temperatures:(0.00, 3.26)\n",
            "Old & New Losses 2153.956651687622 2157.264232635498 Probab: tensor(0.3621, device='cuda:0')\n",
            "Epoch 731, LR: 0.0305, Train Loss: 2.1560, Train Accuracy: 23.10%, Temperatures:(0.00, 3.22)\n",
            "Old & New Losses 2154.1147232055664 2155.5886268615723 Probab: tensor(0.6330, device='cuda:0')\n",
            "Epoch 732, LR: 0.0305, Train Loss: 2.1531, Train Accuracy: 22.60%, Temperatures:(0.00, 3.19)\n",
            "Old & New Losses 2149.441957473755 2156.4948558807373 Probab: tensor(0.1097, device='cuda:0')\n",
            "Epoch 733, LR: 0.0305, Train Loss: 2.1513, Train Accuracy: 22.70%, Temperatures:(0.00, 3.16)\n",
            "Old & New Losses 2155.527114868164 2154.054880142212 Probab: tensor(1.5936, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 734, LR: 0.0305, Train Loss: 2.1560, Train Accuracy: 22.30%, Temperatures:(0.00, 3.13)\n",
            "Old & New Losses 2157.715082168579 2153.096914291382 Probab: tensor(4.3781, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 735, LR: 0.0305, Train Loss: 2.1558, Train Accuracy: 21.20%, Temperatures:(0.00, 3.10)\n",
            "Old & New Losses 2155.6556224823 2154.829978942871 Probab: tensor(1.3056, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 736, LR: 0.0305, Train Loss: 2.1535, Train Accuracy: 22.40%, Temperatures:(0.00, 3.07)\n",
            "Old & New Losses 2155.10892868042 2160.153865814209 Probab: tensor(0.1929, device='cuda:0')\n",
            "Epoch 737, LR: 0.0305, Train Loss: 2.1544, Train Accuracy: 22.40%, Temperatures:(0.00, 3.03)\n",
            "Old & New Losses 2157.3264598846436 2160.9907150268555 Probab: tensor(0.2990, device='cuda:0')\n",
            "Epoch 738, LR: 0.0305, Train Loss: 2.1561, Train Accuracy: 22.80%, Temperatures:(0.00, 3.00)\n",
            "Old & New Losses 2153.897523880005 2159.1575145721436 Probab: tensor(0.1736, device='cuda:0')\n",
            "Epoch 739, LR: 0.0305, Train Loss: 2.1560, Train Accuracy: 21.90%, Temperatures:(0.00, 2.97)\n",
            "Old & New Losses 2154.6683311462402 2156.550645828247 Probab: tensor(0.5311, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 740, LR: 0.0305, Train Loss: 2.1491, Train Accuracy: 23.00%, Temperatures:(0.00, 2.94)\n",
            "Old & New Losses 2157.517671585083 2157.3290824890137 Probab: tensor(1.0661, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 741, LR: 0.0305, Train Loss: 2.1572, Train Accuracy: 24.50%, Temperatures:(0.00, 2.92)\n",
            "Old & New Losses 2156.421899795532 2156.61358833313 Probab: tensor(0.9364, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 742, LR: 0.0305, Train Loss: 2.1580, Train Accuracy: 24.00%, Temperatures:(0.00, 2.89)\n",
            "Old & New Losses 2157.355308532715 2158.214569091797 Probab: tensor(0.7425, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 743, LR: 0.0305, Train Loss: 2.1566, Train Accuracy: 22.90%, Temperatures:(0.00, 2.86)\n",
            "Old & New Losses 2159.574031829834 2159.101724624634 Probab: tensor(1.1798, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 744, LR: 0.0305, Train Loss: 2.1580, Train Accuracy: 24.00%, Temperatures:(0.00, 2.83)\n",
            "Old & New Losses 2158.588171005249 2156.5048694610596 Probab: tensor(2.0887, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 745, LR: 0.0305, Train Loss: 2.1593, Train Accuracy: 23.50%, Temperatures:(0.00, 2.80)\n",
            "Old & New Losses 2159.9764823913574 2159.444808959961 Probab: tensor(1.2091, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 746, LR: 0.0305, Train Loss: 2.1603, Train Accuracy: 25.40%, Temperatures:(0.00, 2.77)\n",
            "Old & New Losses 2161.8573665618896 2160.250663757324 Probab: tensor(1.7853, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 747, LR: 0.0305, Train Loss: 2.1561, Train Accuracy: 27.40%, Temperatures:(0.00, 2.74)\n",
            "Old & New Losses 2168.9682006835938 2164.759874343872 Probab: tensor(4.6338, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 748, LR: 0.0305, Train Loss: 2.1624, Train Accuracy: 23.00%, Temperatures:(0.00, 2.72)\n",
            "Old & New Losses 2165.2040481567383 2160.6180667877197 Probab: tensor(5.4079, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 749, LR: 0.0305, Train Loss: 2.1682, Train Accuracy: 24.30%, Temperatures:(0.00, 2.69)\n",
            "Old & New Losses 2159.22212600708 2162.3497009277344 Probab: tensor(0.3126, device='cuda:0')\n",
            "Epoch 750, LR: 0.0305, Train Loss: 2.1548, Train Accuracy: 24.90%, Temperatures:(0.00, 2.66)\n",
            "Old & New Losses 2158.8099002838135 2161.1337661743164 Probab: tensor(0.4178, device='cuda:0')\n",
            "Epoch 751, LR: 0.0305, Train Loss: 2.1606, Train Accuracy: 25.80%, Temperatures:(0.00, 2.64)\n",
            "Old & New Losses 2162.94002532959 2164.6087169647217 Probab: tensor(0.5310, device='cuda:0')\n",
            "Epoch 752, LR: 0.0153, Train Loss: 2.1595, Train Accuracy: 26.80%, Temperatures:(0.00, 2.61)\n",
            "Old & New Losses 2159.139394760132 2163.949728012085 Probab: tensor(0.1583, device='cuda:0')\n",
            "Epoch 753, LR: 0.0153, Train Loss: 2.1620, Train Accuracy: 25.10%, Temperatures:(0.00, 2.58)\n",
            "Old & New Losses 2160.463571548462 2162.6553535461426 Probab: tensor(0.4282, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 754, LR: 0.0153, Train Loss: 2.1634, Train Accuracy: 25.90%, Temperatures:(0.00, 2.56)\n",
            "Old & New Losses 2161.834239959717 2160.4623794555664 Probab: tensor(1.7097, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 755, LR: 0.0153, Train Loss: 2.1640, Train Accuracy: 25.10%, Temperatures:(0.00, 2.53)\n",
            "Old & New Losses 2163.485050201416 2161.902904510498 Probab: tensor(1.8678, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 756, LR: 0.0153, Train Loss: 2.1610, Train Accuracy: 24.30%, Temperatures:(0.00, 2.51)\n",
            "Old & New Losses 2165.0943756103516 2165.8241748809814 Probab: tensor(0.7474, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 757, LR: 0.0153, Train Loss: 2.1595, Train Accuracy: 24.60%, Temperatures:(0.00, 2.48)\n",
            "Old & New Losses 2171.2193489074707 2172.823190689087 Probab: tensor(0.5240, device='cuda:0')\n",
            "Epoch 758, LR: 0.0153, Train Loss: 2.1670, Train Accuracy: 23.30%, Temperatures:(0.00, 2.46)\n",
            "Old & New Losses 2165.4655933380127 2167.6886081695557 Probab: tensor(0.4047, device='cuda:0')\n",
            "Epoch 759, LR: 0.0153, Train Loss: 2.1620, Train Accuracy: 23.90%, Temperatures:(0.00, 2.43)\n",
            "Old & New Losses 2167.3011779785156 2167.048931121826 Probab: tensor(1.1093, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 760, LR: 0.0153, Train Loss: 2.1686, Train Accuracy: 22.90%, Temperatures:(0.00, 2.41)\n",
            "Old & New Losses 2167.5496101379395 2165.983200073242 Probab: tensor(1.9163, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 761, LR: 0.0153, Train Loss: 2.1657, Train Accuracy: 24.30%, Temperatures:(0.00, 2.38)\n",
            "Old & New Losses 2160.5167388916016 2168.259620666504 Probab: tensor(0.0389, device='cuda:0')\n",
            "Epoch 762, LR: 0.0153, Train Loss: 2.1668, Train Accuracy: 25.00%, Temperatures:(0.00, 2.36)\n",
            "Old & New Losses 2167.694330215454 2162.8243923187256 Probab: tensor(7.8709, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 763, LR: 0.0153, Train Loss: 2.1643, Train Accuracy: 22.90%, Temperatures:(0.00, 2.34)\n",
            "Old & New Losses 2163.116216659546 2166.9459342956543 Probab: tensor(0.1942, device='cuda:0')\n",
            "Epoch 764, LR: 0.0153, Train Loss: 2.1662, Train Accuracy: 21.60%, Temperatures:(0.00, 2.31)\n",
            "Old & New Losses 2162.601947784424 2172.502279281616 Probab: tensor(0.0138, device='cuda:0')\n",
            "Epoch 765, LR: 0.0153, Train Loss: 2.1657, Train Accuracy: 23.30%, Temperatures:(0.00, 2.29)\n",
            "Old & New Losses 2167.7324771881104 2170.5923080444336 Probab: tensor(0.2869, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 766, LR: 0.0153, Train Loss: 2.1643, Train Accuracy: 23.50%, Temperatures:(0.00, 2.27)\n",
            "Old & New Losses 2172.3074913024902 2171.627998352051 Probab: tensor(1.3494, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 767, LR: 0.0153, Train Loss: 2.1659, Train Accuracy: 22.30%, Temperatures:(0.00, 2.24)\n",
            "Old & New Losses 2171.8342304229736 2171.1277961730957 Probab: tensor(1.3699, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 768, LR: 0.0153, Train Loss: 2.1685, Train Accuracy: 22.70%, Temperatures:(0.00, 2.22)\n",
            "Old & New Losses 2174.785852432251 2172.495126724243 Probab: tensor(2.8033, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 769, LR: 0.0153, Train Loss: 2.1758, Train Accuracy: 22.00%, Temperatures:(0.00, 2.20)\n",
            "Old & New Losses 2169.8877811431885 2170.977830886841 Probab: tensor(0.6093, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 770, LR: 0.0153, Train Loss: 2.1735, Train Accuracy: 25.00%, Temperatures:(0.00, 2.18)\n",
            "Old & New Losses 2169.2304611206055 2172.7418899536133 Probab: tensor(0.1995, device='cuda:0')\n",
            "Epoch 771, LR: 0.0153, Train Loss: 2.1716, Train Accuracy: 23.80%, Temperatures:(0.00, 2.16)\n",
            "Old & New Losses 2172.137498855591 2172.3713874816895 Probab: tensor(0.8972, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 772, LR: 0.0153, Train Loss: 2.1686, Train Accuracy: 24.30%, Temperatures:(0.00, 2.13)\n",
            "Old & New Losses 2167.942762374878 2169.557571411133 Probab: tensor(0.4693, device='cuda:0')\n",
            "Epoch 773, LR: 0.0153, Train Loss: 2.1709, Train Accuracy: 24.50%, Temperatures:(0.00, 2.11)\n",
            "Old & New Losses 2168.2918071746826 2172.5564002990723 Probab: tensor(0.1329, device='cuda:0')\n",
            "Epoch 774, LR: 0.0153, Train Loss: 2.1710, Train Accuracy: 22.70%, Temperatures:(0.00, 2.09)\n",
            "Old & New Losses 2169.616460800171 2168.562650680542 Probab: tensor(1.6548, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 775, LR: 0.0153, Train Loss: 2.1702, Train Accuracy: 21.50%, Temperatures:(0.00, 2.07)\n",
            "Old & New Losses 2173.9144325256348 2172.3783016204834 Probab: tensor(2.0993, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 776, LR: 0.0153, Train Loss: 2.1696, Train Accuracy: 25.20%, Temperatures:(0.00, 2.05)\n",
            "Old & New Losses 2171.5259552001953 2166.6154861450195 Probab: tensor(10.9644, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 777, LR: 0.0153, Train Loss: 2.1740, Train Accuracy: 23.80%, Temperatures:(0.00, 2.03)\n",
            "Old & New Losses 2172.3549365997314 2170.2349185943604 Probab: tensor(2.8414, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 778, LR: 0.0153, Train Loss: 2.1728, Train Accuracy: 23.50%, Temperatures:(0.00, 2.01)\n",
            "Old & New Losses 2168.5078144073486 2170.238971710205 Probab: tensor(0.4226, device='cuda:0')\n",
            "Epoch 779, LR: 0.0153, Train Loss: 2.1705, Train Accuracy: 22.40%, Temperatures:(0.00, 1.99)\n",
            "Old & New Losses 2169.6362495422363 2169.4114208221436 Probab: tensor(1.1196, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 780, LR: 0.0153, Train Loss: 2.1724, Train Accuracy: 21.90%, Temperatures:(0.00, 1.97)\n",
            "Old & New Losses 2167.437791824341 2170.804500579834 Probab: tensor(0.1810, device='cuda:0')\n",
            "Epoch 781, LR: 0.0153, Train Loss: 2.1713, Train Accuracy: 23.80%, Temperatures:(0.00, 1.95)\n",
            "Old & New Losses 2173.116683959961 2172.5261211395264 Probab: tensor(1.3537, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 782, LR: 0.0153, Train Loss: 2.1709, Train Accuracy: 21.20%, Temperatures:(0.00, 1.93)\n",
            "Old & New Losses 2173.0945110321045 2170.0222492218018 Probab: tensor(4.9104, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 783, LR: 0.0153, Train Loss: 2.1727, Train Accuracy: 24.20%, Temperatures:(0.00, 1.91)\n",
            "Old & New Losses 2172.9092597961426 2172.8038787841797 Probab: tensor(1.0567, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 784, LR: 0.0153, Train Loss: 2.1681, Train Accuracy: 22.50%, Temperatures:(0.00, 1.89)\n",
            "Old & New Losses 2173.2571125030518 2170.1102256774902 Probab: tensor(5.2756, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 785, LR: 0.0153, Train Loss: 2.1693, Train Accuracy: 23.60%, Temperatures:(0.00, 1.87)\n",
            "Old & New Losses 2173.858642578125 2178.4322261810303 Probab: tensor(0.0870, device='cuda:0')\n",
            "Epoch 786, LR: 0.0153, Train Loss: 2.1727, Train Accuracy: 23.40%, Temperatures:(0.00, 1.85)\n",
            "Old & New Losses 2169.3644523620605 2175.3201484680176 Probab: tensor(0.0403, device='cuda:0')\n",
            "Epoch 787, LR: 0.0153, Train Loss: 2.1722, Train Accuracy: 22.70%, Temperatures:(0.00, 1.84)\n",
            "Old & New Losses 2176.581382751465 2171.6606616973877 Probab: tensor(14.5874, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 788, LR: 0.0153, Train Loss: 2.1690, Train Accuracy: 23.20%, Temperatures:(0.00, 1.82)\n",
            "Old & New Losses 2171.306371688843 2172.1246242523193 Probab: tensor(0.6375, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 789, LR: 0.0153, Train Loss: 2.1708, Train Accuracy: 23.00%, Temperatures:(0.00, 1.80)\n",
            "Old & New Losses 2169.590711593628 2168.4839725494385 Probab: tensor(1.8497, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 790, LR: 0.0153, Train Loss: 2.1673, Train Accuracy: 21.90%, Temperatures:(0.00, 1.78)\n",
            "Old & New Losses 2173.081398010254 2170.322895050049 Probab: tensor(4.7042, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 791, LR: 0.0153, Train Loss: 2.1671, Train Accuracy: 22.90%, Temperatures:(0.00, 1.76)\n",
            "Old & New Losses 2176.341772079468 2161.125898361206 Probab: tensor(5583.4365, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 792, LR: 0.0153, Train Loss: 2.1692, Train Accuracy: 22.90%, Temperatures:(0.00, 1.75)\n",
            "Old & New Losses 2163.666009902954 2164.4909381866455 Probab: tensor(0.6235, device='cuda:0')\n",
            "Epoch 793, LR: 0.0153, Train Loss: 2.1629, Train Accuracy: 20.70%, Temperatures:(0.00, 1.73)\n",
            "Old & New Losses 2161.3612174987793 2162.3902320861816 Probab: tensor(0.5514, device='cuda:0')\n",
            "Epoch 794, LR: 0.0153, Train Loss: 2.1687, Train Accuracy: 21.20%, Temperatures:(0.00, 1.71)\n",
            "Old & New Losses 2166.1293506622314 2161.540985107422 Probab: tensor(14.6039, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 795, LR: 0.0153, Train Loss: 2.1659, Train Accuracy: 19.60%, Temperatures:(0.00, 1.69)\n",
            "Old & New Losses 2165.100574493408 2177.272081375122 Probab: tensor(0.0008, device='cuda:0')\n",
            "Epoch 796, LR: 0.0153, Train Loss: 2.1615, Train Accuracy: 21.80%, Temperatures:(0.00, 1.68)\n",
            "Old & New Losses 2164.022207260132 2169.76261138916 Probab: tensor(0.0326, device='cuda:0')\n",
            "Epoch 797, LR: 0.0153, Train Loss: 2.1679, Train Accuracy: 20.10%, Temperatures:(0.00, 1.66)\n",
            "Old & New Losses 2164.536714553833 2166.4693355560303 Probab: tensor(0.3123, device='cuda:0')\n",
            "Epoch 798, LR: 0.0153, Train Loss: 2.1670, Train Accuracy: 21.70%, Temperatures:(0.00, 1.64)\n",
            "Old & New Losses 2164.742946624756 2165.3592586517334 Probab: tensor(0.6873, device='cuda:0')\n",
            "Epoch 799, LR: 0.0153, Train Loss: 2.1627, Train Accuracy: 19.80%, Temperatures:(0.00, 1.63)\n",
            "Old & New Losses 2164.975643157959 2171.058416366577 Probab: tensor(0.0238, device='cuda:0')\n",
            "Epoch 800, LR: 0.0153, Train Loss: 2.1666, Train Accuracy: 20.30%, Temperatures:(0.00, 1.61)\n",
            "Old & New Losses 2165.0969982147217 2167.6855087280273 Probab: tensor(0.2006, device='cuda:0')\n",
            "Epoch 801, LR: 0.0153, Train Loss: 2.1652, Train Accuracy: 21.70%, Temperatures:(0.00, 1.60)\n",
            "Old & New Losses 2162.4414920806885 2169.174909591675 Probab: tensor(0.0147, device='cuda:0')\n",
            "Epoch 802, LR: 0.0076, Train Loss: 2.1633, Train Accuracy: 24.30%, Temperatures:(0.00, 1.58)\n",
            "Old & New Losses 2167.083263397217 2164.875030517578 Probab: tensor(4.0489, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 803, LR: 0.0076, Train Loss: 2.1666, Train Accuracy: 21.00%, Temperatures:(0.00, 1.56)\n",
            "Old & New Losses 2164.1030311584473 2164.297342300415 Probab: tensor(0.8831, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 804, LR: 0.0076, Train Loss: 2.1667, Train Accuracy: 21.30%, Temperatures:(0.00, 1.55)\n",
            "Old & New Losses 2169.504165649414 2166.3126945495605 Probab: tensor(7.8630, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 805, LR: 0.0076, Train Loss: 2.1693, Train Accuracy: 22.60%, Temperatures:(0.00, 1.53)\n",
            "Old & New Losses 2165.0071144104004 2173.269510269165 Probab: tensor(0.0045, device='cuda:0')\n",
            "Epoch 806, LR: 0.0076, Train Loss: 2.1683, Train Accuracy: 22.40%, Temperatures:(0.00, 1.52)\n",
            "Old & New Losses 2164.8988723754883 2168.7183380126953 Probab: tensor(0.0806, device='cuda:0')\n",
            "Epoch 807, LR: 0.0076, Train Loss: 2.1668, Train Accuracy: 21.10%, Temperatures:(0.00, 1.50)\n",
            "Old & New Losses 2165.5030250549316 2166.6276454925537 Probab: tensor(0.4729, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 808, LR: 0.0076, Train Loss: 2.1660, Train Accuracy: 21.50%, Temperatures:(0.00, 1.49)\n",
            "Old & New Losses 2165.4603481292725 2171.8509197235107 Probab: tensor(0.0136, device='cuda:0')\n",
            "Epoch 809, LR: 0.0076, Train Loss: 2.1665, Train Accuracy: 18.90%, Temperatures:(0.00, 1.47)\n",
            "Old & New Losses 2167.410135269165 2169.6956157684326 Probab: tensor(0.2116, device='cuda:0')\n",
            "Epoch 810, LR: 0.0076, Train Loss: 2.1698, Train Accuracy: 20.60%, Temperatures:(0.00, 1.46)\n",
            "Old & New Losses 2165.026903152466 2174.919605255127 Probab: tensor(0.0011, device='cuda:0')\n",
            "Epoch 811, LR: 0.0076, Train Loss: 2.1667, Train Accuracy: 20.60%, Temperatures:(0.00, 1.44)\n",
            "Old & New Losses 2166.6691303253174 2167.1018600463867 Probab: tensor(0.7408, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 812, LR: 0.0076, Train Loss: 2.1660, Train Accuracy: 21.00%, Temperatures:(0.00, 1.43)\n",
            "Old & New Losses 2168.9934730529785 2164.409875869751 Probab: tensor(24.7706, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 813, LR: 0.0076, Train Loss: 2.1698, Train Accuracy: 20.60%, Temperatures:(0.00, 1.41)\n",
            "Old & New Losses 2168.3199405670166 2161.9973182678223 Probab: tensor(87.5426, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 814, LR: 0.0076, Train Loss: 2.1663, Train Accuracy: 23.40%, Temperatures:(0.00, 1.40)\n",
            "Old & New Losses 2166.6271686553955 2160.749673843384 Probab: tensor(66.6377, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 815, LR: 0.0076, Train Loss: 2.1623, Train Accuracy: 22.00%, Temperatures:(0.00, 1.39)\n",
            "Old & New Losses 2159.5242023468018 2157.3963165283203 Probab: tensor(4.6444, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 816, LR: 0.0076, Train Loss: 2.1585, Train Accuracy: 21.30%, Temperatures:(0.00, 1.37)\n",
            "Old & New Losses 2157.5376987457275 2160.8548164367676 Probab: tensor(0.0891, device='cuda:0')\n",
            "Epoch 817, LR: 0.0076, Train Loss: 2.1586, Train Accuracy: 23.40%, Temperatures:(0.00, 1.36)\n",
            "Old & New Losses 2161.022663116455 2163.4840965270996 Probab: tensor(0.1633, device='cuda:0')\n",
            "Epoch 818, LR: 0.0076, Train Loss: 2.1624, Train Accuracy: 24.30%, Temperatures:(0.00, 1.34)\n",
            "Old & New Losses 2158.7953567504883 2161.4162921905518 Probab: tensor(0.1424, device='cuda:0')\n",
            "Epoch 819, LR: 0.0076, Train Loss: 2.1603, Train Accuracy: 23.40%, Temperatures:(0.00, 1.33)\n",
            "Old & New Losses 2157.0889949798584 2165.473699569702 Probab: tensor(0.0018, device='cuda:0')\n",
            "Epoch 820, LR: 0.0076, Train Loss: 2.1596, Train Accuracy: 20.70%, Temperatures:(0.00, 1.32)\n",
            "Old & New Losses 2157.0310592651367 2160.377025604248 Probab: tensor(0.0789, device='cuda:0')\n",
            "Epoch 821, LR: 0.0076, Train Loss: 2.1598, Train Accuracy: 23.10%, Temperatures:(0.00, 1.30)\n",
            "Old & New Losses 2163.1453037261963 2164.740562438965 Probab: tensor(0.2944, device='cuda:0')\n",
            "Epoch 822, LR: 0.0076, Train Loss: 2.1620, Train Accuracy: 22.10%, Temperatures:(0.00, 1.29)\n",
            "Old & New Losses 2160.34197807312 2162.071466445923 Probab: tensor(0.2621, device='cuda:0')\n",
            "Epoch 823, LR: 0.0076, Train Loss: 2.1603, Train Accuracy: 23.40%, Temperatures:(0.00, 1.28)\n",
            "Old & New Losses 2158.812999725342 2162.9650592803955 Probab: tensor(0.0389, device='cuda:0')\n",
            "Epoch 824, LR: 0.0076, Train Loss: 2.1573, Train Accuracy: 22.80%, Temperatures:(0.00, 1.27)\n",
            "Old & New Losses 2158.1625938415527 2158.682346343994 Probab: tensor(0.6632, device='cuda:0')\n",
            "Epoch 825, LR: 0.0076, Train Loss: 2.1617, Train Accuracy: 23.10%, Temperatures:(0.00, 1.25)\n",
            "Old & New Losses 2161.43798828125 2160.984516143799 Probab: tensor(1.4360, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 826, LR: 0.0076, Train Loss: 2.1630, Train Accuracy: 23.10%, Temperatures:(0.00, 1.24)\n",
            "Old & New Losses 2161.0896587371826 2162.1434688568115 Probab: tensor(0.4277, device='cuda:0')\n",
            "Epoch 827, LR: 0.0076, Train Loss: 2.1634, Train Accuracy: 20.00%, Temperatures:(0.00, 1.23)\n",
            "Old & New Losses 2162.9226207733154 2159.525156021118 Probab: tensor(15.8976, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 828, LR: 0.0076, Train Loss: 2.1610, Train Accuracy: 20.50%, Temperatures:(0.00, 1.22)\n",
            "Old & New Losses 2162.3923778533936 2162.6970767974854 Probab: tensor(0.7783, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 829, LR: 0.0076, Train Loss: 2.1628, Train Accuracy: 22.20%, Temperatures:(0.00, 1.20)\n",
            "Old & New Losses 2161.668062210083 2162.3153686523438 Probab: tensor(0.5841, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 830, LR: 0.0076, Train Loss: 2.1626, Train Accuracy: 21.60%, Temperatures:(0.00, 1.19)\n",
            "Old & New Losses 2164.9985313415527 2162.0845794677734 Probab: tensor(11.5320, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 831, LR: 0.0076, Train Loss: 2.1593, Train Accuracy: 22.30%, Temperatures:(0.00, 1.18)\n",
            "Old & New Losses 2159.5492362976074 2163.3424758911133 Probab: tensor(0.0402, device='cuda:0')\n",
            "Epoch 832, LR: 0.0076, Train Loss: 2.1662, Train Accuracy: 20.00%, Temperatures:(0.00, 1.17)\n",
            "Old & New Losses 2163.0406379699707 2163.0256175994873 Probab: tensor(1.0129, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 833, LR: 0.0076, Train Loss: 2.1615, Train Accuracy: 20.50%, Temperatures:(0.00, 1.16)\n",
            "Old & New Losses 2163.4154319763184 2163.071393966675 Probab: tensor(1.3465, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 834, LR: 0.0076, Train Loss: 2.1630, Train Accuracy: 21.20%, Temperatures:(0.00, 1.14)\n",
            "Old & New Losses 2164.045572280884 2163.973569869995 Probab: tensor(1.0649, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 835, LR: 0.0076, Train Loss: 2.1613, Train Accuracy: 20.00%, Temperatures:(0.00, 1.13)\n",
            "Old & New Losses 2164.5517349243164 2166.1622524261475 Probab: tensor(0.2415, device='cuda:0')\n",
            "Epoch 836, LR: 0.0076, Train Loss: 2.1646, Train Accuracy: 20.80%, Temperatures:(0.00, 1.12)\n",
            "Old & New Losses 2163.42830657959 2172.0633506774902 Probab: tensor(0.0005, device='cuda:0')\n",
            "Epoch 837, LR: 0.0076, Train Loss: 2.1635, Train Accuracy: 21.70%, Temperatures:(0.00, 1.11)\n",
            "Old & New Losses 2162.644863128662 2162.78076171875 Probab: tensor(0.8848, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 838, LR: 0.0076, Train Loss: 2.1638, Train Accuracy: 21.10%, Temperatures:(0.00, 1.10)\n",
            "Old & New Losses 2167.363166809082 2161.5164279937744 Probab: tensor(203.7316, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 839, LR: 0.0076, Train Loss: 2.1664, Train Accuracy: 21.60%, Temperatures:(0.00, 1.09)\n",
            "Old & New Losses 2163.666248321533 2165.9674644470215 Probab: tensor(0.1208, device='cuda:0')\n",
            "Epoch 840, LR: 0.0076, Train Loss: 2.1629, Train Accuracy: 23.10%, Temperatures:(0.00, 1.08)\n",
            "Old & New Losses 2165.729284286499 2163.4607315063477 Probab: tensor(8.2056, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 841, LR: 0.0076, Train Loss: 2.1614, Train Accuracy: 21.70%, Temperatures:(0.00, 1.07)\n",
            "Old & New Losses 2164.3667221069336 2169.3763732910156 Probab: tensor(0.0091, device='cuda:0')\n",
            "Epoch 842, LR: 0.0076, Train Loss: 2.1629, Train Accuracy: 22.40%, Temperatures:(0.00, 1.06)\n",
            "Old & New Losses 2168.5502529144287 2164.1788482666016 Probab: tensor(62.6933, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 843, LR: 0.0076, Train Loss: 2.1635, Train Accuracy: 24.00%, Temperatures:(0.00, 1.05)\n",
            "Old & New Losses 2167.8028106689453 2166.8860912323 Probab: tensor(2.4027, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 844, LR: 0.0076, Train Loss: 2.1686, Train Accuracy: 22.60%, Temperatures:(0.00, 1.04)\n",
            "Old & New Losses 2164.968967437744 2167.774438858032 Probab: tensor(0.0666, device='cuda:0')\n",
            "Epoch 845, LR: 0.0076, Train Loss: 2.1633, Train Accuracy: 23.10%, Temperatures:(0.00, 1.02)\n",
            "Old & New Losses 2166.9559478759766 2161.2987518310547 Probab: tensor(249.4851, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 846, LR: 0.0076, Train Loss: 2.1695, Train Accuracy: 22.50%, Temperatures:(0.00, 1.01)\n",
            "Old & New Losses 2170.2358722686768 2162.8148555755615 Probab: tensor(1500.2555, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 847, LR: 0.0076, Train Loss: 2.1709, Train Accuracy: 22.90%, Temperatures:(0.00, 1.00)\n",
            "Old & New Losses 2166.1791801452637 2165.597677230835 Probab: tensor(1.7840, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 848, LR: 0.0076, Train Loss: 2.1622, Train Accuracy: 22.90%, Temperatures:(0.00, 0.99)\n",
            "Old & New Losses 2168.64013671875 2167.949676513672 Probab: tensor(2.0022, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 849, LR: 0.0076, Train Loss: 2.1686, Train Accuracy: 23.20%, Temperatures:(0.00, 0.98)\n",
            "Old & New Losses 2166.6512489318848 2165.2169227600098 Probab: tensor(4.2922, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 850, LR: 0.0076, Train Loss: 2.1675, Train Accuracy: 23.70%, Temperatures:(0.00, 0.97)\n",
            "Old & New Losses 2167.468786239624 2174.217700958252 Probab: tensor(0.0010, device='cuda:0')\n",
            "Epoch 851, LR: 0.0076, Train Loss: 2.1657, Train Accuracy: 22.90%, Temperatures:(0.00, 0.96)\n",
            "Old & New Losses 2162.7743244171143 2169.017791748047 Probab: tensor(0.0015, device='cuda:0')\n",
            "Epoch 852, LR: 0.0038, Train Loss: 2.1670, Train Accuracy: 21.70%, Temperatures:(0.00, 0.96)\n",
            "Old & New Losses 2169.3835258483887 2166.625499725342 Probab: tensor(17.9389, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 853, LR: 0.0038, Train Loss: 2.1690, Train Accuracy: 23.00%, Temperatures:(0.00, 0.95)\n",
            "Old & New Losses 2166.7253971099854 2166.5050983428955 Probab: tensor(1.2623, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 854, LR: 0.0038, Train Loss: 2.1717, Train Accuracy: 22.60%, Temperatures:(0.00, 0.94)\n",
            "Old & New Losses 2167.2260761260986 2163.5000705718994 Probab: tensor(53.4848, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 855, LR: 0.0038, Train Loss: 2.1611, Train Accuracy: 23.50%, Temperatures:(0.00, 0.93)\n",
            "Old & New Losses 2165.4675006866455 2171.3616847991943 Probab: tensor(0.0017, device='cuda:0')\n",
            "Epoch 856, LR: 0.0038, Train Loss: 2.1669, Train Accuracy: 23.70%, Temperatures:(0.00, 0.92)\n",
            "Old & New Losses 2164.999485015869 2167.7253246307373 Probab: tensor(0.0513, device='cuda:0')\n",
            "Epoch 857, LR: 0.0038, Train Loss: 2.1680, Train Accuracy: 22.80%, Temperatures:(0.00, 0.91)\n",
            "Old & New Losses 2168.809175491333 2172.3077297210693 Probab: tensor(0.0213, device='cuda:0')\n",
            "Epoch 858, LR: 0.0038, Train Loss: 2.1694, Train Accuracy: 22.30%, Temperatures:(0.00, 0.90)\n",
            "Old & New Losses 2164.0560626983643 2164.9861335754395 Probab: tensor(0.3556, device='cuda:0')\n",
            "Epoch 859, LR: 0.0038, Train Loss: 2.1673, Train Accuracy: 25.30%, Temperatures:(0.00, 0.89)\n",
            "Old & New Losses 2161.778211593628 2171.3361740112305 Probab: tensor(2.1790e-05, device='cuda:0')\n",
            "Epoch 860, LR: 0.0038, Train Loss: 2.1684, Train Accuracy: 23.60%, Temperatures:(0.00, 0.88)\n",
            "Old & New Losses 2167.201280593872 2168.4610843658447 Probab: tensor(0.2395, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 861, LR: 0.0038, Train Loss: 2.1685, Train Accuracy: 24.00%, Temperatures:(0.00, 0.87)\n",
            "Old & New Losses 2167.1507358551025 2167.9062843322754 Probab: tensor(0.4207, device='cuda:0')\n",
            "Epoch 862, LR: 0.0038, Train Loss: 2.1630, Train Accuracy: 22.70%, Temperatures:(0.00, 0.86)\n",
            "Old & New Losses 2168.5738563537598 2169.083595275879 Probab: tensor(0.5543, device='cuda:0')\n",
            "Epoch 863, LR: 0.0038, Train Loss: 2.1673, Train Accuracy: 24.80%, Temperatures:(0.00, 0.86)\n",
            "Old & New Losses 2167.80948638916 2172.9774475097656 Probab: tensor(0.0024, device='cuda:0')\n",
            "Epoch 864, LR: 0.0038, Train Loss: 2.1705, Train Accuracy: 22.60%, Temperatures:(0.00, 0.85)\n",
            "Old & New Losses 2173.541784286499 2173.0732917785645 Probab: tensor(1.7389, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 865, LR: 0.0038, Train Loss: 2.1677, Train Accuracy: 22.60%, Temperatures:(0.00, 0.84)\n",
            "Old & New Losses 2168.9507961273193 2173.022508621216 Probab: tensor(0.0078, device='cuda:0')\n",
            "Epoch 866, LR: 0.0038, Train Loss: 2.1675, Train Accuracy: 23.40%, Temperatures:(0.00, 0.83)\n",
            "Old & New Losses 2171.7076301574707 2169.2066192626953 Probab: tensor(20.3579, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 867, LR: 0.0038, Train Loss: 2.1706, Train Accuracy: 21.40%, Temperatures:(0.00, 0.82)\n",
            "Old & New Losses 2168.6477661132812 2168.8926219940186 Probab: tensor(0.7423, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 868, LR: 0.0038, Train Loss: 2.1676, Train Accuracy: 23.20%, Temperatures:(0.00, 0.81)\n",
            "Old & New Losses 2168.0502891540527 2171.3874340057373 Probab: tensor(0.0165, device='cuda:0')\n",
            "Epoch 869, LR: 0.0038, Train Loss: 2.1680, Train Accuracy: 23.00%, Temperatures:(0.00, 0.81)\n",
            "Old & New Losses 2167.4463748931885 2167.196273803711 Probab: tensor(1.3642, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 870, LR: 0.0038, Train Loss: 2.1692, Train Accuracy: 23.10%, Temperatures:(0.00, 0.80)\n",
            "Old & New Losses 2166.4040088653564 2168.348789215088 Probab: tensor(0.0872, device='cuda:0')\n",
            "Epoch 871, LR: 0.0038, Train Loss: 2.1701, Train Accuracy: 21.30%, Temperatures:(0.00, 0.79)\n",
            "Old & New Losses 2169.189929962158 2168.1466102600098 Probab: tensor(3.7505, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 872, LR: 0.0038, Train Loss: 2.1682, Train Accuracy: 19.80%, Temperatures:(0.00, 0.78)\n",
            "Old & New Losses 2169.7983741760254 2169.053077697754 Probab: tensor(2.5956, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 873, LR: 0.0038, Train Loss: 2.1678, Train Accuracy: 20.50%, Temperatures:(0.00, 0.77)\n",
            "Old & New Losses 2171.3273525238037 2172.9609966278076 Probab: tensor(0.1210, device='cuda:0')\n",
            "Epoch 874, LR: 0.0038, Train Loss: 2.1704, Train Accuracy: 22.50%, Temperatures:(0.00, 0.77)\n",
            "Old & New Losses 2174.1552352905273 2171.297311782837 Probab: tensor(41.7550, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 875, LR: 0.0038, Train Loss: 2.1692, Train Accuracy: 21.10%, Temperatures:(0.00, 0.76)\n",
            "Old & New Losses 2167.0053005218506 2166.172742843628 Probab: tensor(2.9985, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 876, LR: 0.0038, Train Loss: 2.1665, Train Accuracy: 18.90%, Temperatures:(0.00, 0.75)\n",
            "Old & New Losses 2171.9002723693848 2171.811819076538 Probab: tensor(1.1251, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 877, LR: 0.0038, Train Loss: 2.1732, Train Accuracy: 20.10%, Temperatures:(0.00, 0.74)\n",
            "Old & New Losses 2171.2234020233154 2167.1786308288574 Probab: tensor(231.1924, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 878, LR: 0.0038, Train Loss: 2.1717, Train Accuracy: 20.10%, Temperatures:(0.00, 0.74)\n",
            "Old & New Losses 2175.337314605713 2174.437999725342 Probab: tensor(3.3956, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 879, LR: 0.0038, Train Loss: 2.1702, Train Accuracy: 19.80%, Temperatures:(0.00, 0.73)\n",
            "Old & New Losses 2174.204111099243 2169.9438095092773 Probab: tensor(347.1335, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 880, LR: 0.0038, Train Loss: 2.1693, Train Accuracy: 21.30%, Temperatures:(0.00, 0.72)\n",
            "Old & New Losses 2168.4372425079346 2176.9394874572754 Probab: tensor(7.5638e-06, device='cuda:0')\n",
            "Epoch 881, LR: 0.0038, Train Loss: 2.1706, Train Accuracy: 21.50%, Temperatures:(0.00, 0.71)\n",
            "Old & New Losses 2171.2987422943115 2172.135591506958 Probab: tensor(0.3096, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 882, LR: 0.0038, Train Loss: 2.1750, Train Accuracy: 20.30%, Temperatures:(0.00, 0.71)\n",
            "Old & New Losses 2172.11651802063 2171.522378921509 Probab: tensor(2.3181, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 883, LR: 0.0038, Train Loss: 2.1712, Train Accuracy: 21.30%, Temperatures:(0.00, 0.70)\n",
            "Old & New Losses 2176.027297973633 2169.6999073028564 Probab: tensor(8470.6221, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 884, LR: 0.0038, Train Loss: 2.1703, Train Accuracy: 21.60%, Temperatures:(0.00, 0.69)\n",
            "Old & New Losses 2174.441337585449 2178.290843963623 Probab: tensor(0.0039, device='cuda:0')\n",
            "Epoch 885, LR: 0.0038, Train Loss: 2.1721, Train Accuracy: 21.80%, Temperatures:(0.00, 0.69)\n",
            "Old & New Losses 2177.2406101226807 2171.018600463867 Probab: tensor(8728.1309, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 886, LR: 0.0038, Train Loss: 2.1739, Train Accuracy: 21.50%, Temperatures:(0.00, 0.68)\n",
            "Old & New Losses 2170.8133220672607 2175.833225250244 Probab: tensor(0.0006, device='cuda:0')\n",
            "Epoch 887, LR: 0.0038, Train Loss: 2.1750, Train Accuracy: 21.30%, Temperatures:(0.00, 0.67)\n",
            "Old & New Losses 2175.692081451416 2178.6556243896484 Probab: tensor(0.0122, device='cuda:0')\n",
            "Epoch 888, LR: 0.0038, Train Loss: 2.1753, Train Accuracy: 22.10%, Temperatures:(0.00, 0.67)\n",
            "Old & New Losses 2174.247980117798 2172.9867458343506 Probab: tensor(6.6573, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 889, LR: 0.0038, Train Loss: 2.1722, Train Accuracy: 21.20%, Temperatures:(0.00, 0.66)\n",
            "Old & New Losses 2169.5849895477295 2170.151948928833 Probab: tensor(0.4228, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 890, LR: 0.0038, Train Loss: 2.1758, Train Accuracy: 20.70%, Temperatures:(0.00, 0.65)\n",
            "Old & New Losses 2176.713228225708 2168.9279079437256 Probab: tensor(153188.0625, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 891, LR: 0.0038, Train Loss: 2.1746, Train Accuracy: 19.30%, Temperatures:(0.00, 0.65)\n",
            "Old & New Losses 2171.4954376220703 2175.596237182617 Probab: tensor(0.0017, device='cuda:0')\n",
            "Epoch 892, LR: 0.0038, Train Loss: 2.1725, Train Accuracy: 21.40%, Temperatures:(0.00, 0.64)\n",
            "Old & New Losses 2170.0358390808105 2170.295476913452 Probab: tensor(0.6661, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 893, LR: 0.0038, Train Loss: 2.1740, Train Accuracy: 20.90%, Temperatures:(0.00, 0.63)\n",
            "Old & New Losses 2170.1061725616455 2170.513868331909 Probab: tensor(0.5250, device='cuda:0')\n",
            "Epoch 894, LR: 0.0038, Train Loss: 2.1708, Train Accuracy: 21.60%, Temperatures:(0.00, 0.63)\n",
            "Old & New Losses 2170.3405380249023 2171.412944793701 Probab: tensor(0.1805, device='cuda:0')\n",
            "Epoch 895, LR: 0.0038, Train Loss: 2.1712, Train Accuracy: 20.80%, Temperatures:(0.00, 0.62)\n",
            "Old & New Losses 2170.250177383423 2182.4724674224854 Probab: tensor(2.7550e-09, device='cuda:0')\n",
            "Epoch 896, LR: 0.0038, Train Loss: 2.1662, Train Accuracy: 21.30%, Temperatures:(0.00, 0.61)\n",
            "Old & New Losses 2171.278715133667 2173.285484313965 Probab: tensor(0.0381, device='cuda:0')\n",
            "Epoch 897, LR: 0.0038, Train Loss: 2.1720, Train Accuracy: 20.00%, Temperatures:(0.00, 0.61)\n",
            "Old & New Losses 2170.039653778076 2167.1557426452637 Probab: tensor(115.0155, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 898, LR: 0.0038, Train Loss: 2.1704, Train Accuracy: 20.30%, Temperatures:(0.00, 0.60)\n",
            "Old & New Losses 2171.760320663452 2173.5453605651855 Probab: tensor(0.0515, device='cuda:0')\n",
            "Epoch 899, LR: 0.0038, Train Loss: 2.1693, Train Accuracy: 21.40%, Temperatures:(0.00, 0.60)\n",
            "Old & New Losses 2171.729564666748 2168.7378883361816 Probab: tensor(151.7635, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 900, LR: 0.0038, Train Loss: 2.1761, Train Accuracy: 18.60%, Temperatures:(0.00, 0.59)\n",
            "Old & New Losses 2171.426296234131 2174.7732162475586 Probab: tensor(0.0034, device='cuda:0')\n",
            "Epoch 901, LR: 0.0038, Train Loss: 2.1728, Train Accuracy: 18.40%, Temperatures:(0.00, 0.58)\n",
            "Old & New Losses 2172.985553741455 2171.595811843872 Probab: tensor(10.8095, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 902, LR: 0.0019, Train Loss: 2.1715, Train Accuracy: 22.00%, Temperatures:(0.00, 0.58)\n",
            "Old & New Losses 2172.964572906494 2168.048858642578 Probab: tensor(4939.0938, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 903, LR: 0.0019, Train Loss: 2.1743, Train Accuracy: 19.50%, Temperatures:(0.00, 0.57)\n",
            "Old & New Losses 2177.898645401001 2173.9273071289062 Probab: tensor(1033.2107, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 904, LR: 0.0019, Train Loss: 2.1677, Train Accuracy: 20.80%, Temperatures:(0.00, 0.57)\n",
            "Old & New Losses 2171.5102195739746 2169.8222160339355 Probab: tensor(19.6839, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 905, LR: 0.0019, Train Loss: 2.1702, Train Accuracy: 21.40%, Temperatures:(0.00, 0.56)\n",
            "Old & New Losses 2170.248508453369 2173.7325191497803 Probab: tensor(0.0020, device='cuda:0')\n",
            "Epoch 906, LR: 0.0019, Train Loss: 2.1728, Train Accuracy: 19.40%, Temperatures:(0.00, 0.56)\n",
            "Old & New Losses 2169.945001602173 2179.316759109497 Probab: tensor(4.6692e-08, device='cuda:0')\n",
            "Epoch 907, LR: 0.0019, Train Loss: 2.1728, Train Accuracy: 20.40%, Temperatures:(0.00, 0.55)\n",
            "Old & New Losses 2168.8179969787598 2173.3243465423584 Probab: tensor(0.0003, device='cuda:0')\n",
            "Epoch 908, LR: 0.0019, Train Loss: 2.1748, Train Accuracy: 21.70%, Temperatures:(0.00, 0.54)\n",
            "Old & New Losses 2169.065475463867 2170.541524887085 Probab: tensor(0.0664, device='cuda:0')\n",
            "Epoch 909, LR: 0.0019, Train Loss: 2.1701, Train Accuracy: 21.90%, Temperatures:(0.00, 0.54)\n",
            "Old & New Losses 2167.6058769226074 2172.1181869506836 Probab: tensor(0.0002, device='cuda:0')\n",
            "Epoch 910, LR: 0.0019, Train Loss: 2.1729, Train Accuracy: 20.80%, Temperatures:(0.00, 0.53)\n",
            "Old & New Losses 2168.0057048797607 2171.025276184082 Probab: tensor(0.0035, device='cuda:0')\n",
            "Epoch 911, LR: 0.0019, Train Loss: 2.1731, Train Accuracy: 21.20%, Temperatures:(0.00, 0.53)\n",
            "Old & New Losses 2169.2891120910645 2175.844430923462 Probab: tensor(4.0556e-06, device='cuda:0')\n",
            "Epoch 912, LR: 0.0019, Train Loss: 2.1729, Train Accuracy: 21.40%, Temperatures:(0.00, 0.52)\n",
            "Old & New Losses 2170.8011627197266 2171.633243560791 Probab: tensor(0.2036, device='cuda:0')\n",
            "Epoch 913, LR: 0.0019, Train Loss: 2.1688, Train Accuracy: 21.30%, Temperatures:(0.00, 0.52)\n",
            "Old & New Losses 2169.6205139160156 2171.279191970825 Probab: tensor(0.0405, device='cuda:0')\n",
            "Epoch 914, LR: 0.0019, Train Loss: 2.1681, Train Accuracy: 21.40%, Temperatures:(0.00, 0.51)\n",
            "Old & New Losses 2173.3250617980957 2169.1718101501465 Probab: tensor(3317.0042, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 915, LR: 0.0019, Train Loss: 2.1720, Train Accuracy: 21.60%, Temperatures:(0.00, 0.51)\n",
            "Old & New Losses 2170.8285808563232 2171.912431716919 Probab: tensor(0.1180, device='cuda:0')\n",
            "Epoch 916, LR: 0.0019, Train Loss: 2.1700, Train Accuracy: 20.60%, Temperatures:(0.00, 0.50)\n",
            "Old & New Losses 2171.0784435272217 2171.55122756958 Probab: tensor(0.3900, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 917, LR: 0.0019, Train Loss: 2.1709, Train Accuracy: 19.30%, Temperatures:(0.00, 0.50)\n",
            "Old & New Losses 2171.1814403533936 2180.540084838867 Probab: tensor(6.6643e-09, device='cuda:0')\n",
            "Epoch 918, LR: 0.0019, Train Loss: 2.1710, Train Accuracy: 20.50%, Temperatures:(0.00, 0.49)\n",
            "Old & New Losses 2172.0521450042725 2172.5354194641113 Probab: tensor(0.3746, device='cuda:0')\n",
            "Epoch 919, LR: 0.0019, Train Loss: 2.1690, Train Accuracy: 22.50%, Temperatures:(0.00, 0.49)\n",
            "Old & New Losses 2172.8994846343994 2171.236038208008 Probab: tensor(30.3941, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 920, LR: 0.0019, Train Loss: 2.1710, Train Accuracy: 20.30%, Temperatures:(0.00, 0.48)\n",
            "Old & New Losses 2168.0195331573486 2173.754930496216 Probab: tensor(6.8525e-06, device='cuda:0')\n",
            "Epoch 921, LR: 0.0019, Train Loss: 2.1735, Train Accuracy: 21.80%, Temperatures:(0.00, 0.48)\n",
            "Old & New Losses 2176.180124282837 2173.713445663452 Probab: tensor(175.1583, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 922, LR: 0.0019, Train Loss: 2.1725, Train Accuracy: 20.60%, Temperatures:(0.00, 0.47)\n",
            "Old & New Losses 2173.798084259033 2171.5641021728516 Probab: tensor(112.8022, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 923, LR: 0.0019, Train Loss: 2.1742, Train Accuracy: 20.10%, Temperatures:(0.00, 0.47)\n",
            "Old & New Losses 2173.93159866333 2175.4508018493652 Probab: tensor(0.0389, device='cuda:0')\n",
            "Epoch 924, LR: 0.0019, Train Loss: 2.1711, Train Accuracy: 22.10%, Temperatures:(0.00, 0.46)\n",
            "Old & New Losses 2171.630382537842 2172.727108001709 Probab: tensor(0.0938, device='cuda:0')\n",
            "Epoch 925, LR: 0.0019, Train Loss: 2.1710, Train Accuracy: 21.60%, Temperatures:(0.00, 0.46)\n",
            "Old & New Losses 2175.7123470306396 2169.8477268218994 Probab: tensor(356973.4375, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 926, LR: 0.0019, Train Loss: 2.1730, Train Accuracy: 21.40%, Temperatures:(0.00, 0.45)\n",
            "Old & New Losses 2169.548511505127 2173.931121826172 Probab: tensor(6.4361e-05, device='cuda:0')\n",
            "Epoch 927, LR: 0.0019, Train Loss: 2.1746, Train Accuracy: 20.00%, Temperatures:(0.00, 0.45)\n",
            "Old & New Losses 2173.4044551849365 2172.8413105010986 Probab: tensor(3.4996, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 928, LR: 0.0019, Train Loss: 2.1710, Train Accuracy: 20.00%, Temperatures:(0.00, 0.45)\n",
            "Old & New Losses 2171.626329421997 2168.6012744903564 Probab: tensor(894.9534, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 929, LR: 0.0019, Train Loss: 2.1714, Train Accuracy: 20.70%, Temperatures:(0.00, 0.44)\n",
            "Old & New Losses 2173.441171646118 2174.9353408813477 Probab: tensor(0.0337, device='cuda:0')\n",
            "Epoch 930, LR: 0.0019, Train Loss: 2.1711, Train Accuracy: 20.60%, Temperatures:(0.00, 0.44)\n",
            "Old & New Losses 2174.165964126587 2179.372549057007 Probab: tensor(6.5515e-06, device='cuda:0')\n",
            "Epoch 931, LR: 0.0019, Train Loss: 2.1732, Train Accuracy: 20.10%, Temperatures:(0.00, 0.43)\n",
            "Old & New Losses 2172.4162101745605 2172.2583770751953 Probab: tensor(1.4412, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 932, LR: 0.0019, Train Loss: 2.1709, Train Accuracy: 19.80%, Temperatures:(0.00, 0.43)\n",
            "Old & New Losses 2168.422222137451 2172.755479812622 Probab: tensor(3.9649e-05, device='cuda:0')\n",
            "Epoch 933, LR: 0.0019, Train Loss: 2.1698, Train Accuracy: 20.40%, Temperatures:(0.00, 0.42)\n",
            "Old & New Losses 2168.680429458618 2168.839454650879 Probab: tensor(0.6868, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 934, LR: 0.0019, Train Loss: 2.1726, Train Accuracy: 20.80%, Temperatures:(0.00, 0.42)\n",
            "Old & New Losses 2173.1865406036377 2175.21071434021 Probab: tensor(0.0080, device='cuda:0')\n",
            "Epoch 935, LR: 0.0019, Train Loss: 2.1692, Train Accuracy: 20.80%, Temperatures:(0.00, 0.41)\n",
            "Old & New Losses 2173.290491104126 2176.755905151367 Probab: tensor(0.0002, device='cuda:0')\n",
            "Epoch 936, LR: 0.0019, Train Loss: 2.1718, Train Accuracy: 19.80%, Temperatures:(0.00, 0.41)\n",
            "Old & New Losses 2177.212953567505 2172.179698944092 Probab: tensor(210167.6875, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 937, LR: 0.0019, Train Loss: 2.1746, Train Accuracy: 20.80%, Temperatures:(0.00, 0.41)\n",
            "Old & New Losses 2172.022819519043 2176.774263381958 Probab: tensor(8.4080e-06, device='cuda:0')\n",
            "Epoch 938, LR: 0.0019, Train Loss: 2.1782, Train Accuracy: 19.00%, Temperatures:(0.00, 0.40)\n",
            "Old & New Losses 2176.356315612793 2177.0365238189697 Probab: tensor(0.1845, device='cuda:0')\n",
            "Epoch 939, LR: 0.0019, Train Loss: 2.1755, Train Accuracy: 21.90%, Temperatures:(0.00, 0.40)\n",
            "Old & New Losses 2177.122116088867 2179.4216632843018 Probab: tensor(0.0031, device='cuda:0')\n",
            "Epoch 940, LR: 0.0019, Train Loss: 2.1769, Train Accuracy: 19.50%, Temperatures:(0.00, 0.39)\n",
            "Old & New Losses 2177.507162094116 2177.471160888672 Probab: tensor(1.0956, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 941, LR: 0.0019, Train Loss: 2.1786, Train Accuracy: 21.60%, Temperatures:(0.00, 0.39)\n",
            "Old & New Losses 2172.9724407196045 2177.6087284088135 Probab: tensor(6.9912e-06, device='cuda:0')\n",
            "Epoch 942, LR: 0.0019, Train Loss: 2.1705, Train Accuracy: 19.50%, Temperatures:(0.00, 0.39)\n",
            "Old & New Losses 2172.7311611175537 2175.6553649902344 Probab: tensor(0.0005, device='cuda:0')\n",
            "Epoch 943, LR: 0.0019, Train Loss: 2.1768, Train Accuracy: 19.80%, Temperatures:(0.00, 0.38)\n",
            "Old & New Losses 2177.4094104766846 2174.5247840881348 Probab: tensor(1873.9983, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 944, LR: 0.0019, Train Loss: 2.1747, Train Accuracy: 21.20%, Temperatures:(0.00, 0.38)\n",
            "Old & New Losses 2176.87726020813 2174.41987991333 Probab: tensor(654.9327, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 945, LR: 0.0019, Train Loss: 2.1750, Train Accuracy: 20.50%, Temperatures:(0.00, 0.38)\n",
            "Old & New Losses 2177.2773265838623 2177.074193954468 Probab: tensor(1.7185, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 946, LR: 0.0019, Train Loss: 2.1795, Train Accuracy: 19.50%, Temperatures:(0.00, 0.37)\n",
            "Old & New Losses 2176.7477989196777 2177.125930786133 Probab: tensor(0.3613, device='cuda:0')\n",
            "Epoch 947, LR: 0.0019, Train Loss: 2.1785, Train Accuracy: 20.40%, Temperatures:(0.00, 0.37)\n",
            "Old & New Losses 2174.9041080474854 2178.673267364502 Probab: tensor(3.5340e-05, device='cuda:0')\n",
            "Epoch 948, LR: 0.0019, Train Loss: 2.1783, Train Accuracy: 21.00%, Temperatures:(0.00, 0.36)\n",
            "Old & New Losses 2174.706220626831 2171.653985977173 Probab: tensor(4379.1968, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 949, LR: 0.0019, Train Loss: 2.1760, Train Accuracy: 20.40%, Temperatures:(0.00, 0.36)\n",
            "Old & New Losses 2172.903299331665 2181.9822788238525 Probab: tensor(1.1459e-11, device='cuda:0')\n",
            "Epoch 950, LR: 0.0019, Train Loss: 2.1752, Train Accuracy: 23.00%, Temperatures:(0.00, 0.36)\n",
            "Old & New Losses 2179.3465614318848 2178.577184677124 Probab: tensor(8.6402, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 951, LR: 0.0019, Train Loss: 2.1800, Train Accuracy: 19.90%, Temperatures:(0.00, 0.35)\n",
            "Old & New Losses 2181.168556213379 2183.366060256958 Probab: tensor(0.0020, device='cuda:0')\n",
            "Epoch 952, LR: 0.0010, Train Loss: 2.1747, Train Accuracy: 20.70%, Temperatures:(0.00, 0.35)\n",
            "Old & New Losses 2178.2891750335693 2180.1841259002686 Probab: tensor(0.0044, device='cuda:0')\n",
            "Epoch 953, LR: 0.0010, Train Loss: 2.1751, Train Accuracy: 20.60%, Temperatures:(0.00, 0.35)\n",
            "Old & New Losses 2178.220272064209 2185.0922107696533 Probab: tensor(2.3938e-09, device='cuda:0')\n",
            "Epoch 954, LR: 0.0010, Train Loss: 2.1783, Train Accuracy: 21.10%, Temperatures:(0.00, 0.34)\n",
            "Old & New Losses 2175.715446472168 2176.361322402954 Probab: tensor(0.1519, device='cuda:0')\n",
            "Epoch 955, LR: 0.0010, Train Loss: 2.1801, Train Accuracy: 20.10%, Temperatures:(0.00, 0.34)\n",
            "Old & New Losses 2179.0149211883545 2183.4022998809814 Probab: tensor(2.4224e-06, device='cuda:0')\n",
            "Epoch 956, LR: 0.0010, Train Loss: 2.1795, Train Accuracy: 20.00%, Temperatures:(0.00, 0.34)\n",
            "Old & New Losses 2174.84712600708 2180.2642345428467 Probab: tensor(9.9121e-08, device='cuda:0')\n",
            "Epoch 957, LR: 0.0010, Train Loss: 2.1805, Train Accuracy: 20.50%, Temperatures:(0.00, 0.33)\n",
            "Old & New Losses 2176.441192626953 2177.365779876709 Probab: tensor(0.0620, device='cuda:0')\n",
            "Epoch 958, LR: 0.0010, Train Loss: 2.1793, Train Accuracy: 20.80%, Temperatures:(0.00, 0.33)\n",
            "Old & New Losses 2176.0315895080566 2179.378032684326 Probab: tensor(3.8504e-05, device='cuda:0')\n",
            "Epoch 959, LR: 0.0010, Train Loss: 2.1780, Train Accuracy: 21.10%, Temperatures:(0.00, 0.33)\n",
            "Old & New Losses 2179.0993213653564 2179.466485977173 Probab: tensor(0.3242, device='cuda:0')\n",
            "Epoch 960, LR: 0.0010, Train Loss: 2.1792, Train Accuracy: 18.70%, Temperatures:(0.00, 0.32)\n",
            "Old & New Losses 2180.4215908050537 2185.7316493988037 Probab: tensor(7.1277e-08, device='cuda:0')\n",
            "Epoch 961, LR: 0.0010, Train Loss: 2.1802, Train Accuracy: 20.70%, Temperatures:(0.00, 0.32)\n",
            "Old & New Losses 2178.0714988708496 2183.421850204468 Probab: tensor(5.3208e-08, device='cuda:0')\n",
            "Epoch 962, LR: 0.0010, Train Loss: 2.1773, Train Accuracy: 20.40%, Temperatures:(0.00, 0.32)\n",
            "Old & New Losses 2183.7267875671387 2179.8617839813232 Probab: tensor(203099.0156, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 963, LR: 0.0010, Train Loss: 2.1762, Train Accuracy: 20.70%, Temperatures:(0.00, 0.31)\n",
            "Old & New Losses 2181.3271045684814 2184.882640838623 Probab: tensor(1.1694e-05, device='cuda:0')\n",
            "Epoch 964, LR: 0.0010, Train Loss: 2.1779, Train Accuracy: 21.80%, Temperatures:(0.00, 0.31)\n",
            "Old & New Losses 2175.7972240448 2182.2612285614014 Probab: tensor(8.7683e-10, device='cuda:0')\n",
            "Epoch 965, LR: 0.0010, Train Loss: 2.1744, Train Accuracy: 21.70%, Temperatures:(0.00, 0.31)\n",
            "Old & New Losses 2182.990074157715 2179.4512271881104 Probab: tensor(101991.7422, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 966, LR: 0.0010, Train Loss: 2.1808, Train Accuracy: 22.00%, Temperatures:(0.00, 0.30)\n",
            "Old & New Losses 2179.0478229522705 2176.8510341644287 Probab: tensor(1382.1088, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 967, LR: 0.0010, Train Loss: 2.1780, Train Accuracy: 24.30%, Temperatures:(0.00, 0.30)\n",
            "Old & New Losses 2182.4049949645996 2180.222272872925 Probab: tensor(1418.9004, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 968, LR: 0.0010, Train Loss: 2.1819, Train Accuracy: 20.50%, Temperatures:(0.00, 0.30)\n",
            "Old & New Losses 2184.011220932007 2177.8788566589355 Probab: tensor(8.8078e+08, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 969, LR: 0.0010, Train Loss: 2.1829, Train Accuracy: 21.40%, Temperatures:(0.00, 0.29)\n",
            "Old & New Losses 2184.5574378967285 2182.2006702423096 Probab: tensor(2967.4412, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 970, LR: 0.0010, Train Loss: 2.1786, Train Accuracy: 22.00%, Temperatures:(0.00, 0.29)\n",
            "Old & New Losses 2185.8105659484863 2183.932304382324 Probab: tensor(624.1954, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 971, LR: 0.0010, Train Loss: 2.1810, Train Accuracy: 21.90%, Temperatures:(0.00, 0.29)\n",
            "Old & New Losses 2183.894395828247 2182.750940322876 Probab: tensor(52.3523, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 972, LR: 0.0010, Train Loss: 2.1841, Train Accuracy: 22.60%, Temperatures:(0.00, 0.29)\n",
            "Old & New Losses 2179.037094116211 2179.880380630493 Probab: tensor(0.0524, device='cuda:0')\n",
            "Epoch 973, LR: 0.0010, Train Loss: 2.1801, Train Accuracy: 23.80%, Temperatures:(0.00, 0.28)\n",
            "Old & New Losses 2180.9215545654297 2182.3737621307373 Probab: tensor(0.0059, device='cuda:0')\n",
            "Epoch 974, LR: 0.0010, Train Loss: 2.1787, Train Accuracy: 22.70%, Temperatures:(0.00, 0.28)\n",
            "Old & New Losses 2182.939052581787 2181.260108947754 Probab: tensor(399.1929, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 975, LR: 0.0010, Train Loss: 2.1778, Train Accuracy: 23.40%, Temperatures:(0.00, 0.28)\n",
            "Old & New Losses 2183.239698410034 2183.770179748535 Probab: tensor(0.1479, device='cuda:0')\n",
            "Epoch 976, LR: 0.0010, Train Loss: 2.1838, Train Accuracy: 23.10%, Temperatures:(0.00, 0.27)\n",
            "Old & New Losses 2184.251546859741 2186.3245964050293 Probab: tensor(0.0005, device='cuda:0')\n",
            "Epoch 977, LR: 0.0010, Train Loss: 2.1782, Train Accuracy: 22.70%, Temperatures:(0.00, 0.27)\n",
            "Old & New Losses 2187.2971057891846 2185.5759620666504 Probab: tensor(559.9991, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 978, LR: 0.0010, Train Loss: 2.1819, Train Accuracy: 23.10%, Temperatures:(0.00, 0.27)\n",
            "Old & New Losses 2184.5436096191406 2185.7831478118896 Probab: tensor(0.0100, device='cuda:0')\n",
            "Epoch 979, LR: 0.0010, Train Loss: 2.1864, Train Accuracy: 21.70%, Temperatures:(0.00, 0.27)\n",
            "Old & New Losses 2182.140827178955 2184.9615573883057 Probab: tensor(2.5388e-05, device='cuda:0')\n",
            "Epoch 980, LR: 0.0010, Train Loss: 2.1840, Train Accuracy: 21.30%, Temperatures:(0.00, 0.26)\n",
            "Old & New Losses 2182.833433151245 2194.0510272979736 Probab: tensor(3.4702e-19, device='cuda:0')\n",
            "Epoch 981, LR: 0.0010, Train Loss: 2.1847, Train Accuracy: 20.60%, Temperatures:(0.00, 0.26)\n",
            "Old & New Losses 2186.634302139282 2186.262607574463 Probab: tensor(4.1480, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 982, LR: 0.0010, Train Loss: 2.1873, Train Accuracy: 20.90%, Temperatures:(0.00, 0.26)\n",
            "Old & New Losses 2185.2591037750244 2186.0806941986084 Probab: tensor(0.0417, device='cuda:0')\n",
            "Epoch 983, LR: 0.0010, Train Loss: 2.1887, Train Accuracy: 20.60%, Temperatures:(0.00, 0.26)\n",
            "Old & New Losses 2186.4984035491943 2188.6377334594727 Probab: tensor(0.0002, device='cuda:0')\n",
            "Epoch 984, LR: 0.0010, Train Loss: 2.1903, Train Accuracy: 20.50%, Temperatures:(0.00, 0.25)\n",
            "Old & New Losses 2187.27707862854 2183.777093887329 Probab: tensor(990433.1875, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 985, LR: 0.0010, Train Loss: 2.1862, Train Accuracy: 21.10%, Temperatures:(0.00, 0.25)\n",
            "Old & New Losses 2186.7470741271973 2183.861255645752 Probab: tensor(98544.6719, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 986, LR: 0.0010, Train Loss: 2.1862, Train Accuracy: 19.80%, Temperatures:(0.00, 0.25)\n",
            "Old & New Losses 2183.6047172546387 2186.8183612823486 Probab: tensor(2.4150e-06, device='cuda:0')\n",
            "Epoch 987, LR: 0.0010, Train Loss: 2.1840, Train Accuracy: 21.20%, Temperatures:(0.00, 0.25)\n",
            "Old & New Losses 2186.255931854248 2189.715623855591 Probab: tensor(7.7944e-07, device='cuda:0')\n",
            "Epoch 988, LR: 0.0010, Train Loss: 2.1880, Train Accuracy: 20.00%, Temperatures:(0.00, 0.24)\n",
            "Old & New Losses 2185.2121353149414 2186.502456665039 Probab: tensor(0.0050, device='cuda:0')\n",
            "Epoch 989, LR: 0.0010, Train Loss: 2.1860, Train Accuracy: 20.20%, Temperatures:(0.00, 0.24)\n",
            "Old & New Losses 2187.3526573181152 2189.81671333313 Probab: tensor(3.6415e-05, device='cuda:0')\n",
            "Epoch 990, LR: 0.0010, Train Loss: 2.1857, Train Accuracy: 21.50%, Temperatures:(0.00, 0.24)\n",
            "Old & New Losses 2186.2337589263916 2184.9441528320312 Probab: tensor(222.0978, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 991, LR: 0.0010, Train Loss: 2.1865, Train Accuracy: 21.30%, Temperatures:(0.00, 0.24)\n",
            "Old & New Losses 2192.1911239624023 2191.0347938537598 Probab: tensor(133.4413, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 992, LR: 0.0010, Train Loss: 2.1889, Train Accuracy: 19.50%, Temperatures:(0.00, 0.23)\n",
            "Old & New Losses 2188.6115074157715 2191.363573074341 Probab: tensor(7.7753e-06, device='cuda:0')\n",
            "Epoch 993, LR: 0.0010, Train Loss: 2.1849, Train Accuracy: 18.10%, Temperatures:(0.00, 0.23)\n",
            "Old & New Losses 2185.8251094818115 2188.077926635742 Probab: tensor(5.9613e-05, device='cuda:0')\n",
            "Epoch 994, LR: 0.0010, Train Loss: 2.1890, Train Accuracy: 19.10%, Temperatures:(0.00, 0.23)\n",
            "Old & New Losses 2189.189672470093 2185.6913566589355 Probab: tensor(4232400.5000, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 995, LR: 0.0010, Train Loss: 2.1901, Train Accuracy: 18.20%, Temperatures:(0.00, 0.23)\n",
            "Old & New Losses 2189.3107891082764 2192.6732063293457 Probab: tensor(3.6855e-07, device='cuda:0')\n",
            "Epoch 996, LR: 0.0010, Train Loss: 2.1891, Train Accuracy: 19.80%, Temperatures:(0.00, 0.22)\n",
            "Old & New Losses 2192.7874088287354 2194.354295730591 Probab: tensor(0.0009, device='cuda:0')\n",
            "Epoch 997, LR: 0.0010, Train Loss: 2.1905, Train Accuracy: 20.10%, Temperatures:(0.00, 0.22)\n",
            "Old & New Losses 2190.4165744781494 2191.696882247925 Probab: tensor(0.0032, device='cuda:0')\n",
            "Epoch 998, LR: 0.0010, Train Loss: 2.1882, Train Accuracy: 20.60%, Temperatures:(0.00, 0.22)\n",
            "Old & New Losses 2194.085121154785 2186.462163925171 Probab: tensor(1.0762e+15, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 999, LR: 0.0010, Train Loss: 2.1934, Train Accuracy: 19.40%, Temperatures:(0.00, 0.22)\n",
            "Old & New Losses 2192.5714015960693 2195.73712348938 Probab: tensor(4.9475e-07, device='cuda:0')\n",
            "Epoch 1000, LR: 0.0010, Train Loss: 2.1917, Train Accuracy: 21.80%, Temperatures:(0.00, 0.22)\n"
          ]
        }
      ],
      "source": [
        "# Training parameters\n",
        "lr = params_RRAM[\"ext_lr\"] / 25  # Initial learning rate\n",
        "num_epochs = params_RRAM[\"epochs\"]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    if epoch == 1:\n",
        "        lr *= 5\n",
        "    elif epoch == 2:\n",
        "        lr *= 5\n",
        "\n",
        "    model_RRAM.train()\n",
        "    outputs = model_RRAM(train_in)\n",
        "    loss = criterion(outputs, train_lab)\n",
        "    loss.backward()\n",
        "    model_RRAM.backprop(lr)\n",
        "    model_RRAM.anneal(train_in, train_lab,0.99, 0.99)\n",
        "\n",
        "    _, train_preds = torch.max(outputs, dim=1)\n",
        "    train_accuracy = (train_preds == train_lab).float().mean().item() * 100\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, LR: {lr:.4f}, Train Loss: {loss.item():.4f}, \"\n",
        "          f\"Train Accuracy: {train_accuracy:.2f}%, Temperatures:({model_RRAM.temperature_1:.2f}, {model_RRAM.temperature_2:.2f})\")\n",
        "\n",
        "    if epoch % 50 == 0 and epoch != 0:\n",
        "        lr /= 2\n",
        "\n",
        "model_RRAM = SoftBinaryRecurrentForwardNetwork(**model_params).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "282eeb54-9acf-4ce6-b5f0-13e5c60f8614",
      "metadata": {
        "id": "282eeb54-9acf-4ce6-b5f0-13e5c60f8614"
      },
      "source": [
        "### Loading Past Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "e0de620a-e0e4-49cf-863d-721e2680dd58",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0de620a-e0e4-49cf-863d-721e2680dd58",
        "outputId": "3fc15a9d-cc8e-4351-ff9c-d40906fc055f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Validation Loss: 1.77316\n",
            "Parameters for Best Loss Model: {'scaling': 5, 'G_ON': 6e-05, 'G_OFF': 2.88e-06, 'V_INV': 0.6, 'R_INV': 1000.0, 'V_1': 0.1, 'V_0': -0.1, 'zeta': 10.0, 'initial_factor': 0.01, 'crossbar': (64, 64), 'input_size': 676, 'encoding_size': 4, 'output_size': 10, 'data_in': 52, 'bin_active': True, 'monitor_volts': False, 'monitor_grads': False, 'monitor_latents': False, 'dropout': 0.1, 'int_lr': 0.01, 'int_norm': True, 'ext_lr': 500, 'epochs': 1000, 'temperature_1': 5, 'temperature_2': 500, 'monitor_annealing': True}\n",
            "Best Validation Accuracy: 68.34\n",
            "Parameters for Best Accuracy Model: {'scaling': 5, 'G_ON': 6e-05, 'G_OFF': 2.88e-06, 'V_INV': 0.6, 'R_INV': 1000.0, 'V_1': 0.1, 'V_0': -0.1, 'zeta': 10.0, 'initial_factor': 0.01, 'crossbar': (64, 64), 'input_size': 676, 'encoding_size': 4, 'output_size': 10, 'data_in': 52, 'bin_active': True, 'monitor_volts': False, 'monitor_grads': False, 'monitor_latents': False, 'dropout': 0.1, 'int_lr': 0.01, 'int_norm': True, 'ext_lr': 500, 'epochs': 1000, 'temperature_1': 5, 'temperature_2': 500, 'monitor_annealing': True}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-9a709494f6b6>:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint_loss = torch.load(\"Best_model_loss.pth\")\n",
            "<ipython-input-24-9a709494f6b6>:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint_acc = torch.load(\"Best_model_acc.pth\")\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    # Load best validation loss model\n",
        "    with open(\"Best_Val_Loss.txt\", 'r') as f:\n",
        "        global_best_val_loss = float(f.read())\n",
        "    with open(\"Best_Params_Loss.txt\", 'r') as f:\n",
        "        params_best_loss = ast.literal_eval(f.read())\n",
        "\n",
        "    model_best_loss = SoftBinaryRecurrentForwardNetwork(**model_params).to(device)\n",
        "\n",
        "    print(\"Best Validation Loss:\", global_best_val_loss)\n",
        "    print(\"Parameters for Best Loss Model:\", params_best_loss)\n",
        "\n",
        "    checkpoint_loss = torch.load(\"Best_model_loss.pth\")\n",
        "    model_best_loss.load_state_dict(checkpoint_loss)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error loading best loss model:\", e)\n",
        "    global_best_val_loss = float('inf')\n",
        "    print(\"No Saved Model for Best Loss\")\n",
        "\n",
        "try:\n",
        "    # Load best validation accuracy model\n",
        "    with open(\"Best_Val_Acc.txt\", 'r') as f:\n",
        "        global_best_val_acc = float(f.read())\n",
        "    with open(\"Best_Params_Acc.txt\", 'r') as f:\n",
        "        params_best_acc = ast.literal_eval(f.read())\n",
        "\n",
        "    model_best_acc = SoftBinaryRecurrentForwardNetwork(**model_params).to(device)\n",
        "\n",
        "    print(\"Best Validation Accuracy:\", global_best_val_acc)\n",
        "    print(\"Parameters for Best Accuracy Model:\", params_best_acc)\n",
        "\n",
        "    checkpoint_acc = torch.load(\"Best_model_acc.pth\")\n",
        "    model_best_acc.load_state_dict(checkpoint_acc)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error loading best accuracy model:\", e)\n",
        "    global_best_val_acc = 0.0\n",
        "    print(\"No Saved Model for Best Accuracy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "786757d6-3ef8-4327-900a-94924d5a1b57",
      "metadata": {
        "id": "786757d6-3ef8-4327-900a-94924d5a1b57"
      },
      "outputs": [],
      "source": [
        "history_RRAM = {\n",
        "    \"train_loss\": [],\n",
        "    \"train_accuracy\": [],\n",
        "    \"val_loss\": [],\n",
        "    \"val_accuracy\": []\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dc53685-ceb0-4b1c-a03f-04ca24416013",
      "metadata": {
        "id": "8dc53685-ceb0-4b1c-a03f-04ca24416013"
      },
      "source": [
        "### Complete Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9ed1842-f618-4562-8e82-c5355ca39f90",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9ed1842-f618-4562-8e82-c5355ca39f90",
        "outputId": "aba2264c-4110-4c7e-b283-98b16068904d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, LR: 20.0000, Train Loss: 2.1951, Train Acc: 18.64%, Val Loss: 2.1903, Val Acc: 17.76%, Temperatures: (0.31, 15690.53)\n",
            "Old & New Losses 21903.25975418091 22606.94980621338 Probab: tensor(0.9561, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 2, LR: 100.0000, Train Loss: 2.2072, Train Acc: 18.37%, Val Loss: 2.1846, Val Acc: 21.50%, Temperatures: (0.28, 14121.48)\n",
            "Old & New Losses 21846.49705886841 22691.352367401123 Probab: tensor(0.9419, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 3, LR: 500.0000, Train Loss: 2.2255, Train Acc: 17.35%, Val Loss: 2.2103, Val Acc: 18.74%, Temperatures: (0.25, 12709.33)\n",
            "Old & New Losses 22102.61106491089 22336.318492889404 Probab: tensor(0.9818, device='cuda:0')\n",
            "Annealed weights accepted\n",
            "Epoch 4, LR: 500.0000, Train Loss: 2.2127, Train Acc: 18.05%, Val Loss: 2.2080, Val Acc: 19.14%, Temperatures: (0.23, 11438.40)\n",
            "Old & New Losses 22080.12819290161 22149.863243103027 Probab: tensor(0.9939, device='cuda:0')\n",
            "Annealed weights accepted\n"
          ]
        }
      ],
      "source": [
        "lr = params_RRAM[\"ext_lr\"] / 25\n",
        "num_epochs = params_RRAM[\"epochs\"]\n",
        "patience, wait, wait_2, wait_3 = 500, 0, 75, 15\n",
        "wait_lr = 0\n",
        "cur_best_val_loss, cur_best_val_acc = float('inf'), 0\n",
        "temp_boosted = False\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    if epoch == 0:\n",
        "        lr = lr\n",
        "    elif epoch <= 2:\n",
        "        lr *= 5\n",
        "\n",
        "    model_RRAM.train().to(device)\n",
        "    train_loss, train_correct, total_samples = 0, 0, 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model_RRAM(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        model_RRAM.backprop(lr)\n",
        "\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "        train_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "        total_samples += inputs.size(0)\n",
        "\n",
        "        # model_RRAM.anneal(inputs, labels, 0.99, 0.999)\n",
        "\n",
        "    train_loss /= total_samples\n",
        "    train_accuracy = 100 * train_correct / total_samples\n",
        "\n",
        "    model_RRAM.eval()\n",
        "    val_loss, val_correct, total_test_samples = 0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model_RRAM(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "            total_test_samples += inputs.size(0)\n",
        "\n",
        "    val_loss /= total_test_samples\n",
        "    val_accuracy = 100 * val_correct / total_test_samples\n",
        "\n",
        "    history_RRAM[\"train_loss\"].append(train_loss)\n",
        "    history_RRAM[\"train_accuracy\"].append(train_accuracy)\n",
        "    history_RRAM[\"val_loss\"].append(val_loss)\n",
        "    history_RRAM[\"val_accuracy\"].append(val_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, LR: {lr:.4f}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%, Temperatures: ({model_RRAM.temperature_1:.2f}, {model_RRAM.temperature_2:.2f})\")\n",
        "\n",
        "    if val_loss < global_best_val_loss:\n",
        "        global_best_val_loss = val_loss\n",
        "        torch.save(model_RRAM.state_dict(), \"Best_model_loss.pth\")\n",
        "        with open(\"Best_Val_Loss.txt\", \"w\") as f:\n",
        "            f.write(f\"{val_loss:.6f}\")\n",
        "        with open(\"Best_Params_Loss.txt\", \"w\") as f:\n",
        "            f.write(f\"{params_RRAM}\")\n",
        "        print(f\"Model saved with best validation loss: {val_loss:.6f}\")\n",
        "\n",
        "    if val_accuracy > global_best_val_acc:\n",
        "        global_best_val_acc = val_accuracy\n",
        "        torch.save(model_RRAM.state_dict(), \"Best_model_acc.pth\")\n",
        "        with open(\"Best_Val_Acc.txt\", \"w\") as f:\n",
        "            f.write(f\"{val_accuracy:.6f}\")\n",
        "        with open(\"Best_Params_Acc.txt\", \"w\") as f:\n",
        "            f.write(f\"{params_RRAM}\")\n",
        "        print(f\"Model saved with best validation accuracy: {val_accuracy:.6f}\")\n",
        "\n",
        "    # Anneal Using Validation Set, Once Per Epoch\n",
        "    model_RRAM.anneal(inputs, labels, 0.9, 0.9)\n",
        "\n",
        "    if wait_lr >= wait_2 and epoch > 3:\n",
        "        lr /= 5\n",
        "        wait_lr = 0\n",
        "        print(f\"No improvement for {wait_2} epochs. Reducing LR to {lr:.4f}\")\n",
        "\n",
        "    if wait_lr >= wait_3 and not temp_boosted:\n",
        "        model_RRAM.temperature_2 *= 10\n",
        "        temp_boosted = True\n",
        "        print(f\"Training plateau detected. Temporarily increasing temperature_2 to {model_RRAM.temperature_2:.2f}\")\n",
        "\n",
        "    if val_loss < cur_best_val_loss - 0.001 or val_accuracy > cur_best_val_acc + 0.1:\n",
        "        if temp_boosted:\n",
        "            model_RRAM.temperature_2 /= 10\n",
        "            temp_boosted = False\n",
        "            print(f\"Training improved. Restoring temperature_2 to {model_RRAM.temperature_2:.2f}\")\n",
        "\n",
        "        cur_best_val_loss = min(cur_best_val_loss, val_loss)\n",
        "        cur_best_val_acc = max(cur_best_val_acc, val_accuracy)\n",
        "        wait = 0\n",
        "        wait_lr = 0\n",
        "    else:\n",
        "        wait += 1\n",
        "        wait_lr += 1\n",
        "\n",
        "    if wait >= patience and epoch > 6:\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "070a2ef8-94c5-49ca-bd35-3ed75ae2327b",
      "metadata": {
        "id": "070a2ef8-94c5-49ca-bd35-3ed75ae2327b"
      },
      "outputs": [],
      "source": [
        "plot_history(history_RRAM, num_epochs, \"RRAM\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c93e55bc-1bae-4483-a3cf-01cde188dc69",
      "metadata": {
        "id": "c93e55bc-1bae-4483-a3cf-01cde188dc69"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af52e53e-15ed-441f-8412-d94c5d6a0bdb",
      "metadata": {
        "id": "af52e53e-15ed-441f-8412-d94c5d6a0bdb"
      },
      "source": [
        "### Current Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51271c7c-25b1-4702-9db1-9ae4b805843c",
      "metadata": {
        "id": "51271c7c-25b1-4702-9db1-9ae4b805843c"
      },
      "outputs": [],
      "source": [
        "print(0 + (model_RRAM.w > 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d477c9e-5678-4d07-a5ac-48f01480c916",
      "metadata": {
        "id": "9d477c9e-5678-4d07-a5ac-48f01480c916"
      },
      "outputs": [],
      "source": [
        "cm = test(model_RRAM, val_inputs, val_labels, class_names = [\"A\", \"T\", \"V\", \"X\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed7941df-4678-4955-8a0f-f38b43f6b197",
      "metadata": {
        "id": "ed7941df-4678-4955-8a0f-f38b43f6b197"
      },
      "source": [
        "## Best Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "867574d7-3627-49ff-a430-1d658fa4d655",
      "metadata": {
        "id": "867574d7-3627-49ff-a430-1d658fa4d655"
      },
      "outputs": [],
      "source": [
        "0+1*(model_best.w>0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32ca56c6-7023-416e-9675-93f1a3c8cc78",
      "metadata": {
        "id": "32ca56c6-7023-416e-9675-93f1a3c8cc78"
      },
      "outputs": [],
      "source": [
        "cm = test(model_best, val_inputs, val_labels, class_names = [\"A\", \"T\", \"V\", \"X\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad288f0b-0ec9-415d-96cc-a4ecec7aa5ce",
      "metadata": {
        "id": "ad288f0b-0ec9-415d-96cc-a4ecec7aa5ce"
      },
      "source": [
        "## PWL Generation\n",
        "\n",
        "Let's assume that we will program the two crossbars with seperate PWLs. That is, during programming, we will cut the Inverting Amplifier stages with a pass transistor and connect the programming lines with a pass transistor. First array has 16 Top PWLs and 8 Bottom PWLs. Second array has 8 Top PWLs and 4 Bottom PWLs. And then once the programming switch is toggled to inference mode, only the 16 Top PWLs are to be changed. Let's also generate a PWL for that too.\n",
        "\n",
        "In the code below, we will first maintain tuples for each PWL that holds what the voltage should be. And then we will write a function that will take there and space pulses of the given voltage that are 100us apart from other and have an ON duration of 100us"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58d5cebb-fc99-4a00-87a1-e183838c9a64",
      "metadata": {
        "id": "58d5cebb-fc99-4a00-87a1-e183838c9a64"
      },
      "outputs": [],
      "source": [
        "WL_FC1 = [list() for i in range(16)]\n",
        "BL_FC1 = [list() for i in range(8)]\n",
        "WL_FC2 = [list() for i in range(8)]\n",
        "BL_FC2 = [list() for i in range(4)]\n",
        "Mode = []\n",
        "Mode_B = []\n",
        "\n",
        "V_WRITE = 1.5\n",
        "V_READ = 0.1\n",
        "V_mode = 1.2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c647afda-e1a7-47a7-b634-212ec5527be2",
      "metadata": {
        "id": "c647afda-e1a7-47a7-b634-212ec5527be2"
      },
      "source": [
        "#### Fully Connected Weights 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03334172-89f6-4d73-9461-6ac922bdb6ea",
      "metadata": {
        "id": "03334172-89f6-4d73-9461-6ac922bdb6ea"
      },
      "outputs": [],
      "source": [
        "target = (model_RRAM_best.w1>0).int()\n",
        "target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49287afe-8437-40aa-8825-93f2cffe3a66",
      "metadata": {
        "id": "49287afe-8437-40aa-8825-93f2cffe3a66"
      },
      "outputs": [],
      "source": [
        "for ind_i, i in enumerate(target):\n",
        "    for ind_j, j in enumerate(i):\n",
        "        if j==1: WL_FC1[ind_j].append(V_WRITE)\n",
        "        else: WL_FC1[ind_j].append(V_WRITE/3)\n",
        "    for ind_k in range(len(target)):\n",
        "        if ind_k==ind_i: BL_FC1[ind_i].append(0)\n",
        "        else: BL_FC1[ind_k].append(2*V_WRITE/3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be77131c-005d-4f06-8a70-f40d267eb67e",
      "metadata": {
        "id": "be77131c-005d-4f06-8a70-f40d267eb67e"
      },
      "source": [
        "#### Fully Connected Weights 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9adc2b24-ffb0-4470-8eae-3830221cb33d",
      "metadata": {
        "id": "9adc2b24-ffb0-4470-8eae-3830221cb33d"
      },
      "outputs": [],
      "source": [
        "target = (model_RRAM_best.w2>0).int()\n",
        "target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f7ca037-0e2f-49a3-98b3-b2e0b93f8b79",
      "metadata": {
        "id": "3f7ca037-0e2f-49a3-98b3-b2e0b93f8b79"
      },
      "outputs": [],
      "source": [
        "for ind_i, i in enumerate(target):\n",
        "    for ind_j, j in enumerate(i):\n",
        "        if j==1: WL_FC2[ind_j].append(V_WRITE)\n",
        "        else: WL_FC2[ind_j].append(V_WRITE/3)\n",
        "    for ind_k in range(len(target)):\n",
        "        if ind_k==ind_i: BL_FC2[ind_i].append(0)\n",
        "        else: BL_FC2[ind_k].append(2*V_WRITE/3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a602368f-07a2-4beb-90e3-d782ca717fea",
      "metadata": {
        "id": "a602368f-07a2-4beb-90e3-d782ca717fea"
      },
      "source": [
        "#### Filling Out Programming Mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4aa24d2-0d95-4ff4-8dfd-5c0800d9de11",
      "metadata": {
        "id": "c4aa24d2-0d95-4ff4-8dfd-5c0800d9de11"
      },
      "outputs": [],
      "source": [
        "WL_FC1 = [i + [0,0] for i in WL_FC1]\n",
        "BL_FC1 = [i + [0,0] for i in BL_FC1]\n",
        "while(len(WL_FC2[0]) < len(WL_FC1[0])):\n",
        "    WL_FC2 = [i + [0,] for i in WL_FC2]\n",
        "    BL_FC2 = [i + [0,] for i in BL_FC2]\n",
        "Mode.extend([V_mode]*(len(WL_FC1[0])-1) + [-V_mode])\n",
        "Mode_B.extend([-V_mode]*(len(WL_FC1[0])-1) + [V_mode])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f953198a-a107-428c-9624-3c712086e922",
      "metadata": {
        "id": "f953198a-a107-428c-9624-3c712086e922"
      },
      "outputs": [],
      "source": [
        "print(WL_FC1[0])\n",
        "print(BL_FC1[0])\n",
        "print(WL_FC2[0])\n",
        "print(BL_FC2[0])\n",
        "print(Mode)\n",
        "print(Mode_B)\n",
        "print(len(WL_FC1[0]), len(BL_FC1[0]), len(WL_FC2[0]), len(BL_FC2[0]), len(Mode), len(Mode_B))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92339f84-e8d6-4280-92af-e5215aa129a0",
      "metadata": {
        "id": "92339f84-e8d6-4280-92af-e5215aa129a0"
      },
      "source": [
        "### Inference: Loading the Testing Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b97dacd2-42bb-49f0-8ca9-e0f02ca4cc58",
      "metadata": {
        "id": "b97dacd2-42bb-49f0-8ca9-e0f02ca4cc58"
      },
      "outputs": [],
      "source": [
        "val_inputs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2336c3ab-2181-4ee7-b1c9-0a97a3700248",
      "metadata": {
        "id": "2336c3ab-2181-4ee7-b1c9-0a97a3700248"
      },
      "outputs": [],
      "source": [
        "V_1 = 0.1\n",
        "V_0 = -0.1\n",
        "include_testing = True\n",
        "include_every = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a1cfba2-3243-4062-9ef6-4cd309622c73",
      "metadata": {
        "id": "6a1cfba2-3243-4062-9ef6-4cd309622c73"
      },
      "outputs": [],
      "source": [
        "if include_testing:\n",
        "    for i in val_inputs[::include_every]:\n",
        "        i = i.flatten()\n",
        "        for ind, j in enumerate(i):\n",
        "            WL_FC1[ind].append(V_1 if j==1 else V_0)\n",
        "        BL_FC1 = [i + [0,] for i in BL_FC1]\n",
        "        WL_FC2 = [i + [0,] for i in WL_FC2]\n",
        "        BL_FC2 = [i + [0,] for i in BL_FC2]\n",
        "        Mode = Mode + [-V_mode,]\n",
        "        Mode_B = Mode_B + [V_mode,]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9dd97b8-fb1b-4634-9ae4-3a6fe05d0a2c",
      "metadata": {
        "id": "b9dd97b8-fb1b-4634-9ae4-3a6fe05d0a2c"
      },
      "source": [
        "### PWL Convertion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c98af5ff-8f34-4472-89fc-e308d98d4072",
      "metadata": {
        "id": "c98af5ff-8f34-4472-89fc-e308d98d4072"
      },
      "outputs": [],
      "source": [
        "def pwl(l):\n",
        "    t = 0\n",
        "    res = \"pwl(time, 0us, 0V\"\n",
        "    for i in l:\n",
        "        res += f\", {t+5}us, {i:.2f}V, {t+100}us, {i:.2f}V, {t+105}us, 0V, {t+200}us, 0V\"\n",
        "        t+=200\n",
        "    res += \")\"\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54850dbc-1317-44a2-9093-c5a4e048094c",
      "metadata": {
        "id": "54850dbc-1317-44a2-9093-c5a4e048094c"
      },
      "outputs": [],
      "source": [
        "pwl_data = []\n",
        "\n",
        "for ind, i in enumerate(WL_FC1):\n",
        "    pwl_data.append({\"Signal\": f\"WL_FC1_{ind}\", \"Index\": ind, \"PWL\": pwl(i)})\n",
        "for ind, i in enumerate(BL_FC1):\n",
        "    pwl_data.append({\"Signal\": f\"BL_FC1_{ind}\", \"Index\": ind, \"PWL\": pwl(i)})\n",
        "for ind, i in enumerate(WL_FC2):\n",
        "    pwl_data.append({\"Signal\": f\"WL_FC2_{ind}\", \"Index\": ind, \"PWL\": pwl(i)})\n",
        "for ind, i in enumerate(BL_FC2):\n",
        "    pwl_data.append({\"Signal\": f\"BL_FC2_{ind}\", \"Index\": ind, \"PWL\": pwl(i)})\n",
        "pwl_data.append({\"Signal\": \"Mode\", \"Index\": \"\", \"PWL\": pwl(Mode)})\n",
        "pwl_data.append({\"Signal\": \"Mode_b\", \"Index\": \"\", \"PWL\": pwl(Mode_B)})\n",
        "\n",
        "pwl_data = pd.DataFrame(pwl_data)\n",
        "pwl_data.to_csv(\"pwl_data.csv\", index=False)\n",
        "pwl_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03e13866-8cd8-4b8e-a8d0-a5c69e951174",
      "metadata": {
        "id": "03e13866-8cd8-4b8e-a8d0-a5c69e951174"
      },
      "source": [
        "#### Testing Accuracy on 160 Images\n",
        "ADS isn't allowing PWLs longer than 160 Images, so let's check software accuracy for the same too"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd4e4e97-5ecd-4c41-9035-4a51015d684e",
      "metadata": {
        "id": "fd4e4e97-5ecd-4c41-9035-4a51015d684e"
      },
      "outputs": [],
      "source": [
        "test(model_RRAM_best, val_inputs[::4], val_labels[::4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cf44b69-14ac-4fdd-a2b5-b9c2cc361e7f",
      "metadata": {
        "id": "7cf44b69-14ac-4fdd-a2b5-b9c2cc361e7f"
      },
      "outputs": [],
      "source": [
        "test(model_RRAM_best, train_inputs, train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1079074f-8781-43af-b52e-fb3581ef8931",
      "metadata": {
        "id": "1079074f-8781-43af-b52e-fb3581ef8931"
      },
      "source": [
        "## Simulation Data from ADS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0d523a2-f99f-46bb-93e0-fead2768da75",
      "metadata": {
        "id": "e0d523a2-f99f-46bb-93e0-fead2768da75"
      },
      "outputs": [],
      "source": [
        "simu = pd.read_csv(\"Testing_160_Images.csv\")\n",
        "simu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd96e845-0c19-470b-8910-6b45df78022c",
      "metadata": {
        "id": "fd96e845-0c19-470b-8910-6b45df78022c"
      },
      "outputs": [],
      "source": [
        "def remove_units(value):\n",
        "    return float(value.replace('E', 'e').split('V')[0].replace('sec', ''))\n",
        "\n",
        "simu['time'] = simu['time'].apply(remove_units)\n",
        "for col in ['A', 'X', 'V', 'T']:\n",
        "    simu[col] = simu[col].apply(remove_units)\n",
        "simu"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "223576d6-266c-49bd-aaca-ff3444617f09",
      "metadata": {
        "id": "223576d6-266c-49bd-aaca-ff3444617f09"
      },
      "source": [
        "We just need one sample every 0.1ms samples of these starting from 2.050ms to 33.850ms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a13d6e19-daa6-481f-8ad2-11a357dc20d3",
      "metadata": {
        "id": "a13d6e19-daa6-481f-8ad2-11a357dc20d3"
      },
      "outputs": [],
      "source": [
        "t_stamps = np.arange(2.05e-3, 33.9e-3, 0.2e-3)\n",
        "t_stamps.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5548dd9-e376-4bd1-b9d2-484f4358f700",
      "metadata": {
        "id": "f5548dd9-e376-4bd1-b9d2-484f4358f700"
      },
      "outputs": [],
      "source": [
        "sampled = []\n",
        "window = 0.02e-3\n",
        "\n",
        "for t in t_stamps:\n",
        "    filtered = simu[(simu['time'] >= t - window) & (simu['time'] <= t + window)]\n",
        "\n",
        "    avg_A = filtered['A'].mean()\n",
        "    avg_X = filtered['X'].mean()\n",
        "    avg_V = filtered['V'].mean()\n",
        "    avg_T = filtered['T'].mean()\n",
        "\n",
        "    sampled.append({\n",
        "        'Image Index': t,\n",
        "        'A': avg_A,\n",
        "        'X': avg_X,\n",
        "        'V': avg_V,\n",
        "        'T': avg_T\n",
        "    })\n",
        "\n",
        "sampled = pd.DataFrame(sampled)\n",
        "sampled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "096828ea-b6be-4839-8b7d-4e0f5a515a7e",
      "metadata": {
        "id": "096828ea-b6be-4839-8b7d-4e0f5a515a7e"
      },
      "outputs": [],
      "source": [
        "def get_max_column(row):\n",
        "    return row[['A', 'X', 'V', 'T']].idxmax()\n",
        "sampled['Predicted Class'] = sampled.apply(get_max_column, axis=1)\n",
        "sampled.to_csv(\"Sampled_Results.csv\", index=False)\n",
        "sampled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1143c06-5631-4c79-9965-0cb937929706",
      "metadata": {
        "id": "d1143c06-5631-4c79-9965-0cb937929706"
      },
      "outputs": [],
      "source": [
        "ground_truth = ['A']*40 + ['X']*40 + ['V']*40 + ['T']*40\n",
        "correct_predictions = sampled['Predicted Class'] == ground_truth\n",
        "accuracy = correct_predictions.sum() / len(ground_truth)\n",
        "print(accuracy*100,end=\"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "918b4552-b0d3-4768-bfe7-9b874959d42c",
      "metadata": {
        "id": "918b4552-b0d3-4768-bfe7-9b874959d42c"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7, 3.5))\n",
        "\n",
        "plt.scatter(sampled.index, sampled['A'], color='red', label='A_pred', s=30, marker='o')  # Red dots for A\n",
        "plt.scatter(sampled.index, sampled['X'], color='blue', label='X_pred', s=30, marker='o')  # Blue dots for X\n",
        "plt.scatter(sampled.index, sampled['T'], color='green', label='T_pred', s=30, marker='o')  # Green dots for T\n",
        "plt.scatter(sampled.index, sampled['V'], color='orange', label='V_pred', s=30, marker='o')  # Orange dots for V\n",
        "\n",
        "plt.xlabel('Image Index')\n",
        "plt.ylabel('Predicted Voltages (V)')\n",
        "plt.legend()\n",
        "\n",
        "plt.axvline(x=40, color='gray', linestyle='--', linewidth=2)\n",
        "plt.axvline(x=80, color='gray', linestyle='--', linewidth=2)\n",
        "plt.axvline(x=120, color='gray', linestyle='--', linewidth=2)\n",
        "\n",
        "plt.text(20, plt.ylim()[1]*(-0.8), 'A', fontsize=15, color='black', ha='center')\n",
        "plt.text(60, plt.ylim()[1]*0.8, 'X', fontsize=15, color='black', ha='center')\n",
        "plt.text(100, plt.ylim()[1]*0.8, 'V', fontsize=15, color='black', ha='center')\n",
        "plt.text(140, plt.ylim()[1]*(-0.8), 'T', fontsize=15, color='black', ha='center')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e86f31ef-ad69-472f-9217-bdde6f8ab1e8",
      "metadata": {
        "id": "e86f31ef-ad69-472f-9217-bdde6f8ab1e8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}